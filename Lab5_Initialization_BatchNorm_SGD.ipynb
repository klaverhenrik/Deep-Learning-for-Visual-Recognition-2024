{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mihd9hpHKVh"
      },
      "source": [
        "#Lab 5: Training neural networks (part 1/2)\n",
        "**Like always, remember to set Runtime environment to GPU**\n",
        "\n",
        "In this lab we will explore different tools that will help you train your own neural networks.\n",
        "\n",
        "Today we will be using fully connected networks. Next time, in part 2, we will be using ConvNets.\n",
        "\n",
        "**Prerequisites:** You will most likely find it easier to solve this lab if you have already completed the [PyTorch tutorial](https://github.com/klaverhenrik/Deep-Learning-for-Visual-Recognition-2024/blob/main/Lab5_PyTorch_Tutorial.ipynb) and read the slides for Lecture 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tGBcfgIumcH"
      },
      "source": [
        "##1. Download the CIFAR 10 dataset\n",
        "We will be using the CIFAR 10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_awI9jy3V9P"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "# Batch size\n",
        "bs = 256\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=bs,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iorLYSEz8Xbs"
      },
      "source": [
        "Display some stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U50pDb-_5uKq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to show an image\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Number of samples\n",
        "num_train = bs * len(trainloader)\n",
        "num_test = bs * len(testloader)\n",
        "print('num_train',num_train)\n",
        "print('num_test',num_test)\n",
        "\n",
        "# Get a batch of some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "print('images.shape',images.shape)\n",
        "print('images.min()',images.min())\n",
        "print('images.max()',images.max())\n",
        "\n",
        "# show 4 images\n",
        "imshow(torchvision.utils.make_grid(images[0:4]))\n",
        "\n",
        "# print 4 labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PEUHshm8jNO"
      },
      "source": [
        "###Questions\n",
        "1. What is the batch size?\n",
        "2. What is the size of the images?\n",
        "3. What is the range of the pixel intensities?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AdROOxx9ldc"
      },
      "source": [
        "##2. Task 0: Preprocessing\n",
        "Recall that we always prefer to have our data zero-centered. For this purpose, we can extend the transform function used above:\n",
        "\n",
        "```\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "```\n",
        "\n",
        "This transformer converts a loaded image to a PyTorch tensor, as required by PyTorch models. As demonstrated in the [PyTorch Transfer Learning tutorial](https://github.com/klaverhenrik/Deep-Learning-for-Visual-Recognition-2024/blob/main/Lab5_PyTorch_TransferLearning.ipynb) you can chain together multiple transforms to perform a sequence of operations for data augmentation and/or preprocessing(see complete list [here](https://pytorch.org/vision/0.15/transforms.html)).\n",
        "\n",
        "Your task is to add a second transformation that normalizes the pixel intensities to range -1 to 1. Use [`transforms.Normalize()`](https://pytorch.org/vision/0.15/generated/torchvision.transforms.Normalize.html) with suitable input parameters. The normalization is based on the equation\n",
        "\n",
        "```\n",
        "y = (x - mean) / std\n",
        "```\n",
        "\n",
        "where mean and std are user-defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHD3jAAK3lW0"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(???)]) # Your code goes here\n",
        "\n",
        "# Re-initialize trainloader and testloader\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=bs,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Verify that intensities are in range -1 to 1\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "print('images.min()',images.min())\n",
        "print('images.max()',images.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0n4oZ2y2MuE"
      },
      "source": [
        "###Questions\n",
        "Below you are going to initialize and normalize the network weights. To prepare for this task, answer the following questions using appropriate PyTorch functions (use Google to figure out which):\n",
        "1. What is the mean and standard deviation (std) of tensor `x` below?\n",
        "2. What is the mean and std `y`?\n",
        "3. What is the mean and std `z`?\n",
        "\n",
        "Bonus task: See of you can figure out a formula to calculate the mean and std of `x` and `y`, given the mean and std and `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0IHPLaP2OHx"
      },
      "source": [
        "x = torch.randn(512, 512)\n",
        "y = x * 10 + 2\n",
        "z = (y-2) / 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y945T69aHKVy"
      },
      "source": [
        "##3. Base model\n",
        "We'll first create a base model that we can use for our initial experiments. The base model is a fully connected neural network. By default it has\n",
        "\n",
        "- `L = 10` layers\n",
        "- `N = 16` units in each hidden layer.\n",
        "\n",
        "When initializing the model you can specify\n",
        "\n",
        "- a `normalizer` function, which is applied when initializing the weight matrix `W` of each layer. The input to the normalizer functions is `W` and the output is some normalized version of `W`, like Xavier or Kaiming.\n",
        "\n",
        "- an `activation_function` which could be sigmoid, tanh, ReLu, etc.\n",
        "\n",
        "- a `preprocess_function` which is applied in each layer after calculating `Wx+b` but before applying the activation function. It can be used to implement batch normalization.\n",
        "\n",
        "Initially we set all these functions to the identity function. In other words, the base model is a purely linear model without any activation functions, normalization or anything like that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdOBKw9G0YPO"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "def base_normalizer(x): return x\n",
        "def base_activation(x): return x\n",
        "def base_preprocess(x): return x\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 normalizer = base_normalizer,\n",
        "                 activation_function = base_activation,\n",
        "                 preprocess_function = base_preprocess,\n",
        "                 L = 10,\n",
        "                 N = 16):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.W = nn.ParameterList() # List of weights\n",
        "        self.b = nn.ParameterList() # List of biases\n",
        "        self.L = L # Number of layers\n",
        "        self.N = N # Number of units in each hidden layer\n",
        "        self.activation_function = activation_function\n",
        "        self.preprocess_function = preprocess_function\n",
        "        self.layer_activations = [] # Store layer activations here\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        for layer in range(L):\n",
        "          dims = [self.N, self.N] # Size of hidden layer\n",
        "          if layer == 0: dims = [32*32*3,N] # Size of first layer\n",
        "          if layer == L-1: dims = [N,10] # Size of last layer\n",
        "          W = nn.Parameter(normalizer(torch.randn(dims[0], dims[1]))) # Call normalizer here\n",
        "          b = nn.Parameter(torch.zeros(dims[1]))\n",
        "          self.W.append(W)\n",
        "          self.b.append(b)\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, x):\n",
        "        self.layer_activations = []\n",
        "        x = x.view(-1, 32*32*3) # Vectorize image to a 32*32*3 dimensional vector\n",
        "        for layer in range(self.L):\n",
        "          x = x @ self.W[layer] + self.b[layer]\n",
        "          x = self.preprocess_function(x) # Call preprocess function before activation\n",
        "          x = self.activation_function(x)\n",
        "          self.layer_activations.append(x) # Store activations\n",
        "        return x\n",
        "\n",
        "    # Return stored layer activations\n",
        "    def activations(self):\n",
        "        return self.layer_activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83a4sLSObAyE"
      },
      "source": [
        "###3.1 Test the model\n",
        "Let's test the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p68n0tD_bEFO"
      },
      "source": [
        "# Move data to GPU\n",
        "images = images.cuda()\n",
        "labels = labels.cuda()\n",
        "\n",
        "# Calculate scores\n",
        "model = BaseModel().cuda()\n",
        "scores = model(images)  # predictions\n",
        "\n",
        "print(scores.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP2f2yzibZXg"
      },
      "source": [
        "###3.2 Calculating the accuracy\n",
        "These numbers are scores (logits), which don't have any meaningful interpretation. We can convert them into class probabilities using softmax. Since we are only going to be interested in the model's accuracy, we will wrap the softmax inside the function `accuracy` that calculates the accuracy on a batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_hUiKnIcBZU"
      },
      "source": [
        "def accuracy(scores, yb):\n",
        "    score2prob = nn.Softmax(dim=1)\n",
        "    preds = torch.argmax(score2prob(scores), dim=1)\n",
        "    return (preds == yb).float().mean()\n",
        "\n",
        "print('Accuracy', accuracy(scores,labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNpcQS3gzsZE"
      },
      "source": [
        "### Question\n",
        "1. What does `torch.argmax` do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gCGFOqycidx"
      },
      "source": [
        "###3.3 Calculating the loss\n",
        "I order to train your model, we also need a loss function. We will use the cross entropy loss [already provided in PyTorch](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy). Note that `cross_entropy` does the softmax for you, so the input is just the scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpdvLkhNxaTc"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "loss_func = F.cross_entropy\n",
        "loss = loss_func(scores, labels)\n",
        "print('Loss', loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SILeyqxcc7ed"
      },
      "source": [
        "###3.4 Getting layer activations and gradients\n",
        "One special propoerty of our model is that it stores the activations after each layer (in variable `self.layer_activations`).\n",
        "\n",
        "Our goal is develop tools that we can use to inspect our model both before and during training. For this purpose it will be useful to be able to grab the layer activations as well as the gradients at each layer. The activations have alreay been stored during our forward propagation and can be extracted as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJkCQWMd3xH"
      },
      "source": [
        "activations = model.activations()\n",
        "print('Activation list length:',len(activations))\n",
        "print('Activations shape layer 1:',activations[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojGryp4yeLDX"
      },
      "source": [
        "To get the gradients w.r.t. the loss, first call `loss.backward()` and then access the `grad` property of the relevant model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7chAT_iNf9vf"
      },
      "source": [
        "# Calculate gradients\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcypHRQ1gls5"
      },
      "source": [
        "*Side-note:* If you are interested in understanding in detail how `backward()` works, I recommend [this blog post](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95).\n",
        "\n",
        "Since we are going to need both the layer activations and layer-wise gradients, lets wrap that up in a single function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btQ9Lid5glGN"
      },
      "source": [
        "def get_layer_data(model):\n",
        "  gradients = []\n",
        "  layer_names = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "      if param.requires_grad and name.startswith('W'):\n",
        "          layer_names.append(name)\n",
        "          gradients.append(param.grad)\n",
        "\n",
        "  activations = model.activations()\n",
        "\n",
        "  return layer_names, activations, gradients\n",
        "\n",
        "layer_names, activations, gradients = get_layer_data(model)\n",
        "print('layer_names',layer_names)\n",
        "print('Activation list length:',len(activations))\n",
        "print('Activations shape layer 1:',activations[0].shape)\n",
        "print('Gradient list length:',len(gradients))\n",
        "print('Gradient shape layer 1:',gradients[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAz2dtpSiA3m"
      },
      "source": [
        "Note that we are not considering the biases in this Lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VALKE9BohZIs"
      },
      "source": [
        "###Questions\n",
        "1. Why is the shape of the activations of the first layer [256,16]?\n",
        "2. Why is the shape of the gradients of the first layer [3072,16]?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaV8gN64ihO0"
      },
      "source": [
        "###3.5 Getting layer stats\n",
        "We can create a simple function that calculates the mean and the variance of the layer activations and gradients. For the gradients, we are interested in the *gradient flow*, so we will take the mean of the *absolute* gradient values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IczlmBWRGkte"
      },
      "source": [
        "def get_layer_stats(x,absolute=False):\n",
        "  avg = []\n",
        "  std = []\n",
        "  for layer in range(len(x)):\n",
        "    if absolute:\n",
        "      avg.append(x[layer].abs().mean().detach().cpu().numpy())\n",
        "    else:\n",
        "      avg.append(x[layer].mean().detach().cpu().numpy())\n",
        "\n",
        "    std.append(x[layer].std().detach().cpu().numpy())\n",
        "\n",
        "  return avg, std\n",
        "\n",
        "activation_mean, activation_std = get_layer_stats(activations)\n",
        "gradient_mean, gradient_std = get_layer_stats(gradients,absolute=True)\n",
        "\n",
        "print('activation_mean',activation_mean)\n",
        "print('activation_std',activation_std)\n",
        "print('gradient_mean',gradient_mean)\n",
        "print('gradient_std',gradient_std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiiKrxrGkH8w"
      },
      "source": [
        "###3.6 Visualizing layer stats\n",
        "These numbers are perhaps not very easy to interpret, so lets make a function that allows to plot histograms (and display mean and standard deviation at the same time).\n",
        "\n",
        "Note that you can specifiy a fixed range of x values (like -1 to 1). If set to `None` the x range is adapted to each individual plot. So notice the values on the x-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2il8nJOukZfk"
      },
      "source": [
        "def plot_hist(hs,xrange=(-1,1),avg=None,sd=None):\n",
        "  plt.figure(figsize=(20,3))\n",
        "  for layer in range(len(hs)):\n",
        "    plt.subplot(1,len(hs),layer+1)\n",
        "    activations = hs[layer].detach().cpu().numpy().flatten()\n",
        "    plt.hist(activations, bins=20, range=xrange)\n",
        "\n",
        "    title = 'Layer ' + str(layer+1)\n",
        "    if avg:\n",
        "      title += '\\n' + \"mean {0:.2f}\".format(avg[layer])\n",
        "    if sd:\n",
        "      title += '\\n' + \"std {0:.4f}\".format(sd[layer])\n",
        "\n",
        "    plt.title(title)\n",
        "\n",
        "print('Gradients:\\n')\n",
        "plot_hist(gradients,xrange=None,avg=gradient_mean,sd=gradient_std)\n",
        "plt.show()\n",
        "\n",
        "print('Activations:\\n')\n",
        "plot_hist(activations,xrange=None,avg=activation_mean,sd=activation_std)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Infpb-u-l4Z2"
      },
      "source": [
        "### Questions\n",
        "Carefully inspect the histograms above, and remember that the x-axes can be scaled differently.\n",
        "1. What happens to the gradients across layers? How are they distributed? Do they get smaller or larger towards the end of the network?\n",
        "2. What about the activations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9GH7xMKndnB"
      },
      "source": [
        "###3.7 Wrapping up\n",
        "Finally, let's combine all of the above into just one function that we can call. To do this we first define a helper function `get_stats` which takes a model as input, runs a batch trough the model, and calculates all the stats that we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IpKGmjVoJ_6"
      },
      "source": [
        "def get_stats(model,dataloader=trainloader):\n",
        "\n",
        "  dataiter = iter(dataloader)\n",
        "  images, labels = next(dataiter)\n",
        "  images = images.cuda()\n",
        "  labels = labels.cuda()\n",
        "\n",
        "  scores = model(images)  # predictions\n",
        "  loss = loss_func(scores, labels)\n",
        "  acc = accuracy(scores,labels)\n",
        "\n",
        "  # Calculate gradients\n",
        "  loss.backward()\n",
        "\n",
        "  layer_names, activations, gradients = get_layer_data(model)\n",
        "\n",
        "  activation_mean, activation_std = get_layer_stats(activations)\n",
        "  gradient_mean, gradient_std = get_layer_stats(gradients,absolute=True)\n",
        "\n",
        "  stats = {'loss': loss,\n",
        "           'accuracy': acc,\n",
        "           'names': layer_names,\n",
        "           'grads': gradients,\n",
        "           'activations': activations,\n",
        "           'activation_mean': activation_mean,\n",
        "           'activation_std': activation_std,\n",
        "           'gradient_mean': gradient_mean,\n",
        "           'gradient_std': gradient_std\n",
        "           }\n",
        "\n",
        "  return stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBT8SJstoTpG"
      },
      "source": [
        "Here is the function to display the stats:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcy9gvEsx8bv"
      },
      "source": [
        "def show_stats(stats):\n",
        "  print('loss',stats['loss'].detach().cpu().numpy())\n",
        "  print('accuracy',stats['accuracy'].detach().cpu().numpy(),'\\n')\n",
        "\n",
        "  print('Gradients:\\n')\n",
        "  print(' (note that we use the mean of the absolute gradient values to quantify gradient flow\\n')\n",
        "  #[print(name, avg, std) for name, avg, std in iter(zip(stats['names'],stats['gradient_mean'],stats['gradient_std']))]\n",
        "\n",
        "  plot_hist(stats['grads'],xrange=None,avg=stats['gradient_mean'],sd=stats['gradient_std'])\n",
        "  plt.show()\n",
        "\n",
        "  print('Activations:\\n')\n",
        "  #[print(name, avg, std) for name, avg, std in iter(zip(stats['names'],stats['activation_mean'],stats['activation_std']))]\n",
        "\n",
        "  plot_hist(stats['activations'],xrange=None,avg=stats['activation_mean'],sd=stats['activation_std'])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN8DoAeVoh5E"
      },
      "source": [
        "So from now on we just make these two function calls:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9YmjWw8Erer"
      },
      "source": [
        "# Calculate stats\n",
        "stats = get_stats(model)\n",
        "\n",
        "# Show stats\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zksMi4QUpZnP"
      },
      "source": [
        "## 4. Task 1: tanh activation\n",
        "Your task is very simple: Add tanh activation function to our base model and explain what you see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYTdu6g4wyQ-"
      },
      "source": [
        "def tanh(x):\n",
        "  # Your code goes here\n",
        "\n",
        "model_tanh = BaseModel(activation_function=tanh).cuda()\n",
        "stats = get_stats(model_tanh)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRUIID0JqZ2t"
      },
      "source": [
        "### Questions:\n",
        "1. How are the gradients and activations distributed across layers?\n",
        "2. Is there gradient flow through the entire network?\n",
        "3. Do the activations have similar variance across layers?\n",
        "4. Are we within the linear range of tanh, within the saturated range, or a little of both?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_JhLhvprXIT"
      },
      "source": [
        "##5. Task 2: Increase values of initial weights\n",
        "In our base model the weights are initialized with random values from a standard normal disitrbution (mean 0 and standard deviation 1). Let's increase the std to 10 and see what happens. Note that we keep using tanh acivation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5i3_DabFXbN"
      },
      "source": [
        "def upscale(x): return x * 10\n",
        "\n",
        "model_saturated = BaseModel(normalizer=upscale, activation_function=tanh).cuda()\n",
        "stats = get_stats(model_saturated)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZcQmnrlr7WQ"
      },
      "source": [
        "### Questions:\n",
        "1. How are the gradients and activations distributed across layers?\n",
        "2. Is there gradient flow through the entire network?\n",
        "3. Do the activations have similar variance across layers?\n",
        "4. Are we within the linear range of tanh, within the saturated range, or a little of both?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUiAszp6sK0S"
      },
      "source": [
        "##6. Task 3: Implement Xavier initialization\n",
        "Your task is to implement Xavier initialization by providing a new normalizer function. Remember that the normalizer takes a weight matrix (`x`) as input, which contains random values with zero mean and standard deviation 1. To change the standard deviation of `x` to some value `sd`, simply multiply `x` with `sd`.\n",
        "\n",
        "Note that there are different ways to implement Xavier. In the lecture slides, we normalize based on the number of columns of the weight matrix (often referred to as the *fan-in*). Others normalize based on the number of rows (*fan-out*). Others again normalize based on the sum of the fan-in and fan-out. For instance, [this is how PyTorch does it](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_normal_).\n",
        "\n",
        "**Do not use PyTorch's Xavier implementation - make your own :-)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOTVJkerzNEs"
      },
      "source": [
        "def xavier(x):\n",
        "  # Your code goes here\n",
        "\n",
        "model_xavier = BaseModel(normalizer=xavier, activation_function=tanh).cuda()\n",
        "stats = get_stats(model_xavier)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeB6m1H_tMIU"
      },
      "source": [
        "### Questions:\n",
        "1. How are the gradients and activations distributed across layers?\n",
        "2. Is there gradient flow through the entire network?\n",
        "3. Do the activations have similar variance across layers?\n",
        "4. Are we within the linear range of tanh, within the saturated range, or a little of both?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3bOqMwBta9S"
      },
      "source": [
        "##7. Task 4: ReLu and Kaiming initialization\n",
        "Your task is to change the activation function to ReLU. In this case, we know that Xavier initialization will not work, so you also need to implement Kaiming initialization.\n",
        "\n",
        "You can check PyTorch's Kaiming formula [here](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_) (again, recall that fan-in is the number of columns of the weight matrix).\n",
        "\n",
        "**Again, make our own implementation - do not PyTorch's**\n",
        "\n",
        "**Hint:** For ReLU you could use PyTorch's [`clamp`](https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh4udDWJEXop"
      },
      "source": [
        "def relu(x):\n",
        "  # Your code goes here\n",
        "\n",
        "def kaiming(x):\n",
        "  # Your code goes here\n",
        "\n",
        "model_kaiming = BaseModel(normalizer=kaiming, activation_function=relu).cuda()\n",
        "stats = get_stats(model_kaiming)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqhJnpJIugjr"
      },
      "source": [
        "### Questions:\n",
        "1. How are the gradients and activations distributed across layers?\n",
        "2. Is there gradient flow through the entire network?\n",
        "3. Do the activations have similar variance across layers?\n",
        "4. Do enough of the ReLUs get activated in all layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cExn9vfUuy_P"
      },
      "source": [
        "##8. Task 5: Batch normalization\n",
        "Finally, see if you can implement simple batch normalization (without the learnable parameters) by passing on a preprocessing function to our model. The input to the preprocess function is the result of multiplying layer input `x` with weight matrix `W` and adding the biases: That is: `y = Wx+b`. The output should be `z = (y - mean(y)) / std(y)`.\n",
        "\n",
        "Hint: You can call `.mean()` and `.std()` on a tensor to get the mean and standard deviation, respectively. Remember to specify the correct axis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOkgJsggBdzd"
      },
      "source": [
        "def batch_norm(y):\n",
        "  # Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWQt9kr70psB"
      },
      "source": [
        "# Check your results\n",
        "y = torch.randn(512,512)*10 + 2\n",
        "print('before', y.mean(),y.std())\n",
        "z = batch_norm(y)\n",
        "print('after', z.mean(),z.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your implementation is correct, z should have mean 0 and standard deviation 1."
      ],
      "metadata": {
        "id": "KiodIuGKtflo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNymTnEoDP-4"
      },
      "source": [
        "model_batchnorm = BaseModel(normalizer=kaiming, activation_function=relu,preprocess_function=batch_norm).cuda()\n",
        "stats = get_stats(model_batchnorm)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrsmRwouwO4a"
      },
      "source": [
        "### Questions:\n",
        "1. How are the gradients and activations distributed across layers?\n",
        "2. Is there gradient flow through the entire network?\n",
        "3. Do the activations have similar variance across layers?\n",
        "4. Do enough of the ReLUs get activated in all layers?\n",
        "\n",
        "Compare to the results with ReLu and Kaiming (without batch norm).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "praCQ52q_LwQ"
      },
      "source": [
        "##9. Model training\n",
        "The code below can be used to train a model and monitor important stats as training progresses.\n",
        "\n",
        "The training is carried out by calling the `fit` function, which takes a model as input, as well as a function handle returning an optimizer. The base optimizer is just SGD with zero momentum.\n",
        "\n",
        "In addition, `lr` is the learning rate, `bs` the batch size, and `epochs` the number of epochs.\n",
        "\n",
        "(Note: the cpu/gpu copying and torch/numpy conversions could have been made nicer - sorry...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56_hDxQWHKWo"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "# Function handle that returns an optimizer - in this case just a simple SGD without momentum\n",
        "def base_optimizer(model,lr=0.1):\n",
        "    return optim.SGD(model.parameters(), lr=lr,momentum=0.)\n",
        "\n",
        "# Used to print gradients/activations as a function of time\n",
        "def print_history(history,title=''):\n",
        "  plt.figure()\n",
        "  history = np.asarray(history)\n",
        "  lines = []\n",
        "  labels = []\n",
        "  for i in range(history.shape[1]):\n",
        "    l, = plt.plot(history[:,i])\n",
        "    lines.append(l)\n",
        "    labels.append('Layer ' + str(i+1))\n",
        "  plt.legend(lines, labels, loc=(1, 0), prop=dict(size=14))\n",
        "  plt.title(title)\n",
        "\n",
        "# Function to fit a model\n",
        "def fit(model,\n",
        "        opt_func=base_optimizer,\n",
        "        lr=0.1,\n",
        "        bs=256,\n",
        "        epochs=2):\n",
        "\n",
        "  train_dl = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "  valid_dl = torch.utils.data.DataLoader(testset, batch_size=bs,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "  opt = opt_func(model,lr) # Initialize optimizer\n",
        "\n",
        "  train_loss_history = []\n",
        "  valid_loss_history = []\n",
        "  plot_time_train = []\n",
        "  plot_time_valid = []\n",
        "  activation_mean_history = []\n",
        "  gradient_mean_history = []\n",
        "\n",
        "  t = 1\n",
        "\n",
        "  # Get initial validation loss and accuracy\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    valid_acc = sum(accuracy(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n",
        "    valid_loss = sum(loss_func(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n",
        "    valid_loss_history.append(valid_loss.detach().cpu().numpy())\n",
        "    plot_time_valid.append(t)\n",
        "\n",
        "  # Train\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "      pred = model(xb.cuda())\n",
        "      loss = loss_func(pred, yb.cuda())\n",
        "\n",
        "      train_loss_history.append(loss.detach().cpu().numpy())\n",
        "      plot_time_train.append(t)\n",
        "      t += 1\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      layer_names, activations, gradients = get_layer_data(model)\n",
        "      activation_mean, activation_std = get_layer_stats(activations)\n",
        "      gradient_mean, gradient_std = get_layer_stats(gradients,absolute=True)\n",
        "      activation_mean_history.append(activation_mean)\n",
        "      gradient_mean_history.append(gradient_mean)\n",
        "\n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "\n",
        "    # Validation loss and accuracy\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_acc = sum(accuracy(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n",
        "        valid_loss = sum(loss_func(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n",
        "        valid_loss_history.append(valid_loss.detach().cpu().numpy())\n",
        "        plot_time_valid.append(t-1)\n",
        "        if epoch == epochs-1:\n",
        "          print('validation loss',valid_loss.detach().cpu().numpy())\n",
        "          print('validation accuracy', valid_acc.detach().cpu().numpy())\n",
        "\n",
        "  # Summary\n",
        "  plt.figure()\n",
        "  plt.plot(plot_time_train,train_loss_history)\n",
        "  plt.plot(plot_time_valid,valid_loss_history)\n",
        "  plt.title('Loss')\n",
        "  print_history(activation_mean_history,'Layer activations (mean)')\n",
        "  print_history(gradient_mean_history,'Layer gradients (mean)')\n",
        "  print('train loss',loss_func(model(xb.cuda()), yb.cuda()).detach().cpu().numpy())\n",
        "  print('train accuracy', accuracy(model(xb.cuda()), yb.cuda()).detach().cpu().numpy())\n",
        "  print('\\n')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V0LMYxaN85v"
      },
      "source": [
        "###9.1 Training the base model\n",
        "Let's train the base model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VMx9bxyEPrs"
      },
      "source": [
        "model = BaseModel().cuda() # Re-initialize weights\n",
        "fit(model,lr=1e-8)\n",
        "stats = get_stats(model)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXYnX37XO8bG"
      },
      "source": [
        "###Questions\n",
        "1. What is the validation accuracy?\n",
        "2. What do the first three plots show?\n",
        "3. What happened to the gradient flow over time? Did it increase, decrease, or stay constant?\n",
        "4. What happens if you increase the learning rate to 0.1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufjD_62jPQSy"
      },
      "source": [
        "###9.2 Training the tanh model\n",
        "Recall that the base model is entirely linear (i.e., no activation functions used). Let's see what happens if we add tanh activations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk0H5D2LCT3t"
      },
      "source": [
        "model_tanh = BaseModel(activation_function=tanh).cuda() # Re-initialize weights\n",
        "fit(model_tanh,lr=0.1)\n",
        "stats = get_stats(model_tanh)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkBQ4ZDMPhRv"
      },
      "source": [
        "###Questions\n",
        "1. Did results improve significantly?\n",
        "2. Are the activations within the linear range of tanh, within the saturated range, or a mix of both?\n",
        "3. What happens to the gradients over time?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPwnhZQhQNqd"
      },
      "source": [
        "###9.3 Could we possible do any worse?\n",
        "Yes! Let's use a std of 10 when initializing the weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPeBEzGEF2aN"
      },
      "source": [
        "model_saturated = BaseModel(normalizer=upscale, activation_function=tanh).cuda() # Re-initialize weights\n",
        "fit(model_saturated,lr=0.1)\n",
        "stats = get_stats(model_saturated)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6ESARbbQjO6"
      },
      "source": [
        "###Questions\n",
        "1. What happens to the loss over time? Does it decrease?\n",
        "2. It seems that there is almost no gradient signal. Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCApI-TCHKWq"
      },
      "source": [
        "###9.4 Xavier initialization\n",
        "Now, let's see what happens with Xavier initialization:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oufKaXqtHKWr"
      },
      "source": [
        "model_xavier = BaseModel(normalizer=xavier, activation_function=tanh).cuda()\n",
        "fit(model_xavier,lr=0.1)\n",
        "stats = get_stats(model_xavier)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhKplpxGR15s"
      },
      "source": [
        "###Questions\n",
        "1. What happened to the validation accuracy?\n",
        "2. What happens to the gradient signal over time? Does it vanish? Or is it maintained?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50C_gv8HSOl7"
      },
      "source": [
        "###9.5 ReLU and Kaiming initialization\n",
        "Does ReLU with Kaiming initialization perform better than tanh with Xavier initialization?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUzWSVU2EwrD"
      },
      "source": [
        "model_kaiming = BaseModel(normalizer=kaiming, activation_function=relu).cuda()\n",
        "fit(model_kaiming,lr=0.1)\n",
        "stats = get_stats(model_kaiming)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hDbVWHgBLlL"
      },
      "source": [
        "###Questions\n",
        "1. We do not end up at the same level of accuracy as with tanh and Xavier initialization. Why do you think that is?\n",
        "2. Could we fix it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQJdvWcsSjZK"
      },
      "source": [
        "###9.6 Batch normalization\n",
        "On average, results should become even better if we add batch normalization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ut46Hb6DrPW"
      },
      "source": [
        "model_batchnorm = BaseModel(normalizer=kaiming, activation_function=relu, preprocess_function=batch_norm).cuda()\n",
        "fit(model_batchnorm,lr=0.1)\n",
        "stats = get_stats(model_batchnorm)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Iiz96aS7iI"
      },
      "source": [
        "###Questions\n",
        "1. Compare the curves with and without batch normalization (i.e., section 9.5 and 9.6). Can you explain the differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzPSPmyrTeSw"
      },
      "source": [
        "###9.7 Decreasing the batch size\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXAtDU-ANFW6"
      },
      "source": [
        "bs=32\n",
        "model_batchsize = BaseModel(normalizer=kaiming, activation_function=relu,preprocess_function=batch_norm).cuda()\n",
        "fit(model_batchsize,lr=0.1,bs=bs)\n",
        "stats = get_stats(model_batchsize)\n",
        "show_stats(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuK5QM3AT5VY"
      },
      "source": [
        "###Questions\n",
        "1. What just happened to the three top-most curves? Why?\n",
        "2. Did your results get better, worse, or same?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOgnYtEUXlL"
      },
      "source": [
        "## 9. Optimizers (optional)\n",
        "Below I have defined a few other optimizers (read more here: https://pytorch.org/docs/stable/optim.html). Your task is to experiment with these.\n",
        "\n",
        "Suggestions:\n",
        "\n",
        "1. Start with `momentum_optimizer` and train the model with momenum 0.0, 0.9, 0.99 and 1.0. How does the optimizer behave for different choices of momentum?\n",
        "\n",
        "2. Move on to experiment with Adagrad or RMSprop. Notice any different behavior?\n",
        "\n",
        "3. Finally, try Adam with different learning rates. Theory says it shouldn't make a huge difference. Do your experiments confirm this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46WEppfOLVQj"
      },
      "source": [
        "#SGD + momentum\n",
        "def momentum_optimizer(model,lr=0.1):\n",
        "    return optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "def adagrad_optimizer(model,lr=0.1):\n",
        "  return optim.Adagrad(model.parameters(), lr=lr, lr_decay=0.0, weight_decay=0.0, initial_accumulator_value=0)\n",
        "\n",
        "def rmsprop_optimizer(model,lr=0.1):\n",
        "  return optim.RMSprop(model.parameters(), lr=lr, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
        "\n",
        "def adam_optimizer(model,lr=0.001):\n",
        "  return optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "model = BaseModel(normalizer=kaiming, activation_function=relu,preprocess_function=batch_norm).cuda()\n",
        "fit(model,opt_func=momentum_optimizer,lr=0.01,bs=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL_9mIyLB3_N"
      },
      "source": [
        "##10. Competition (optional)\n",
        "How high validation accuracy can you get in 5 epochs?\n",
        "\n",
        "Rules - you are allowed to:\n",
        "- modify the number of layers of the base model (parameter `L`).\n",
        "- modify the number of units in the hidden layers of the base model (parameter `N`).\n",
        "- choose between different activation functions and normalizers\n",
        "- use batch norm\n",
        "- change the batch size\n",
        "- use any optimizer and any optimization hyperparameters (e.g., learning rate)\n",
        "- and **no more than 5 epochs!**\n",
        "\n",
        "Think about your hyperparameter search strategy. What quick pre-experiments could you do before training the final model?\n",
        "\n",
        "You are not done, until you have reached at least 50% accuracy! ;-)"
      ]
    }
  ]
}