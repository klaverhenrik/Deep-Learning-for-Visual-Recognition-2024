{"cells":[{"cell_type":"markdown","metadata":{"id":"x1WDnqH5gw8R"},"source":["# Timeseries forecasting for weather prediction\n","\n","This lab is based on [this](https://keras.io/examples/timeseries/timeseries_weather_forecasting/) Keras tutorial. Our goal is to perform timeseries forecasting using a Recurrent Neural Network based on the [Long Short-Term Memory (LSTM)  block](https://builtin.com/data-science/recurrent-neural-networks-and-lstm).\n","\n","Your task is simple: See if you can figure out how this works! If you succeed, you should have a pretty good understanding of how RNNs work."]},{"cell_type":"markdown","metadata":{"id":"Gc_gdTtSgw8S"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SElVsMvRgw8S"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import keras"]},{"cell_type":"markdown","metadata":{"id":"GMLsXG6Kgw8T"},"source":["## Climate Data Time-Series\n","\n","We will be using Jena Climate dataset recorded by the\n","[Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/).\n","The dataset consists of 14 features such as temperature, pressure, humidity etc, recorded once per 10 minutes using a weather station.\n","\n","The table below shows the column names, their value formats, and their description.\n","\n","Index| Features      |Format             |Description\n","-----|---------------|-------------------|-----------------------\n","1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n","2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n","3    |T (degC)       |-8.02              |Temperature in Celsius\n","4    |Tpot (K)       |265.4              |Temperature in Kelvin\n","5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n","6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n","7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n","8    |VPact (mbar)   |3.11               |Vapor pressure\n","9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n","10   |sh (g/kg)      |1.94               |Specific humidity\n","11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n","12   |rho (g/m ** 3) |1307.75            |Airtight\n","13   |wv (m/s)       |1.03               |Wind speed\n","14   |max. wv (m/s)  |1.75               |Maximum wind speed\n","15   |wd (deg)       |152.3              |Wind direction in degrees"]},{"cell_type":"markdown","source":["Download dataset and load it into a [Pandas dataframe](https://www.datacamp.com/tutorial/pandas-tutorial-dataframe-python)."],"metadata":{"id":"oWz6qHfgAyh7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8OlMhD0gw8T"},"outputs":[],"source":["from zipfile import ZipFile\n","\n","uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n","zip_path = keras.utils.get_file(origin=uri, fname=\"jena_climate_2009_2016.csv.zip\")\n","zip_file = ZipFile(zip_path)\n","zip_file.extractall()\n","csv_path = \"jena_climate_2009_2016.csv\"\n","df = pd.read_csv(csv_path)"]},{"cell_type":"markdown","metadata":{"id":"x0pVvIFEgw8U"},"source":["## Raw Data Visualization\n","\n","To give us a sense of the data we are working with, each feature has been plotted below.\n","This shows the distinct pattern of each feature over the time period from 2009 to 2016.\n","Perhaps you will notice that there seems to be some anomalies or outliers in the data. These will be ignored here, but in general they should addressed, for instance through [data cleaning](https://en.wikipedia.org/wiki/Data_cleansing)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWmftMj5gw8U"},"outputs":[],"source":["titles = [\n","    \"Pressure\",\n","    \"Temperature\",\n","    \"Temperature in Kelvin\",\n","    \"Temperature (dew point)\",\n","    \"Relative Humidity\",\n","    \"Saturation vapor pressure\",\n","    \"Vapor pressure\",\n","    \"Vapor pressure deficit\",\n","    \"Specific humidity\",\n","    \"Water vapor concentration\",\n","    \"Airtight\",\n","    \"Wind speed\",\n","    \"Maximum wind speed\",\n","    \"Wind direction in degrees\",\n","]\n","\n","feature_keys = [\n","    \"p (mbar)\",\n","    \"T (degC)\",\n","    \"Tpot (K)\",\n","    \"Tdew (degC)\",\n","    \"rh (%)\",\n","    \"VPmax (mbar)\",\n","    \"VPact (mbar)\",\n","    \"VPdef (mbar)\",\n","    \"sh (g/kg)\",\n","    \"H2OC (mmol/mol)\",\n","    \"rho (g/m**3)\",\n","    \"wv (m/s)\",\n","    \"max. wv (m/s)\",\n","    \"wd (deg)\",\n","]\n","\n","colors = [\n","    \"blue\",\n","    \"orange\",\n","    \"green\",\n","    \"red\",\n","    \"purple\",\n","    \"brown\",\n","    \"pink\",\n","    \"gray\",\n","    \"olive\",\n","    \"cyan\",\n","]\n","\n","date_time_key = \"Date Time\"\n","\n","def show_raw_visualization(data):\n","    time_data = data[date_time_key]\n","    fig, axes = plt.subplots(\n","        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n","    )\n","    for i in range(len(feature_keys)):\n","        key = feature_keys[i]\n","        c = colors[i % (len(colors))]\n","        t_data = data[key]\n","        t_data.index = time_data\n","        t_data.head()\n","        ax = t_data.plot(\n","            ax=axes[i // 2, i % 2],\n","            color=c,\n","            title=\"{} - {}\".format(titles[i], key),\n","            rot=25,\n","        )\n","        ax.legend([titles[i]])\n","    plt.tight_layout()\n","\n","\n","show_raw_visualization(df)\n"]},{"cell_type":"markdown","source":["## Goal\n","Our goal is to train a model that can predict the temperature 12 hours into the future, given weather station measurements from the past 120 hours (5 days).\n","\n","In the context of what was covered in Lecture 11, this mostly resembles **Sequence classification**, where the goal is to predict a label given a sequence of inputs. The only difference here is that we will be doing regression instead of classification. So we could call it **sequence regression**, where the goal is to predict a real-valued number (the temperature) given a sequence of inputs (past measurements from the weather station)."],"metadata":{"id":"h7AJ0XQAl77q"}},{"cell_type":"markdown","source":["## Data Preprocessing\n","\n","Before we can build and train our model, we will need to do some preprocessing of our data.\n","\n","### Feature selection\n","As stated above, there are a total of 14 features but as some of them are strongly correlated (for instance, Relative Humidity and Specific Humidity), we will only be using a subset of them. Specifically, we will be using the following 7:\n","\n","*Pressure, Temperature, Saturation vapor pressure, Vapor pressure deficit, Specific humidity, Airtight, Wind speed*."],"metadata":{"id":"VBWYN0a4xfYQ"}},{"cell_type":"code","source":["# Feature selection\n","print(\n","    \"The selected features are:\",\n","    \", \".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]),\n",")\n","selected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]]\n","features = df[selected_features]\n","features.index = df[date_time_key] # Make timestamp the index (this is Pandas-stuff)"],"metadata":{"id":"lx2OHghvx8qf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Feature normalization\n","Since every feature has values with\n","varying ranges, we do normalization to confine feature values to a range of `[0, 1]` before training our RNN. We do this by subtracting the mean and dividing by the standard deviation of each feature."],"metadata":{"id":"rg8yht-LyBHV"}},{"cell_type":"code","source":["# Utility function to perform data normalization\n","def normalize(data):\n","    data_mean = data.mean(axis=0)\n","    data_std = data.std(axis=0)\n","    return (data - data_mean) / data_std\n","\n","# Feature normalization\n","features = normalize(features.values)\n","features = pd.DataFrame(features)"],"metadata":{"id":"Tb7KLnY5yJPZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train / validation split\n","There are approximately 450.000 observations, of which we will be using initial 71.5 % for training and the remaining 28.5 % for validation."],"metadata":{"id":"rMVYiKFhyR1m"}},{"cell_type":"code","source":["split_fraction = 0.715\n","train_split = int(split_fraction * int(df.shape[0]))\n","\n","# Split inputs (x) into train and validation\n","train_data = features.loc[0 : train_split - 1]\n","val_data = features.loc[train_split:]"],"metadata":{"id":"BOM5B9RYyhhT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Downsampling and creating input / output pairs\n","Observations are recorded every 10 minutes (6 times per hour). We will resample one point per hour since no drastic change is expected within 60 minutes.\n","\n","To train and test the model we need to create input / output pairs. The input is a sequence $[x_1, x_2, ..., x_{120}]$ of 120 past measurements from the weather station (5 days = 120 hours). The output (or the target) $y$ is the temperate 12 timestamps (12 hours) into the future, relative to the timestamp of the last measurement $x_{120}$.\n","\n","Let's do both of these steps for the training data:\n"],"metadata":{"id":"cCrndFJly9Ad"}},{"cell_type":"code","source":["# Downsample\n","step = 6\n","train_data = train_data.values[::step]\n","\n","x_train = train_data\n","\n","# The outputs (y) start from the 132nd observation (120 + 12)\n","past = 120  # Number of samples in input (x)\n","future = 12 # Number of samples to look into the future to get the output (y)\n","start = past + future\n","y_train = train_data[start:,1] # 1 is the index of the Temperature that we want to predict.\n","\n","print('x_train.shape', x_train.shape)\n","print('y_train.shape', y_train.shape)"],"metadata":{"id":"y0ucasGizmAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The [`timeseries_dataset_from_array`](https://keras.io/api/data_loading/timeseries/) function takes in a sequence of datapoints gathered at equal intervals, along with time series parameters such as length of the sequences/windows, spacing between two sequence/windows, etc., to produce batches of sub-timeseries inputs ($[x_1, x_2, ..., x_{120}]$) and targets ($y$) sampled from the main timeseries."],"metadata":{"id":"xUlIUc3X0eDs"}},{"cell_type":"code","source":["batch_size = 256\n","sequence_length = 120\n","dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n","    x_train,\n","    y_train,\n","    sequence_length=sequence_length,\n","    batch_size=batch_size,\n",")\n","\n","# Print shapes (256 is the batch size)\n","for batch in dataset_train.take(1):\n","    inputs, targets = batch\n","\n","print(\"Input shape:\", inputs.numpy().shape)\n","print(\"Target shape:\", targets.numpy().shape)"],"metadata":{"id":"BZM1W-nt0P1h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PvDqvrB0gw8W"},"source":["Let's build the validation set the same way:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9epRQ4N7gw8W"},"outputs":[],"source":["# Downsample\n","val_data = val_data.values[::step]\n","x_val = val_data\n","y_val = val_data[start:,1] # Again 1 is the index of the temperature\n","dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n","    x_val,\n","    y_val,\n","    sequence_length=sequence_length,\n","    batch_size=batch_size,\n",")\n","\n","# Print shapes (256 is the batch size)\n","for batch in dataset_val.take(1):\n","    inputs, targets = batch\n","print(\"Input shape:\", inputs.numpy().shape)\n","print(\"Target shape:\", targets.numpy().shape)"]},{"cell_type":"markdown","source":["## Model\n","Let's build a simple RNN. To address vanishing gradients we will be using the LSTM block.\n","\n","Verify for yourself that the input $[x_1, x_2, ..., x_{120}]$ has shape $120\\times7$. The output ($y$) is just is scalar, representing the temperature 12 timesteps into the future as explained above.\n","\n","**Note:** The model might initially look like any fully connected neural network, but recall what is going on under the hood. The LSTM processes the input sequence one token at a time and outputs a hidden state for every token:\n","\n","$h_t = f_{LSTM}(h_{t-1}, x_t)$\n","\n","The last hidden state $h_{120}$ then goes into a dense layer (a fully connected NN) that predicts the temperature.\n","\n","$y_{predicted} = f_{DENSE}(h_{120})$"],"metadata":{"id":"aRVe2yQE2h3N"}},{"cell_type":"code","source":["inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n","lstm_out = keras.layers.LSTM(32)(inputs)\n","outputs = keras.layers.Dense(1)(lstm_out)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","model.summary()"],"metadata":{"id":"EwFicdY722CN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4CZ97Zy-gw8W"},"source":["## Training\n","(This will take a few minutes...).\n","\n","Note that since we are doing regression, we are using the mean squared error loss.\n","\n","We'll use EarlyStopping callback to interrupt training when the validation loss is not longer improving."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsVKqZ1ggw8W"},"outputs":[],"source":["learning_rate = 0.001\n","epochs = 10\n","\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NHVICS1gw8W"},"outputs":[],"source":["es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n","\n","history = model.fit(\n","    dataset_train,\n","    epochs=epochs,\n","    validation_data=dataset_val,\n","    callbacks=[es_callback],\n",")"]},{"cell_type":"markdown","metadata":{"id":"wNobOKfzgw8W"},"source":["We can visualize the loss with the function below. After one point, the loss stops\n","decreasing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kiPT7ZKagw8W"},"outputs":[],"source":["def visualize_loss(history, title):\n","    loss = history.history[\"loss\"]\n","    val_loss = history.history[\"val_loss\"]\n","    epochs = range(len(loss))\n","    plt.figure()\n","    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n","    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n","    plt.title(title)\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.show()\n","\n","\n","visualize_loss(history, \"Training and Validation Loss\")"]},{"cell_type":"markdown","metadata":{"id":"mHs5goPDgw8W"},"source":["## Prediction\n","\n","The trained model above is now able to make predictions for 5 sets of values from the\n","validation set.\n","\n","The plots below show the temperature 120 timestamps (hours) into the past from the point of reference, and then it shows the predicted temperature and the true temperature 12 timestamps (hours) into the future.\n","\n","**Note:** That because of the normalization the predictions are not the actual temperatures. I will leave it as an exercise for you to de-normalize the predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GzrrIUfgw8W"},"outputs":[],"source":["def show_plot(plot_data, delta, title):\n","    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n","    marker = [\".-\", \"rx\", \"go\"]\n","    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n","    if delta:\n","        future = delta\n","    else:\n","        future = 0\n","\n","    plt.title(title)\n","    for i, val in enumerate(plot_data):\n","        if i:\n","            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n","        else:\n","            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n","    plt.legend()\n","    plt.xlim([time_steps[0], (future + 5) * 2])\n","    plt.xlabel(\"Time-Step\")\n","    plt.show()\n","    return\n","\n","\n","for x, y in dataset_val.take(5):\n","    show_plot(\n","        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n","        12,\n","        \"Single Step Prediction\",\n","    )"]},{"cell_type":"code","source":[],"metadata":{"id":"cxkIbntX645e"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/timeseries/ipynb/timeseries_weather_forecasting.ipynb","timestamp":1731350178494}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}