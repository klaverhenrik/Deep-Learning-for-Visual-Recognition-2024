{"cells":[{"cell_type":"markdown","id":"df7483c6","metadata":{"id":"df7483c6"},"source":["# Attention Basics\n","In this lab we will explore the Attention mechanism.\n","\n","**Remember** to enable GPU.\n","\n","## Using dot-product attention to look up and retrieve information from a database\n","Recall that dot-product attention is defined as\n","\n","> $\\text{softmax}(QK^T)V$\n","\n","where $Q$ holds the queries (or query if there is only one), $K$ holds the keys, and $V$ contains the values.\n","\n","In the database analogy, the attention weights\n","\n","> $\\text{softmax}(QK^T)$\n","\n","are used to perform the **lookup**; the attention weights tell us how much we want of each sample in the database.\n","\n","Multiplying $\\text{softmax}(QK^T)$ with $V$ is the **retrieval** step, where we retrieve a weighted sum of the elements in $V$.\n","\n","In the following tasks we will learn how to actually apply this principle to look up and retrieve data in a small database of images."]},{"cell_type":"markdown","source":["## 1. Data download\n","We will be using the MNIST dataset."],"metadata":{"id":"3w8p3yNQjR2p"},"id":"3w8p3yNQjR2p"},{"cell_type":"code","execution_count":null,"id":"3e8491bb","metadata":{"id":"3e8491bb"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision\n","import torch.utils.data.dataloader as dataloader\n","\n","import torch.nn.functional as F\n","\n","from tqdm.notebook import trange, tqdm"]},{"cell_type":"code","source":["# Define the root directory of the dataset\n","data_set_root = \".\"\n","\n","# Define transformations to be applied to the dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Convert images to PyTorch tensors\n","    transforms.Normalize([0.5], [0.5])  # Normalize image data with mean 0.5 and standard deviation 0.5\n","])\n","\n","# Load the MNIST dataset\n","dataset = datasets.MNIST(data_set_root, train=True, download=True, transform=transform);"],"metadata":{"id":"B6UUHR71jWN3"},"id":"B6UUHR71jWN3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Create database of images\n","Here, we build a small \"database\" of 100 randomly selected images. After that we will use attention to look up and retrieve samples from the database."],"metadata":{"id":"-1RtZyuqkMVu"},"id":"-1RtZyuqkMVu"},{"cell_type":"code","execution_count":null,"id":"cd054b40","metadata":{"id":"cd054b40"},"outputs":[],"source":["# Make it deterministic\n","torch.manual_seed(42)\n","\n","# Specify the number of examples to select randomly\n","num_of_examples = 100\n","\n","# Randomly select indices from the dataset\n","rand_perm = torch.randperm(dataset.data.shape[0])[:num_of_examples]\n","\n","# Extract and concatenate the images of randomly selected examples into a tensor\n","# These are the values from which we want to retrieve.\n","image_database = torch.cat([dataset.__getitem__(i)[0].reshape(1, -1) for i in rand_perm])\n","\n","print(\"Shape of image_database\", image_database.detach().numpy().shape)"]},{"cell_type":"markdown","source":["The `image_database` has shape $100 \\times 784$ and holds the 100 selected images, flattened into 784-length vectors.\n","\n","Let's visualise the images stored in it:"],"metadata":{"id":"rGPY21Bckz9q"},"id":"rGPY21Bckz9q"},{"cell_type":"code","execution_count":null,"id":"d109d61c","metadata":{"id":"d109d61c"},"outputs":[],"source":["out = torchvision.utils.make_grid(image_database.reshape(-1, 1, 28, 28), 10, normalize=True, pad_value=0.5)\n","_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"]},{"cell_type":"markdown","source":["## 3. Hard attention\n","The simplest form of attention is **hard attention**, where we always select exactly one element from the database. One way to implement hard attention (using dot-product attention) is to ensure all queries and keys are unique one-hot vectors (why?).\n","\n","How do we implement this in practice? One way would be as follows:\n","- Assign a unique one-hot vector to each sample in the database. These will constitute the rows of the key matrix $K$.\n","- The query $Q$ is also going to be a one-hot vector (note that in the example below we have a single query, meaning that $Q$ is a vector. In general, you can have multiple queries, and in that case $Q$ is a matrix).\n","- Given the query $Q$, we can match it against all keys by calculating the matrix product $QK^T$.\n","- Since $K$ consists of unique one-hot vectors, this will result in *exactly* one match (i.e., there is exactly one row $K_i$ for which $Q \\cdot K_i=1$; for all other rows $j \\neq i$ we will have $Q \\cdot K_j=0$).\n","- Hence, the output of $QK^T$ is another one-hot vector where the 1-element is at the index corresponding to $Q$'s location in the database.\n","- The product $QK^TV$ will retrieve that element from the database.\n","- In order to retrieve the images from the database, we set $V$ = `image_database`.\n","\n","**Note:** we don't need to apply the softmax here, because $QK^T$ is already normalized (because it is a one-hot vector, the elements already sum to 1).\n","\n","Let's implement this:"],"metadata":{"id":"peMdwlJIqBDZ"},"id":"peMdwlJIqBDZ"},{"cell_type":"code","execution_count":null,"id":"eacd199e","metadata":{"id":"eacd199e"},"outputs":[],"source":["# We arbitrarily select the 10'th sample in the database as our query\n","# and use the corresponding one-hot encoding as the query (Q)\n","q_index = 10\n","Q = F.one_hot(torch.tensor([q_index]), num_of_examples)\n","print(\"Shape of query vector (Q):\", Q.detach().numpy().shape)\n","\n","# Lets visualise the image at this index\n","plt.figure(figsize = (3,3))\n","_ = plt.imshow(image_database[q_index].reshape(28, 28).numpy(), cmap=\"gray\")"]},{"cell_type":"code","execution_count":null,"id":"ec91f0c1","metadata":{"id":"ec91f0c1"},"outputs":[],"source":["# The key matrix (K) will consist of unique one-hot vectors, one for every image\n","# in our dataset\n","K = F.one_hot(torch.arange(num_of_examples), num_of_examples)\n","\n","# We already know the the query is at index 10 (q_index).\n","# Let's make it a little bit harder by randomly shuffling the keys.\n","rand_perm = torch.randperm(num_of_examples)\n","K = K[rand_perm]\n","\n","# The keys and values must match, so we need to shuffle the values as well.\n","# (Recall that the values correspond to the image_database)\n","V = image_database[rand_perm]\n","\n","print(\"Shape of key matrix (K)\", K.detach().numpy().shape)\n","print(\"Shape of value matrix (V)\", V.detach().numpy().shape)"]},{"cell_type":"markdown","source":["### 3.1 Question\n","- Why do $K$ and $V$ have the shapes that they have?"],"metadata":{"id":"B7ZeR3MhcGqL"},"id":"B7ZeR3MhcGqL"},{"cell_type":"markdown","source":["Due to the shuffling we no longer know at which index the query is located in the database.\n","\n","Let's perform the **lookup** (i.e, $QK^T$):"],"metadata":{"id":"VskKjS1Tp0XL"},"id":"VskKjS1Tp0XL"},{"cell_type":"code","execution_count":null,"id":"b24751fc","metadata":{"id":"b24751fc"},"outputs":[],"source":["# Multiply our query with the keys\n","alignment_scores = torch.mm(Q, K.t()).float()\n","\n","# Print to confirm that the result is a one-hot vector\n","print(alignment_scores)"]},{"cell_type":"markdown","source":["### 3.2 Question\n","- You should see that `alignment_scores` is a one-hot vector with a 1 at the index at which the query is located in the database. Why?"],"metadata":{"id":"mIzhlqhctBr8"},"id":"mIzhlqhctBr8"},{"cell_type":"markdown","source":["We can now perform the **retrieval** be multiplying $QK^T$ with $V$."],"metadata":{"id":"J2jqblKhcvAG"},"id":"J2jqblKhcvAG"},{"cell_type":"code","source":["# Perform matrix multiplication between the resulting index map and the randomly shuffled dataset\n","output = torch.mm(alignment_scores, V)\n","\n","# Visualize the image at the specified index\n","plt.figure(figsize=(3, 3))\n","_ = plt.imshow(output.reshape(28, 28).numpy(), cmap=\"gray\")"],"metadata":{"id":"w7camasxpAvr"},"id":"w7camasxpAvr","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In summary, we used hard attention to find and retrieve the query image after shuffling the database."],"metadata":{"id":"0aOqTDabn2fu"},"id":"0aOqTDabn2fu"},{"cell_type":"markdown","id":"7460e23c","metadata":{"id":"7460e23c"},"source":["## 4. Soft attention: Quering the database using random vectors\n","What happens if we replace the one-hot vectors with random vectors?\n","\n","In that case, the lookup\n","\n","> $\\text{softmax}(QK^T)$\n","\n","will result in a set of non-zero weights. In practise, this means that when we do the retrieval\n","\n","> $\\text{softmax}(QK^T)V$\n","\n","we are going to get some weighted average of **all** the elements in $V$.\n","\n","Let's verify this."]},{"cell_type":"code","execution_count":null,"id":"a672ca3c","metadata":{"id":"a672ca3c"},"outputs":[],"source":["# Define the embedding size for each of the vectors\n","vec_size = 4\n","\n","# Create a random query vector\n","Q = torch.randn(1, vec_size)\n","\n","# Create a random key vectors (one for each sample in the database)\n","K = torch.randn(num_of_examples, vec_size)\n","\n","V = image_database\n","\n","# Lookup\n","alignment_scores = torch.mm(Q, K.t()).float()\n","attention_weights = F.softmax(alignment_scores, 1)\n","\n","# Retrieval\n","output = torch.mm(attention_weights, V)"]},{"cell_type":"markdown","source":["Let's visualize the result."],"metadata":{"id":"XBt6a3YUqWRG"},"id":"XBt6a3YUqWRG"},{"cell_type":"code","execution_count":null,"id":"236c4cea","metadata":{"id":"236c4cea"},"outputs":[],"source":["plt.figure(figsize = (3,3))\n","_ = plt.imshow(output.reshape(28, 28).numpy(), cmap=\"gray\")"]},{"cell_type":"markdown","source":["### 4.1 Question\n","- You should see that the retrieved image is mostly blurry (otherwise run the code block again). What is the reason for this (hint: think weighted average)."],"metadata":{"id":"S7FBrDk2wdNe"},"id":"S7FBrDk2wdNe"},{"cell_type":"markdown","source":["To see how much is extracted from each of the 100 samples in the database, you can plot the attention weights:"],"metadata":{"id":"rDk9Iz1MdTvx"},"id":"rDk9Iz1MdTvx"},{"cell_type":"code","source":["_ = plt.plot(attention_weights.squeeze().detach().numpy())"],"metadata":{"id":"6f1AIUZRq78B"},"id":"6f1AIUZRq78B","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"a76a9d7e","metadata":{"id":"a76a9d7e"},"source":["## 5. Multiple Queries\n","Until now we have queried the database with a single vector. We can easily perform multiple queries at the same time by putting the queries into matrix $Q$. In the example below, there are 8 queries, which should result in 8 retrievals from the database."]},{"cell_type":"code","execution_count":null,"id":"1101de42","metadata":{"id":"1101de42"},"outputs":[],"source":["# Define the size for each of the vectors\n","vec_size = 16\n","\n","# Number of Queries\n","num_q = 8\n","\n","# Create random query vectors\n","Q = torch.randn(num_q, vec_size)\n","\n","# Create a random key vector for each image in the dataset\n","K = torch.randn(num_of_examples, vec_size)\n","\n","V = image_database\n","\n","# Lookup\n","alignment_scores = torch.mm(Q, K.transpose(0, 1)).float()\n","attention_weights = F.softmax(alignment_scores, -1)\n","\n","# Retrieval\n","output = torch.mm(attention_weights, V)\n","\n","print(\"Size of retrieved data:\", output.detach().numpy().shape)"]},{"cell_type":"code","execution_count":null,"id":"af670e40","metadata":{"id":"af670e40"},"outputs":[],"source":["# Lets visualise an entire batch of images!\n","plt.figure(figsize = (20,10))\n","out = torchvision.utils.make_grid(output.reshape(num_q, 1, 28, 28), 8, normalize=True)\n","_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"]},{"cell_type":"markdown","source":["**Note:** Because we used random vectors for the queries and keys, the lookup and retrieval doesn't really make a whole lot af sense. Next up, you will learn how to train an attention layer to actually do the lookup and retrieval the way it is supposed to work."],"metadata":{"id":"wNhCgpxyToO5"},"id":"wNhCgpxyToO5"},{"cell_type":"markdown","id":"7718c007","metadata":{"id":"7718c007"},"source":["## 6. Pytorch Multi-Head Attention\n","Of course Pytorch has it's own implementation of scaled dot-product attention.\n","\n","[Pytorch MultiheadAttention](https://pytorch.org/docs/2.1/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention)\n","\n","It supports multiple attention heads, as in the Transformer paper, but we will only use\n","one head for simplicity.\n","\n","Recall from lecture 12 that in the transformer model the queries, keys, and values are actually calculated by multiplying the vector embeddings of the inputs and/or outputs (denoted $X$ and $Y$ in the slides) with *learnable* matrices $W_q$, $W_k$, and $W_v$. For instance, for cross attention we would have\n","\n","> $Q=YW_q$\n","\n","> $K=XW_k$\n","\n","> $V=XW_v$\n","\n","In the general case, if we denote the embeddings $X_q$, $X_k$, and $X_v$, the queries, keys, and values are\n","\n","> $Q=X_qW_q$\n","\n","> $K=X_kW_k$\n","\n","> $V=X_vW_v$\n","\n"]},{"cell_type":"markdown","source":["### 6.1 Question\n","- If we denote the input data $X$, how do you calculate $Q$, $K$, and $V$ for *self-attention* (easy question!)."],"metadata":{"id":"ZJAm_2k54AUo"},"id":"ZJAm_2k54AUo"},{"cell_type":"markdown","source":["The code block below shows how to use Pytorch's MultiheadAttention layer. Note that it calculates $Q$, $K$, and $V$ internally using the equations above. So we just need to provide $X_q$, $X_k$, and $X_v$ as input:"],"metadata":{"id":"XNnOPmEg31f1"},"id":"XNnOPmEg31f1"},{"cell_type":"code","execution_count":null,"id":"ac3b6432","metadata":{"id":"ac3b6432"},"outputs":[],"source":["# Define the size for each of the vectors\n","vec_size = 16\n","\n","# Number of attention heads\n","num_heads = 1\n","\n","# Batch Size\n","batch_size = 4\n","\n","# Create a batch of a single random query vector\n","X_q = torch.randn(batch_size, 1, vec_size)\n","\n","# Create random key and value vectors for each image in the dataset\n","X_k = torch.randn(batch_size, num_of_examples, vec_size)\n","X_v = torch.randn(batch_size, num_of_examples, vec_size)\n","\n","# Initialize a MultiheadAttention module with specified parameters\n","multihead_attn = nn.MultiheadAttention(vec_size, num_heads, batch_first=True)\n","\n","# Perform a forward pass through the Multi-Head Attention module\n","# Returns the attention output and the attention weights\n","attn_output, attn_output_weights = multihead_attn(X_q, X_k, X_k, average_attn_weights=False)\n","\n","print(\"X_q:\", X_q.detach().numpy().shape)\n","print(\"X_k:\", X_k.detach().numpy().shape)\n","print(\"X_v:\", X_v.detach().numpy().shape)\n","\n","# Print the shapes of the output of the forward pass from Multi-Head Attention module\n","\n","# Softmaxed \"attention mask\" shape\n","print(\"Attention weights:\", attn_output_weights.detach().numpy().shape)\n","\n","# Attention output shape\n","print(\"Attention output (retrieval):\", attn_output.detach().numpy().shape)"]},{"cell_type":"markdown","source":["### 6.2 Questions\n","- What does the first dimension (size 4) of these arrays represent?\n","- What does `attn_output` represent?\n","- What does `attn_output_weights` represent?"],"metadata":{"id":"eK4WnJ7f46LC"},"id":"eK4WnJ7f46LC"},{"cell_type":"markdown","id":"22d9bb9b","metadata":{"id":"22d9bb9b"},"source":["### 6.3. Train a Multi-Head Attention\n","Lets train a model with attention that when given an image will try to find the best matching image in `image_database`.\n","\n","Specifically, our goal is to train a model that, given any of the 60,000 images in the MNIST dataset as the query, will match the query against `image_database` and retrieve the best matching image.\n","\n","Note that in this particular example, we enforce that the key and value embeddings are the same, i.e., we set $X_k=X_v$. To calculate the embeddings we use a one-layer MLP."]},{"cell_type":"code","execution_count":null,"id":"b6a8375e","metadata":{"id":"b6a8375e"},"outputs":[],"source":["class AttentionTest(nn.Module):\n","    def __init__(self, num_of_examples=100, embed_dim=32, num_heads=1):\n","        super(AttentionTest, self).__init__()\n","\n","        # Simple one-layer MLP use to embed images\n","        # Note the embedding is part of the model (learnable)\n","        self.img_mlp = nn.Sequential(\n","            nn.Linear(784, embed_dim),   # Linear layer to embed image data into a lower-dimensional space\n","        )\n","\n","        # Define the Multi-Head Attention mechanism\n","        self.mha = nn.MultiheadAttention(\n","            embed_dim=embed_dim,     # Dimensionality of the embedding space\n","            num_heads=num_heads,     # Number of attention heads\n","            batch_first=True         # Whether the input is batch-first or sequence-first\n","        )\n","\n","    def forward(self, img, image_database):\n","        # Embed the query image (img)\n","        X_q = self.img_mlp(img)\n","\n","        # Embed the images in the database\n","        X_v = self.img_mlp(image_database)\n","        X_k = X_v # Key and Value embeddings are the same in this example\n","\n","        # Lookup\n","        attn_output, attn_output_weights = self.mha(X_q, X_k, X_v)\n","\n","        # Retrieval\n","        # (by design our model will learn to retrieve images from image_database)\n","        output = torch.bmm(attn_output_weights, image_database)\n","\n","        return output, attn_output_weights"]},{"cell_type":"markdown","source":["### 6.4 Question\n","- See if you can understand what the `AttentionTest` module does."],"metadata":{"id":"cohAWmxJ6FYG"},"id":"cohAWmxJ6FYG"},{"cell_type":"markdown","source":["### 6.5 Set up training"],"metadata":{"id":"JHcVGXJr6Wdb"},"id":"JHcVGXJr6Wdb"},{"cell_type":"code","execution_count":null,"id":"2404abb8","metadata":{"id":"2404abb8"},"outputs":[],"source":["# Set the device to GPU if available, otherwise use CPU\n","device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n","\n","# Define the dimensionality of the embedding space\n","embed_dim = 32\n","\n","# Define the number of attention heads\n","num_heads = 1\n","\n","# Define the batch size\n","batch_size = 64\n","\n","# Duplicate the data value tensor for each batch element and move it to the specified device\n","image_database_batches = image_database.unsqueeze(0).expand(batch_size, num_of_examples, -1).to(device)"]},{"cell_type":"code","execution_count":null,"id":"a9361666","metadata":{"id":"a9361666"},"outputs":[],"source":["# Create a DataLoader for training the model\n","train_loader = dataloader.DataLoader(\n","    dataset,                   # Dataset to load\n","    shuffle=True,              # Shuffle the data for each epoch\n","    batch_size=batch_size,     # Batch size for training\n","    num_workers=4,             # Number of processes to use for data loading\n","    drop_last=True             # Drop the last incomplete batch if it's smaller than the batch size\n",")"]},{"cell_type":"markdown","id":"38eb795f","metadata":{"id":"38eb795f"},"source":["### 6.6 Initialize Model and Optimizer"]},{"cell_type":"code","execution_count":null,"id":"79d3f6a9","metadata":{"id":"79d3f6a9"},"outputs":[],"source":["# Create an instance of the AttentionTest model\n","mha_model = AttentionTest(\n","    num_of_examples=num_of_examples,   # Number of examples in the dataset\n","    embed_dim=embed_dim,               # Dimensionality of the embedding space\n","    num_heads=num_heads                # Number of attention heads\n",").to(device)                           # Move the model to the specified device\n","\n","# Define the Adam optimizer for training the model\n","optimizer = optim.Adam(\n","    mha_model.parameters(),  # Parameters to optimize\n","    lr=1e-4                   # Learning rate\n",")\n","\n","# List to store the training loss for each epoch\n","loss_logger = []"]},{"cell_type":"markdown","id":"7933ca68","metadata":{"id":"7933ca68"},"source":["### 6.7 Training"]},{"cell_type":"code","execution_count":null,"id":"50f5c3b3","metadata":{"id":"50f5c3b3"},"outputs":[],"source":["# Set the model to training mode\n","mha_model.train()\n","\n","# Loop through 10 epochs\n","for _ in trange(10, leave=False):\n","    # Iterate over the training data loader\n","    for queries, _ in tqdm(train_loader, leave=False):\n","\n","        # Reshape the input queries and move it to the specified device\n","        X_q = queries.reshape(queries.shape[0], 1, -1).to(device)\n","\n","        # Perform forward pass through the Multi-Head Attention model\n","        attn_output, attn_output_weights = mha_model(X_q, image_database_batches)\n","\n","        # Calculate the mean squared error loss between the output and input images\n","        loss = (attn_output - X_q).pow(2).mean()\n","\n","        # Zero the gradients, perform backward pass, and update model parameters\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Append the current loss value to the loss logger\n","        loss_logger.append(loss.item())"]},{"cell_type":"markdown","id":"eb5cd0d6","metadata":{"id":"eb5cd0d6"},"source":["Plot the loss:"]},{"cell_type":"code","execution_count":null,"id":"f84f21c7","metadata":{"id":"f84f21c7"},"outputs":[],"source":["_ = plt.plot(loss_logger[100:])\n","print(\"Minimum MSE loss %.4f\" % np.min(loss_logger))"]},{"cell_type":"markdown","id":"4cd8061b","metadata":{"id":"4cd8061b"},"source":["### 6.8 Using the Model\n","Let's see how to use the model to look up and retrieve images from the database."]},{"cell_type":"code","execution_count":null,"id":"22bdb3bf","metadata":{"id":"22bdb3bf"},"outputs":[],"source":["# Set the model to evaluation mode\n","mha_model.eval()\n","\n","# Perform forward pass without gradient computation\n","with torch.no_grad():\n","    queries, _ = next(iter(train_loader))\n","\n","    # Reshape input data and move it to the specified device\n","    X_q = queries.reshape(queries.shape[0], 1, -1).to(device)\n","\n","    # Perform forward pass through the Multi-Head Attention model\n","    attn_output, attn_output_weights = mha_model(X_q, image_database_batches)"]},{"cell_type":"code","execution_count":null,"id":"32f480e9","metadata":{"id":"32f480e9"},"outputs":[],"source":["# Select index of query image\n","q_index = 10"]},{"cell_type":"code","execution_count":null,"id":"52032b2e","metadata":{"id":"52032b2e"},"outputs":[],"source":["# Show the query image\n","plt.figure(figsize=(3, 3))\n","out = torchvision.utils.make_grid(X_q[q_index].cpu().reshape(-1, 1, 28, 28), 1,\n","                                  normalize=True, pad_value=0.5)\n","_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"]},{"cell_type":"markdown","source":["Show the retrival (i.e., sum of all samples in `image_database` weighted by the attention weights)."],"metadata":{"id":"rfxgCDlF9TS_"},"id":"rfxgCDlF9TS_"},{"cell_type":"code","source":["plt.figure(figsize=(3, 3))\n","out = torchvision.utils.make_grid(attn_output[q_index].cpu().reshape(-1, 1, 28, 28), 1,\n","                                  normalize=True, pad_value=0.5)\n","_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"],"metadata":{"id":"HcusHoct8Jt0"},"id":"HcusHoct8Jt0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Show attention weights (how much is retrieved from each of the 100 samples in `image_database`)."],"metadata":{"id":"LhgGqeh_9xRI"},"id":"LhgGqeh_9xRI"},{"cell_type":"code","execution_count":null,"id":"a2e1d4bb","metadata":{"id":"a2e1d4bb"},"outputs":[],"source":["# Plot the attention weights for the given input\n","_ = plt.plot(attn_output_weights[q_index, 0].cpu().numpy().flatten())"]},{"cell_type":"markdown","source":["Use the attention weights to find the 10 \"closest\" matches."],"metadata":{"id":"9j1aUxiV-JK2"},"id":"9j1aUxiV-JK2"},{"cell_type":"code","execution_count":null,"id":"9e84a3b4","metadata":{"id":"9e84a3b4"},"outputs":[],"source":["top10 = attn_output_weights[q_index, 0].argsort(descending=True)[:10]\n","top10_data = image_database_batches[q_index, top10].cpu()\n","\n","plt.figure(figsize=(10, 4))\n","out = torchvision.utils.make_grid(top10_data.reshape(-1, 1, 28, 28), 10, normalize=True, pad_value=0.5)\n","_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"]},{"cell_type":"markdown","source":["Show query image and the retrieved image (weighted sum of all images in `image_database`) for all images in the batch."],"metadata":{"id":"gjzJ47Az-biB"},"id":"gjzJ47Az-biB"},{"cell_type":"code","execution_count":null,"id":"28dbecaf","metadata":{"id":"28dbecaf"},"outputs":[],"source":["# Reshape the target and returned images\n","target_img = X_q.reshape(batch_size, 1, 28, 28)\n","indexed_img = attn_output.reshape(batch_size, 1, 28, 28)\n","\n","# Stack the images with the returned image on top\n","img_pair = torch.cat((indexed_img, target_img), 2).cpu()\n","\n","# Let's visualize the pairs of images, with the returned image on top and the target on bottom\n","plt.figure(figsize=(10, 10))\n","out = torchvision.utils.make_grid(img_pair, 8, normalize=True, pad_value=0.5)\n","_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"]},{"cell_type":"code","source":[],"metadata":{"id":"gFcsBWQM2uVG"},"id":"gFcsBWQM2uVG","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[{"file_id":"https://github.com/LukeDitria/pytorch_tutorials/blob/main/section13_attention/solutions/Pytorch1_Attention_Basics.ipynb","timestamp":1731853926122}],"toc_visible":true,"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}