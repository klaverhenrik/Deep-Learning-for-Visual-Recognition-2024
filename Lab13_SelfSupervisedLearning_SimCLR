{"cells":[{"cell_type":"markdown","metadata":{"id":"rognPbQ1jDJk"},"source":["# Semi-supervised image classification using contrastive pretraining with SimCLR\n","\n","Original source: https://keras.io/examples/vision/semisupervised_simclr/"]},{"cell_type":"markdown","metadata":{"id":"yMse9dlXjDJm"},"source":["## Introduction\n","\n","### Semi-supervised learning\n","\n","Semi-supervised learning is a machine learning paradigm that deals with\n","**partially labeled datasets**. When applying deep learning in the real world,\n","one usually has to gather a large dataset to make it work well. However, while\n","the cost of labeling scales linearly with the dataset size (labeling each\n","example takes a constant time), model performance only scales\n","[sublinearly](https://arxiv.org/abs/2001.08361) with it. This means that\n","labeling more and more samples becomes less and less cost-efficient, while\n","gathering unlabeled data is generally cheap, as it is usually readily available\n","in large quantities.\n","\n","Semi-supervised learning offers to solve this problem by only requiring a\n","partially labeled dataset, and by being label-efficient by utilizing the\n","unlabeled examples for learning as well.\n","\n","In this example, we will pretrain an encoder with contrastive learning on the\n","[STL-10](https://ai.stanford.edu/~acoates/stl10/) semi-supervised dataset using\n","no labels at all, and then fine-tune it using only its labeled subset.\n","\n","### Contrastive learning\n","\n","On the highest level, the main idea behind contrastive learning is to **learn\n","representations that are invariant to image augmentations** in a self-supervised\n","manner. One problem with this objective is that it has a trivial degenerate\n","solution: the case where the representations are constant, and do not depend at all on the\n","input images.\n","\n","Contrastive learning avoids this trap by modifying the objective in the\n","following way: it pulls representations of augmented versions/views of the same\n","image closer to each other (contracting positives), while simultaneously pushing\n","different images away from each other (contrasting negatives) in representation\n","space.\n","\n","One such contrastive approach is [SimCLR](https://arxiv.org/abs/2002.05709),\n","which essentially identifies the core components needed to optimize this\n","objective, and can achieve high performance by scaling this simple approach.\n","\n","For further reading about SimCLR, check out\n","[the official Google AI blog post](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html),\n","and for an overview of self-supervised learning across both vision and language\n","check out\n","[this blog post](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)."]},{"cell_type":"markdown","metadata":{"id":"iSEAvW9ZjDJm"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9C2Vnp6jDJm"},"outputs":[],"source":["import os\n","\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","# Make sure we are able to handle large datasets\n","import resource\n","\n","low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n","resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))\n","\n","import math\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","import keras\n","from keras import ops\n","from keras import layers"]},{"cell_type":"markdown","metadata":{"id":"OCgOaqO6jDJn"},"source":["## Hyperparameter setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKR56tWpjDJn"},"outputs":[],"source":["# Dataset hyperparameters\n","unlabeled_dataset_size = 100000\n","labeled_dataset_size = 5000\n","image_channels = 3\n","\n","# Algorithm hyperparameters\n","num_epochs = 20\n","batch_size = 525  # Corresponds to 200 steps per epoch\n","width = 128\n","temperature = 0.1\n","\n","# Stronger augmentations for contrastive, weaker ones for supervised training\n","contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2}\n","classification_augmentation = {\n","    \"min_area\": 0.75,\n","    \"brightness\": 0.3,\n","    \"jitter\": 0.1,\n","}"]},{"cell_type":"markdown","metadata":{"id":"68D9uvwKjDJn"},"source":["## Dataset\n","\n","During training we will simultaneously load a large batch of unlabeled images along with a\n","smaller batch of labeled images. This will take a while to download. In the meantime, you can skip ahead and see if you can understand the code in the *Self-supervised model for contrastive pretraining* section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ma0lWH0jDJn"},"outputs":[],"source":["\n","def prepare_dataset():\n","    # Labeled and unlabeled samples are loaded synchronously\n","    # with batch sizes selected accordingly\n","    steps_per_epoch = (unlabeled_dataset_size + labeled_dataset_size) // batch_size\n","    unlabeled_batch_size = unlabeled_dataset_size // steps_per_epoch\n","    labeled_batch_size = labeled_dataset_size // steps_per_epoch\n","    print(\n","        f\"batch size is {unlabeled_batch_size} (unlabeled) + {labeled_batch_size} (labeled)\"\n","    )\n","\n","    # Turning off shuffle to lower resource usage\n","    unlabeled_train_dataset = (\n","        tfds.load(\"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=False)\n","        .shuffle(buffer_size=10 * unlabeled_batch_size)\n","        .batch(unlabeled_batch_size)\n","    )\n","    labeled_train_dataset = (\n","        tfds.load(\"stl10\", split=\"train\", as_supervised=True, shuffle_files=False)\n","        .shuffle(buffer_size=10 * labeled_batch_size)\n","        .batch(labeled_batch_size)\n","    )\n","    test_dataset = (\n","        tfds.load(\"stl10\", split=\"test\", as_supervised=True)\n","        .batch(batch_size)\n","        .prefetch(buffer_size=tf.data.AUTOTUNE)\n","    )\n","\n","    # Labeled and unlabeled datasets are zipped together\n","    train_dataset = tf.data.Dataset.zip(\n","        (unlabeled_train_dataset, labeled_train_dataset)\n","    ).prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","    return train_dataset, labeled_train_dataset, test_dataset\n","\n","\n","# Load STL10 dataset\n","train_dataset, labeled_train_dataset, test_dataset = prepare_dataset()"]},{"cell_type":"markdown","metadata":{"id":"f0C8L989jDJo"},"source":["## Image augmentations\n","\n","The two most important image augmentations for contrastive learning are the\n","following:\n","\n","- Cropping: forces the model to encode different parts of the same image\n","similarly, we implement it with the\n","[RandomTranslation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_translation/)\n","and\n","[RandomZoom](https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_zoom/)\n","layers\n","- Color jitter: prevents a trivial color histogram-based solution to the task by\n","distorting color histograms. A principled way to implement that is by affine\n","transformations in color space.\n","\n","In this example we use random horizontal flips as well. Stronger augmentations\n","are applied for contrastive learning, along with weaker ones for supervised\n","classification to avoid overfitting on the few labeled examples.\n","\n","We implement random color jitter as a custom preprocessing layer. Using\n","preprocessing layers for data augmentation has the following two advantages:\n","\n","- The data augmentation will run on GPU in batches, so the training will not be\n","bottlenecked by the data pipeline in environments with constrained CPU\n","resources (such as a Colab Notebook, or a personal machine)\n","- Deployment is easier as the data preprocessing pipeline is encapsulated in the\n","model, and does not have to be reimplemented when deploying it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeUXAkCqjDJo"},"outputs":[],"source":["\n","# Distorts the color distibutions of images\n","class RandomColorAffine(layers.Layer):\n","    def __init__(self, brightness=0, jitter=0, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.seed_generator = keras.random.SeedGenerator(1337)\n","        self.brightness = brightness\n","        self.jitter = jitter\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"brightness\": self.brightness, \"jitter\": self.jitter})\n","        return config\n","\n","    def call(self, images, training=True):\n","        if training:\n","            batch_size = ops.shape(images)[0]\n","\n","            # Same for all colors\n","            brightness_scales = 1 + keras.random.uniform(\n","                (batch_size, 1, 1, 1),\n","                minval=-self.brightness,\n","                maxval=self.brightness,\n","                seed=self.seed_generator,\n","            )\n","            # Different for all colors\n","            jitter_matrices = keras.random.uniform(\n","                (batch_size, 1, 3, 3),\n","                minval=-self.jitter,\n","                maxval=self.jitter,\n","                seed=self.seed_generator,\n","            )\n","\n","            color_transforms = (\n","                ops.tile(ops.expand_dims(ops.eye(3), axis=0), (batch_size, 1, 1, 1))\n","                * brightness_scales\n","                + jitter_matrices\n","            )\n","            images = ops.clip(ops.matmul(images, color_transforms), 0, 1)\n","        return images\n","\n","\n","# Image augmentation module\n","def get_augmenter(min_area, brightness, jitter):\n","    zoom_factor = 1.0 - math.sqrt(min_area)\n","    return keras.Sequential(\n","        [\n","            layers.Rescaling(1 / 255),\n","            layers.RandomFlip(\"horizontal\"),\n","            layers.RandomTranslation(zoom_factor / 2, zoom_factor / 2),\n","            layers.RandomZoom((-zoom_factor, 0.0), (-zoom_factor, 0.0)),\n","            RandomColorAffine(brightness, jitter),\n","        ]\n","    )\n","\n","\n","def visualize_augmentations(num_images):\n","    # Sample a batch from a dataset\n","    images = next(iter(train_dataset))[0][0][:num_images]\n","\n","    # Apply augmentations\n","    augmented_images = zip(\n","        images,\n","        get_augmenter(**classification_augmentation)(images),\n","        get_augmenter(**contrastive_augmentation)(images),\n","        get_augmenter(**contrastive_augmentation)(images),\n","    )\n","    row_titles = [\n","        \"Original:\",\n","        \"Weakly augmented:\",\n","        \"Strongly augmented:\",\n","        \"Strongly augmented:\",\n","    ]\n","    plt.figure(figsize=(num_images * 2.2, 4 * 2.2), dpi=100)\n","    for column, image_row in enumerate(augmented_images):\n","        for row, image in enumerate(image_row):\n","            plt.subplot(4, num_images, row * num_images + column + 1)\n","            plt.imshow(image)\n","            if column == 0:\n","                plt.title(row_titles[row], loc=\"left\")\n","            plt.axis(\"off\")\n","    plt.tight_layout()\n","\n","\n","visualize_augmentations(num_images=8)"]},{"cell_type":"markdown","metadata":{"id":"QUOpL_3JjDJo"},"source":["## Encoder architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJigpd6CjDJo"},"outputs":[],"source":["\n","# Define the encoder architecture\n","def get_encoder():\n","    return keras.Sequential(\n","        [\n","            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n","            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n","            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n","            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n","            layers.Flatten(),\n","            layers.Dense(width, activation=\"relu\"),\n","        ],\n","        name=\"encoder\",\n","    )\n"]},{"cell_type":"markdown","metadata":{"id":"oDJvOecvjDJo"},"source":["## Supervised baseline model\n","\n","A baseline supervised model is trained using random initialization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KbQd6-FjDJp"},"outputs":[],"source":["# Baseline supervised training with random initialization\n","baseline_model = keras.Sequential(\n","    [\n","        keras.Input(shape=(96,96,3)),\n","        get_augmenter(**classification_augmentation),\n","        get_encoder(),\n","        layers.Dense(10),\n","    ],\n","    name=\"baseline_model\",\n",")\n","baseline_model.compile(\n","    optimizer=keras.optimizers.Adam(),\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",")\n","\n","baseline_history = baseline_model.fit(\n","    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n",")\n","\n","print(\n","    \"Maximal validation accuracy: {:.2f}%\".format(\n","        max(baseline_history.history[\"val_acc\"]) * 100\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"id":"GAzoccSljDJp"},"source":["## Self-supervised model for contrastive pretraining\n","\n","We pretrain an encoder on unlabeled images with a contrastive loss.\n","A nonlinear projection head is attached to the top of the encoder, as it\n","improves the quality of representations of the encoder.\n","\n","We use the [InfoNCE/NT-Xent/N-pairs](https://towardsdatascience.com/nt-xent-normalized-temperature-scaled-cross-entropy-loss-explained-and-implemented-in-pytorch-cc081f69848) loss, which can be interpreted in the\n","following way:\n","\n","1. We treat each image in the batch as if it had its own class.\n","2. Then, we have two examples (a pair of augmented views) for each \"class\".\n","3. Each view's representation is compared to every possible pair's one (for both\n","  augmented versions).\n","4. We use the temperature-scaled cosine similarity of compared representations as\n","  logits.\n","5. Finally, we use categorical cross-entropy as the \"classification\" loss\n","\n","The following two metrics are used for monitoring the pretraining performance:\n","\n","- [Contrastive accuracy (SimCLR Table 5)](https://arxiv.org/abs/2002.05709):\n","Self-supervised metric, the ratio of cases in which the representation of an\n","image is more similar to its differently augmented version's one, than to the\n","representation of any other image in the current batch. Self-supervised\n","metrics can be used for hyperparameter tuning even in the case when there are\n","no labeled examples.\n","- [Linear probing accuracy](https://arxiv.org/abs/1603.08511): Linear probing is\n","a popular metric to evaluate self-supervised classifiers. It is computed as\n","the accuracy of a logistic regression classifier trained on top of the\n","encoder's features. In our case, this is done by training a single dense layer\n","on top of the frozen encoder. Note that contrary to traditional approach where\n","the classifier is trained after the pretraining phase, in this example we\n","train it during pretraining. This might slightly decrease its accuracy, but\n","that way we can monitor its value during training, which helps with\n","experimentation and debugging.\n","\n","Another widely used supervised metric is the\n","[KNN accuracy](https://arxiv.org/abs/1805.01978), which is the accuracy of a KNN\n","classifier trained on top of the encoder's features, which is not implemented in\n","this example."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WCG-i_zvjDJp"},"outputs":[],"source":["\n","# Define the contrastive model with model-subclassing\n","class ContrastiveModel(keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.temperature = temperature\n","        self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)\n","        self.classification_augmenter = get_augmenter(**classification_augmentation)\n","        self.encoder = get_encoder()\n","        # Non-linear MLP as projection head\n","        self.projection_head = keras.Sequential(\n","            [\n","                keras.Input(shape=(width,)),\n","                layers.Dense(width, activation=\"relu\"),\n","                layers.Dense(width),\n","            ],\n","            name=\"projection_head\",\n","        )\n","        # Single dense layer for linear probing\n","        self.linear_probe = keras.Sequential(\n","            [layers.Input(shape=(width,)), layers.Dense(10)],\n","            name=\"linear_probe\",\n","        )\n","\n","        self.encoder.summary()\n","        self.projection_head.summary()\n","        self.linear_probe.summary()\n","\n","    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n","        super().compile(**kwargs)\n","\n","        self.contrastive_optimizer = contrastive_optimizer\n","        self.probe_optimizer = probe_optimizer\n","\n","        # self.contrastive_loss will be defined as a method\n","        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n","        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n","            name=\"c_acc\"\n","        )\n","        self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n","        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n","\n","    @property\n","    def metrics(self):\n","        return [\n","            self.contrastive_loss_tracker,\n","            self.contrastive_accuracy,\n","            self.probe_loss_tracker,\n","            self.probe_accuracy,\n","        ]\n","\n","    def contrastive_loss(self, projections_1, projections_2):\n","        # InfoNCE loss (information noise-contrastive estimation)\n","        # NT-Xent loss (normalized temperature-scaled cross entropy)\n","\n","        # Cosine similarity: the dot product of the l2-normalized feature vectors\n","        projections_1 = ops.normalize(projections_1, axis=1)\n","        projections_2 = ops.normalize(projections_2, axis=1)\n","        similarities = (\n","            ops.matmul(projections_1, ops.transpose(projections_2)) / self.temperature\n","        )\n","\n","        # The similarity between the representations of two augmented views of the\n","        # same image should be higher than their similarity with other views\n","        batch_size = ops.shape(projections_1)[0]\n","        contrastive_labels = ops.arange(batch_size)\n","        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n","        self.contrastive_accuracy.update_state(\n","            contrastive_labels, ops.transpose(similarities)\n","        )\n","\n","        # The temperature-scaled similarities are used as logits for cross-entropy\n","        # a symmetrized version of the loss is used here\n","        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n","            contrastive_labels, similarities, from_logits=True\n","        )\n","        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n","            contrastive_labels, ops.transpose(similarities), from_logits=True\n","        )\n","        return (loss_1_2 + loss_2_1) / 2\n","\n","    def train_step(self, data):\n","        (unlabeled_images, _), (labeled_images, labels) = data\n","\n","        # Both labeled and unlabeled images are used, without labels\n","        images = ops.concatenate((unlabeled_images, labeled_images), axis=0)\n","\n","        # Each image is augmented twice, differently\n","        augmented_images_1 = self.contrastive_augmenter(images, training=True)\n","        augmented_images_2 = self.contrastive_augmenter(images, training=True)\n","        with tf.GradientTape() as tape:\n","            features_1 = self.encoder(augmented_images_1, training=True)\n","            features_2 = self.encoder(augmented_images_2, training=True)\n","            # The representations are passed through a projection mlp\n","            projections_1 = self.projection_head(features_1, training=True)\n","            projections_2 = self.projection_head(features_2, training=True)\n","            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n","        gradients = tape.gradient(\n","            contrastive_loss,\n","            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n","        )\n","        self.contrastive_optimizer.apply_gradients(\n","            zip(\n","                gradients,\n","                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n","            )\n","        )\n","        self.contrastive_loss_tracker.update_state(contrastive_loss)\n","\n","        # Labels are only used in evalutation for an on-the-fly logistic regression\n","        preprocessed_images = self.classification_augmenter(\n","            labeled_images, training=True\n","        )\n","        with tf.GradientTape() as tape:\n","            # the encoder is used in inference mode here to avoid regularization\n","            # and updating the batch normalization paramers if they are used\n","            features = self.encoder(preprocessed_images, training=False)\n","            class_logits = self.linear_probe(features, training=True)\n","            probe_loss = self.probe_loss(labels, class_logits)\n","        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n","        self.probe_optimizer.apply_gradients(\n","            zip(gradients, self.linear_probe.trainable_weights)\n","        )\n","        self.probe_loss_tracker.update_state(probe_loss)\n","        self.probe_accuracy.update_state(labels, class_logits)\n","\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def test_step(self, data):\n","        labeled_images, labels = data\n","\n","        # For testing the components are used with a training=False flag\n","        preprocessed_images = self.classification_augmenter(\n","            labeled_images, training=False\n","        )\n","        features = self.encoder(preprocessed_images, training=False)\n","        class_logits = self.linear_probe(features, training=False)\n","        probe_loss = self.probe_loss(labels, class_logits)\n","        self.probe_loss_tracker.update_state(probe_loss)\n","        self.probe_accuracy.update_state(labels, class_logits)\n","\n","        # Only the probe metrics are logged at test time\n","        return {m.name: m.result() for m in self.metrics[2:]}\n","\n","\n","# Contrastive pretraining\n","pretraining_model = ContrastiveModel()\n","pretraining_model.compile(\n","    contrastive_optimizer=keras.optimizers.Adam(),\n","    probe_optimizer=keras.optimizers.Adam(),\n",")\n","\n","pretraining_history = pretraining_model.fit(\n","    train_dataset, epochs=num_epochs, validation_data=test_dataset\n",")\n","print(\n","    \"Maximal validation accuracy: {:.2f}%\".format(\n","        max(pretraining_history.history[\"val_p_acc\"]) * 100\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"id":"yBLCBQwljDJp"},"source":["## Supervised finetuning of the pretrained encoder\n","\n","We then finetune the encoder on the labeled examples, by attaching\n","a single randomly initalized fully connected classification layer on its top."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cBRID8DjDJp"},"outputs":[],"source":["# Supervised finetuning of the pretrained encoder\n","finetuning_model = keras.Sequential(\n","    [\n","        keras.Input(shape=(96,96,3)),\n","        get_augmenter(**classification_augmentation),\n","        pretraining_model.encoder,\n","        layers.Dense(10),\n","    ],\n","    name=\"finetuning_model\",\n",")\n","finetuning_model.compile(\n","    optimizer=keras.optimizers.Adam(),\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",")\n","\n","finetuning_history = finetuning_model.fit(\n","    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n",")\n","print(\n","    \"Maximal validation accuracy: {:.2f}%\".format(\n","        max(finetuning_history.history[\"val_acc\"]) * 100\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"id":"ETpE_WFfjDJp"},"source":["## Comparison against the baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vyeY_10jDJp"},"outputs":[],"source":["\n","# The classification accuracies of the baseline and the pretraining + finetuning process:\n","def plot_training_curves(pretraining_history, finetuning_history, baseline_history):\n","    for metric_key, metric_name in zip([\"acc\", \"loss\"], [\"accuracy\", \"loss\"]):\n","        plt.figure(figsize=(8, 5), dpi=100)\n","        plt.plot(\n","            baseline_history.history[f\"val_{metric_key}\"],\n","            label=\"supervised baseline\",\n","        )\n","        plt.plot(\n","            pretraining_history.history[f\"val_p_{metric_key}\"],\n","            label=\"self-supervised pretraining\",\n","        )\n","        plt.plot(\n","            finetuning_history.history[f\"val_{metric_key}\"],\n","            label=\"supervised finetuning\",\n","        )\n","        plt.legend()\n","        plt.title(f\"Classification {metric_name} during training\")\n","        plt.xlabel(\"epochs\")\n","        plt.ylabel(f\"validation {metric_name}\")\n","\n","\n","plot_training_curves(pretraining_history, finetuning_history, baseline_history)"]},{"cell_type":"markdown","metadata":{"id":"M8BjnSKcjDJp"},"source":["By comparing the training curves, we can see that when using contrastive\n","pretraining, a higher validation accuracy can be reached, paired with a lower\n","validation loss, which means that the pretrained network was able to generalize\n","better when seeing only a small amount of labeled examples."]},{"cell_type":"markdown","metadata":{"id":"WG4sftyEjDJp"},"source":["## Improving further\n","\n","### Architecture\n","\n","The experiment in the original paper demonstrated that increasing the width and depth of the\n","models improves performance at a higher rate than for supervised learning. Also,\n","using a [ResNet-50](https://keras.io/api/applications/resnet/#resnet50-function)\n","encoder is quite standard in the literature. However keep in mind, that more\n","powerful models will not only increase training time but will also require more\n","memory and will limit the maximal batch size you can use.\n","\n","It has [been](https://arxiv.org/abs/1905.09272)\n","[reported](https://arxiv.org/abs/1911.05722) that the usage of BatchNorm layers\n","could sometimes degrade performance, as it introduces an intra-batch dependency\n","between samples, which is why I did not have used them in this example. In my\n","experiments however, using BatchNorm, especially in the projection head,\n","improves performance.\n","\n","### Hyperparameters\n","\n","The hyperparameters used in this example have been tuned manually for this task and\n","architecture. Therefore, without changing them, only marginal gains can be expected\n","from further hyperparameter tuning.\n","\n","However for a different task or model architecture these would need tuning, so\n","here are my notes on the most important ones:\n","\n","- **Batch size**: since the objective can be interpreted as a classification\n","over a batch of images (loosely speaking), the batch size is actually a more\n","important hyperparameter than usual. The higher, the better.\n","- **Temperature**: the temperature defines the \"softness\" of the softmax\n","distribution that is used in the cross-entropy loss, and is an important\n","hyperparameter. Lower values generally lead to a higher contrastive accuracy.\n","A recent trick (in [ALIGN](https://arxiv.org/abs/2102.05918)) is to learn\n","the temperature's value as well (which can be done by defining it as a\n","tf.Variable, and applying gradients on it). Even though this provides a good baseline\n","value, in my experiments the learned temperature was somewhat lower\n","than optimal, as it is optimized with respect to the contrastive loss, which is not a\n","perfect proxy for representation quality.\n","- **Image augmentation strength**: during pretraining stronger augmentations\n","increase the difficulty of the task, however after a point too strong\n","augmentations will degrade performance. During finetuning stronger\n","augmentations reduce overfitting while in my experience too strong\n","augmentations decrease the performance gains from pretraining. The whole data\n","augmentation pipeline can be seen as an important hyperparameter of the\n","algorithm, implementations of other custom image augmentation layers in Keras\n","can be found in\n","[this repository](https://github.com/beresandras/image-augmentation-layers-keras).\n","- **Learning rate schedule**: a constant schedule is used here, but it is\n","quite common in the literature to use a\n","[cosine decay schedule](https://www.tensorflow.org/api_docs/python/tf/keras/experimental/CosineDecay),\n","which can further improve performance.\n","- **Optimizer**: Adam is used in this example, as it provides good performance\n","with default parameters. SGD with momentum requires more tuning, however it\n","could slightly increase performance."]},{"cell_type":"markdown","metadata":{"id":"nJbt4V_RjDJq"},"source":["## Related works\n","\n","Other instance-level (image-level) contrastive learning methods:\n","\n","- [MoCo](https://arxiv.org/abs/1911.05722)\n","([v2](https://arxiv.org/abs/2003.04297),\n","[v3](https://arxiv.org/abs/2104.02057)): uses a momentum-encoder as well,\n","whose weights are an exponential moving average of the target encoder\n","- [SwAV](https://arxiv.org/abs/2006.09882): uses clustering instead of pairwise\n","comparison\n","- [BarlowTwins](https://arxiv.org/abs/2103.03230): uses a cross\n","correlation-based objective instead of pairwise comparison\n","\n","Keras implementations of **MoCo** and **BarlowTwins** can be found in\n","[this repository](https://github.com/beresandras/contrastive-classification-keras),\n","which includes a Colab notebook.\n","\n","There is also a new line of works, which optimize a similar objective, but\n","without the use of any negatives:\n","\n","- [BYOL](https://arxiv.org/abs/2006.07733): momentum-encoder + no negatives\n","- [SimSiam](https://arxiv.org/abs/2011.10566)\n","([Keras example](https://keras.io/examples/vision/simsiam/)):\n","no momentum-encoder + no negatives\n","\n","In my experience, these methods are more brittle (they can collapse to a constant\n","representation, I could not get them to work using this encoder architecture).\n","Even though they are generally more dependent on the\n","[model](https://generallyintelligent.ai/understanding-self-supervised-contrastive-learning.html)\n","[architecture](https://arxiv.org/abs/2010.10241), they can improve\n","performance at smaller batch sizes.\n","\n","You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/semi-supervised-classification-simclr)\n","and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/semi-supervised-classification)."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/semisupervised_simclr.ipynb","timestamp":1732793614585}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}