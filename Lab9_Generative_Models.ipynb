{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FqZrTrrptdl7R9ups0Za8eWTcqOgaNkj","timestamp":1730141030655}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"REaV1GblZovs"},"source":["#Generative models\n","In this Lab we will be experimenting with\n","\n","- Convolutional autoencoders\n","- Latent space visualization and interpolation\n","- Upsampling techniques\n","- Variational autoencoders\n","- Deep Convolutional GANs (DCGANs)\n","\n","If you want to experiment with Denoising Autoencoders, revisit Lab 3 (task 5):\n","https://github.com/klaverhenrik/Deep-Learning-for-Visual-Recognition-2024/blob/main/Lab3_FunWithMNIST.ipynb\n","\n","**Before you start, remember to set runtime to GPU**"]},{"cell_type":"markdown","metadata":{"id":"VSIpfu6wUG1r"},"source":["**NOTE:** In case you have trouble running Keras/TensorFlow in Colab, try one of the following:"]},{"cell_type":"markdown","metadata":{"id":"Qc8RJkIJaxap"},"source":["## 1. Download the MNIST dataset\n","As usual:"]},{"cell_type":"code","metadata":{"id":"r-_9ZwQkiGQg"},"source":["from __future__ import print_function\n","from tensorflow import keras\n","from keras.datasets import mnist\n","from keras import backend as K\n","\n","num_classes = 10\n","\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","if K.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n","    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","# Pre-process inputs\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","# Convert class indices to one-hot vectors\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Son2qjrbQTq"},"source":["# Input shape: 28 x 28 x 1 = image with one color channel\n","print('input_shape :',input_shape)\n","\n","# Pre-process inputs\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# to_categorical converts class indices to one-hot vectors\n","print('y_train shape:', y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6YJnv8WI7b4B"},"source":["## 2. Task 1: Convolutional Autoencoder\n","Recall that *generative models* try to learn something useful about the distribution of the data. A succesful generative model is one that allows us to draw realistically looking samples from the original distribution.\n","\n","In today's lab, we will be using MNIST, but we will assume that we don't know the labels and see if we can detect interesting structures in the data - without using the labels. Hence, all the generative models below represent examples of *unsupervised learning*.\n","\n","The simplest type of generative model is an *autoencoder* (AEs). Traditional AEs are fully connected (i.e., inputs and outputs are vectors), which we know is not very useful when dealing with image data. Therefore, we will be using the convolutional variant, called a Convolutional Autoencoder (CAE).\n","\n","There are many ways to implement CAEs. The one below is designed to map the input data down to a 2D *latent space*, so that you can plot the latent vectors in 2D. In this way we can use to CAE to visualize the distribution of our data in 2D.\n","\n","**Note** that below we define the encoder and the decoder separately and combine them afterwards to form the final CAE model.\n"]},{"cell_type":"markdown","metadata":{"id":"7aABSvT1cME5"},"source":["###2.1 Your task\n","An autoencoder learns an identity function, meaning that we require the input and output of the model to have exactly the same shape.\n","\n","Your task is to specify the kernel size and padding size of the last layer, such that shape of the decoder's output matches the input shape (28x28x1). That is, fill in the missing code marked with `???`:\n","\n","```\n","decoded = Conv2D(1, kernel_size=???, padding=???, activation='sigmoid')(x)\n","```\n","\n","Recall that in Keras, padding can be set to either `same` or `valid` (what's the difference?)."]},{"cell_type":"code","metadata":{"id":"BIBw4LuzVEEh"},"source":["from keras.layers import Input, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n","from keras.models import Model\n","from keras.layers import UpSampling2D, ZeroPadding2D, Conv2DTranspose, Reshape\n","\n","# Number of latent dimensions\n","latent_dim = 2\n","\n","# Encoder (convolutional base)\n","inputs = Input(shape=(28, 28, 1))\n","x = ZeroPadding2D(padding=(2, 2))(inputs)\n","x = Conv2D(8, kernel_size=(3, 3), strides=(2,2), activation='relu', padding='same')(x)\n","x = Conv2D(16, kernel_size=(3, 3), strides=(2,2), activation='relu', padding='same')(x)\n","x = Conv2D(32, kernel_size=(3, 3), strides=(2,2), activation='relu', padding='same')(x)\n","\n","# shape info needed to build decoder model\n","shape = x.shape\n","print(\"shape of x\", x.shape)\n","\n","x = Flatten()(x) # vectorize last feature map\n","encoded = Dense(latent_dim)(x) # map to vector of length \"latent_dim\"\n","encoder = Model(inputs, encoded)\n","encoder.summary()\n","print(\"shape of encoded\", encoded.shape)\n","\n","# Decoder (upsamling)\n","encoding = Input(shape=(latent_dim,))\n","x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(encoding)\n","x = Reshape((shape[1], shape[2], shape[3]))(x)\n","x = Conv2DTranspose(32, (3,3), strides=(2,2), padding='same')(x)\n","x = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(x)\n","x = Conv2DTranspose(8, (3,3), strides=(2,2), padding='same')(x)\n","decoded = Conv2D(1, kernel_size=???, padding=???, activation='sigmoid')(x) # Fix this line !!!\n","decoder = Model(encoding, decoded)\n","decoder.summary()\n","print(\"shape of decoded\", decoded.shape)\n","\n","x = encoder(inputs)\n","predictions = decoder(x)\n","autoencoder = Model(inputs, predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1gIKS_zSWZng"},"source":["###2.2 Questions:\n","1. What does the `ZeroPadding2D` layer do?\n","2. What is the shape of the data before and after zero padding? (Note: for downsampling and upsampling it is more convenient if the shape of the data is a power of 2).\n","3. What is the purpose of the `Reshape`layer in the decoder?\n"]},{"cell_type":"markdown","metadata":{"id":"bKXQlf8tBnmc"},"source":["###2.3 Training\n","Let's train the autoencoder for 30 epochs (add more epochs to improve results):"]},{"cell_type":"code","metadata":{"id":"Vw_OqAHD5k1-"},"source":["autoencoder.compile(optimizer='adam', loss='mse')\n","history = autoencoder.fit(x_train, x_train, epochs=30, batch_size=256,\n","               shuffle=True, validation_data=(x_test, x_test), verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_91-fqbZSPp"},"source":["import matplotlib.pyplot as plt\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.legend(['Train loss','Validation loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gcd5EgZuZI1l"},"source":["###2.4 Plot the latent space representation\n","To get some intuition about what our autoencoder has learned, we can plot the latent representation of the training data:"]},{"cell_type":"code","metadata":{"id":"G7Idi6hcEhjx"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import matplotlib as mpl\n","mpl.rc('image', cmap='jet')\n","\n","# Get latent representation\n","z = encoder.predict(x_train,batch_size=32)\n","\n","# Plot\n","plt.figure(figsize=(12, 10))\n","plt.scatter(z[:, 0], z[:, 1], c=np.argmax(y_train,axis=1))\n","plt.colorbar()\n","plt.xlabel(\"z[0]\")\n","plt.ylabel(\"z[1]\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sujyFZhV7YCz"},"source":["Each color represents a different class. What do you see from this plot? Which classes are clearly separated, and which are mixed up?"]},{"cell_type":"markdown","metadata":{"id":"dlR9C90cfUVu"},"source":["Recall that training was *unsupervised*, i.e., it was performed without knowing the labels. The autoencoder is a generative model, so we are also interested in seing **how well it is able to draw samples from the data distribution** it has learned. To do that we will use the autoencoder to *generate* new samples. We do this by generating latent vectors that span a 2D grid (defined by `grid_x` and `grid_y` below) and then feed each latent vector on the grid into the decoder to generate an image:\n","\n","**Note** you will have to adjust the limits of `grid_x` and `grid_y` to match the range of latent features that your model has learned."]},{"cell_type":"code","metadata":{"id":"Nqk1t_r8_UNu"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","n = 20 # number of grid cells\n","digit_size = 28\n","figure = np.zeros((digit_size * n, digit_size * n))\n","\n","# linearly spaced coordinates corresponding to the 2D plot\n","# of digit classes in the latent space\n","grid_x = np.linspace(-6, 6, n)        # Task : Set range according to your latent representation\n","grid_y = np.linspace(-6, 6, n)[::-1]  # Task : Set range according to your latent representation\n","\n","for i, yi in enumerate(grid_y):\n","    for j, xi in enumerate(grid_x):\n","        z_sample = np.array([[xi, yi]])\n","        x_decoded = decoder.predict(z_sample.reshape(1,2), verbose=False)\n","        digit = x_decoded[0].reshape(digit_size, digit_size)\n","        figure[i * digit_size: (i + 1) * digit_size,\n","                j * digit_size: (j + 1) * digit_size] = digit\n","\n","plt.figure(figsize=(10, 10))\n","pixel_range = np.linspace(digit_size/2,n*digit_size-digit_size/2,n)\n","sample_range_x = np.round(grid_x, 1)\n","sample_range_y = np.round(grid_y, 1)\n","plt.xticks(pixel_range, sample_range_x)\n","plt.yticks(pixel_range, sample_range_y)\n","plt.xlabel(\"z[0]\")\n","plt.ylabel(\"z[1]\")\n","plt.imshow(figure, cmap='Greys_r')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gPirfGdCmZ2v"},"source":["The grid above shows the \"digits\" generated by the autoencoder for different combinations of 2D latent vectors (corresponding to the values on the x- and y-axis)."]},{"cell_type":"markdown","metadata":{"id":"FnnBuylfeHO_"},"source":["###2.5 Question\n","1. Which digits can the autoencoder generate faithfully, which digits does it have trouble generating? Could we explain why?"]},{"cell_type":"markdown","metadata":{"id":"EwXofIvQhOdN"},"source":["###2.6 Encoding, decoding and latent space interpolation\n","Now that we have trained an autoencoder, we can use it to encode existing images and generate new images (from a latent representation). With the latent representation we can also start doing interpolation between training samples.\n","\n","Your task is to\n","\n","1. Encode an image of a 7 and an image of a 9 (or any other pair if you refer)\n","2. Decode the encodings to generate reconstructed images\n","3. Interpolate between the two digits in latent space\n","\n","You just need to figure out what the missing shapes `???` should be."]},{"cell_type":"code","metadata":{"id":"qoBUEtgdSu_i"},"source":["# Draw two samples (a 7 and a 9) and display them\n","y_test_category = np.argmax(y_test,axis=1)\n","ix7 = np.where(y_test_category==7)[0][1] # Pick a 7\n","ix9 = np.where(y_test_category==9)[0][1] # Pick a 9\n","\n","print('Original input images')\n","plt.subplot(221);plt.imshow(x_test[ix7,:].squeeze(),cmap='gray')\n","plt.subplot(222);plt.imshow(x_test[ix9,:].squeeze(),cmap='gray')\n","plt.show()\n","\n","# Subtask 1 (encoding):\n","# Calculate the latent representation of each sample using the encoder\n","z7 = encoder.predict(x_test[ix7,:].reshape(1,???,???,1))\n","z9 = encoder.predict(x_test[ix9,:].reshape(1,???,???,1))\n","\n","# Subtask 2 (decoding):\n","# Reconstruct images from the two latent vectors using the decoder\n","x_hat_7 = decoder.predict(z7.reshape(1,???))\n","x_hat_9 = decoder.predict(z9.reshape(1,???))\n","\n","# Show reconstruction\n","print('Images reconstructewd from 2D latens representation')\n","plt.subplot(223);plt.imshow(x_hat_7.squeeze(),cmap='gray')\n","plt.subplot(224);plt.imshow(x_hat_9.squeeze(),cmap='gray')\n","plt.show()\n","\n","# Subtask 3 (interpolate):\n","# Just run - no changes required)\n","N = 8\n","interp_features = np.zeros((N,latent_dim))\n","for i in range(latent_dim):\n","  interp_features[:,i] = np.linspace(z7[0,i].squeeze(),z9[0,i].squeeze(),N)\n","\n","print('Interpolated images')\n","plt.figure(figsize=(20,6))\n","for i in range(N):\n","  x = interp_features[i,:].reshape(1,latent_dim)\n","  out = decoder.predict(x)\n","  plt.subplot(1,N,i+1)\n","  plt.imshow(out.squeeze(),cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** If you want to make nicer reconstructions and better interpolations, increase the latent dimensionality (latent_dim) and re-train the model."],"metadata":{"id":"InRlNPVUbqPi"}},{"cell_type":"markdown","metadata":{"id":"8QQDl2AC22AF"},"source":["**Perspectives:** Given a dataset of facial images, you could use latent space interpolation to generate images like these:\n","\n","![alt text](https://github.com/davidsandberg/facenet/wiki/20170708-150701-add_smile.png)"]},{"cell_type":"markdown","metadata":{"id":"gE_--D90TNsz"},"source":["##3. Task 2: Implement a CAE from scratch\n","The purpose of this task is to test if you can implement a CAE from scratch. **I recommend you skip ahead and complete the tasks on variational encoders and GANs first, and then return to this task later**.\n","\n","Your task is to implement this CAE archtecture for MNIST:\n","\n","![alt text](https://github.com/aivclab/dlcourse/raw/master/data/Lab9_CAE_architecture.png)\n","\n","**Explanation**:\n","- \"Conv 1\", \"Conv 2\", \"Conv 3\", \"D Conv 1\", \"D Conv 2\", \"D Conv 3\", and \"D Conv 4\" are *all* regular 2D convolutions: [Conv2D](https://keras.io/layers/convolutional/#conv2d).\n","- \"M.P\" is short for Max Pooling\n","- \"U.S\" is short for upsampling. You must use [UpSampling2D](https://keras.io/api/layers/reshaping_layers/up_sampling2d/) and **not** [Conv2DTranspose](https://keras.io/keras_core/api/layers/convolution_layers/convolution2d_transpose/). (What's the difference by the way?)"]},{"cell_type":"markdown","metadata":{"id":"PKKWQM-xVlfI"},"source":["##4. Task 3: Variational Autoencoder\n","Recall that variational autoencoders (VAE) are designed to learn smooth latent space representation. The problem with traditional autoencoders is that they tend to generate gaps in the latent space, making interpolation impossible. The purpose of this task is to see if this is actually the case in practise.\n","\n","Below is an implementation of a convolutional VAE. The structure of the code is very different from above. This is just to show you an alternative way to implement custom models in Keras."]},{"cell_type":"markdown","source":["###4.1 Encoder\n","Let's first build the encoder."],"metadata":{"id":"xp81T6EXZ54G"}},{"cell_type":"code","source":["import os\n","from keras import ops\n","from keras.layers import Layer\n","\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","class Sampling(Layer):\n","    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.seed_generator = keras.random.SeedGenerator(1337)\n","\n","    def call(self, inputs):\n","        z_mean, z_log_var = inputs\n","        batch = ops.shape(z_mean)[0]\n","        dim = ops.shape(z_mean)[1]\n","        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n","        return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n","\n","latent_dim = 2\n","\n","encoder_inputs = keras.Input(shape=(28, 28, 1))\n","x = Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n","x = Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = Flatten()(x)\n","x = Dense(16, activation=\"relu\")(x)\n","z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n","z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n","z = Sampling()([z_mean, z_log_var])\n","encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n","encoder.summary()"],"metadata":{"id":"tMyylsQFCyWN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that we have designed the encoder such that it outputs the logarithm of the variance, `z_log_var` = $\\log(\\sigma^2)$. When sampling we need the standard deviation: $\\sigma$ = `ops.exp(0.5 * z_log_var)` = $\\exp(\\frac{1}{2}\\log(\\sigma^2))=\\exp(\\frac{1}{2}2\\log(\\sigma))=\\exp(\\log(\\sigma))=\\sigma$, where we have used that $\\log(x^2) = 2\\log(x)$.\n","\n","####4.1.1 Questions\n","1. What do you think the `Sampling` layer does?\n","2. The encoder outputs three variables: `[z_mean, z_log_var, z]`. What do you think they represent?"],"metadata":{"id":"xHRx9vGWEtb0"}},{"cell_type":"markdown","source":["###4.2 Decoder\n","Now, let's build the decoder."],"metadata":{"id":"1Evgfi0vFMQ-"}},{"cell_type":"code","source":["latent_inputs = Input(shape=(latent_dim,))\n","x = Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n","x = Reshape((7, 7, 64))(x)\n","x = Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","decoder_outputs = Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n","decoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n","decoder.summary()"],"metadata":{"id":"-t5H-gy-Ed-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yIYCApwOTA11"},"source":["###4.3 VAE\n","We can now build the VAE. This is where to code is structured differently from what you have seen before. Specifically, this example shows how to create a custom training step using the [Trainer pattern](https://keras.io/examples/keras_recipes/trainer_pattern/). This pattern overrides the `train_step()` method of the `keras.Model` class, allowing for training loops beyond plain supervised learning."]},{"cell_type":"code","source":["from keras.metrics import Mean\n","class VAE(Model):\n","    def __init__(self, encoder, decoder, **kwargs):\n","        super().__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.total_loss_tracker = Mean(name=\"total_loss\")\n","        self.reconstruction_loss_tracker = Mean(\n","            name=\"reconstruction_loss\"\n","        )\n","        self.kl_loss_tracker = Mean(name=\"kl_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [\n","            self.total_loss_tracker,\n","            self.reconstruction_loss_tracker,\n","            self.kl_loss_tracker,\n","        ]\n","\n","    def train_step(self, data):\n","        # This function is called at every iteration of the model training.\n","        # The gradient tape just records operations for automatic gradient computation.\n","        with tf.GradientTape() as tape:\n","            z_mean, z_log_var, z = self.encoder(data)\n","            reconstruction = self.decoder(z)\n","            reconstruction_loss = ops.mean(\n","                ops.sum(\n","                    keras.losses.binary_crossentropy(data, reconstruction),\n","                    axis=(1, 2),\n","                )\n","            )\n","            kl_loss = -0.5 * (1 + z_log_var/2 - ops.square(z_mean) - ops.exp(z_log_var))\n","            kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n","            total_loss = reconstruction_loss + kl_loss\n","        grads = tape.gradient(total_loss, self.trainable_weights)\n","        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n","        self.total_loss_tracker.update_state(total_loss)\n","        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n","        self.kl_loss_tracker.update_state(kl_loss)\n","        return {\n","            \"loss\": self.total_loss_tracker.result(),\n","            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n","            \"kl_loss\": self.kl_loss_tracker.result(),\n","        }"],"metadata":{"id":"VHqfDKVjFtB8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####4.3.1 Questions\n","\n","Recall that the VAE loss consists of two terms:\n","\n","- A reconstruction term (or similarity term)\n","- and a KL divergence term\n","\n","You can read more about it here: https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n","\n","The KL term is:\n","\n","![alt text](https://miro.medium.com/max/520/1*uEAxCmyVKxzZOJG6afkCCg.png)\n","\n","*Note* that we are \"cheating\" a little bit with the MNIST dataset by exploiting that the pixel intensities are either close to 0 or 1. In that case, rather than using the mean squared error for the reconstruction term, we can use the binary cross entropy. This makes the training a lot faster.\n","\n","1. Can you identify the individual loss terms in the code block above?\n","2. The `kl_loss` is multiplied by `0.5`, but this could be any scalar like `0.1` or `2.1`. What do you think is the role of this scalar? (hint: it is a hyperparameter)."],"metadata":{"id":"PhBzM2h2Mh-S"}},{"cell_type":"markdown","metadata":{"id":"aMnJJSHMazvZ"},"source":["###4.4 Training\n","Let's train the model."]},{"cell_type":"code","source":["vae = VAE(encoder, decoder)\n","vae.compile(optimizer=keras.optimizers.Adam())\n","vae.fit(x_train, epochs=30, batch_size=128)"],"metadata":{"id":"a8NPRvRrGsV4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"__jYiiWaa4tp"},"source":["###4.5 Plot the latent space representation"]},{"cell_type":"code","metadata":{"id":"CSyV2LxfvYj_"},"source":["z_mean, _, _ = encoder.predict(x_train, batch_size=batch_size, verbose=False)\n","import matplotlib as mpl\n","mpl.rc('image', cmap='jet')\n","plt.figure(figsize=(12, 10))\n","plt.scatter(z_mean[:, 0], z_mean[:, 1], c=np.argmax(y_train,axis=1))\n","plt.colorbar()\n","plt.xlabel(\"z[0]\")\n","plt.ylabel(\"z[1]\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rmdhxSxk2LKw"},"source":["n = 20\n","digit_size = 28\n","figure = np.zeros((digit_size * n, digit_size * n))\n","# linearly spaced coordinates corresponding to the 2D plot\n","# of digit classes in the latent space\n","grid_x = np.linspace(-2, 2, n)\n","grid_y = np.linspace(-2, 2, n)[::-1]\n","\n","for i, yi in enumerate(grid_y):\n","    for j, xi in enumerate(grid_x):\n","        z_sample = np.array([[xi, yi]])\n","        x_decoded = decoder.predict(z_sample, verbose=False)\n","        digit = x_decoded[0].reshape(digit_size, digit_size)\n","        figure[i * digit_size: (i + 1) * digit_size,\n","                j * digit_size: (j + 1) * digit_size] = digit\n","\n","plt.figure(figsize=(10, 10))\n","pixel_range = np.linspace(digit_size/2,n*digit_size-digit_size/2,n)\n","sample_range_x = np.round(grid_x, 1)\n","sample_range_y = np.round(grid_y, 1)\n","plt.xticks(pixel_range, sample_range_x)\n","plt.yticks(pixel_range, sample_range_y)\n","plt.xlabel(\"z[0]\")\n","plt.ylabel(\"z[1]\")\n","plt.imshow(figure, cmap='Greys_r')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRrVCxqraIS5"},"source":["####4.5.1 Questions\n","1. What do you think of this latent representation? In terms of quality? In terms of smoothness? Compare to the same plot for the traditional convolutional autoencoder.\n","2. Which digits does the model faithfully reconstruct? Which digits does it have trouble reconstructing? Why?\n","3. What happens if you set the weight of the KL term to, say 5 (`kl_loss *= 5`), and re-train the model?"]},{"cell_type":"markdown","metadata":{"id":"wR5jbEkPbYIa"},"source":["###4.6 Encoding, decoding and latent space interpolation\n","Like we did for the traditional conovolutional autoencoder (see section 2.6), your task is to\n","\n","1. Encode an image of a 7 and an image of a 9.\n","2. Decode the encodings (to generate reconstructed images)\n","3. Interpolate between the two digits in latent space"]},{"cell_type":"markdown","metadata":{"id":"SDTMCG_d21ue"},"source":["The challenge here is that the encoder expects a fixed batch size (of 256 in our case). So you cannot just input one image to the model. The solution is to first process the training data set in batches of 256, and store the encodings:"]},{"cell_type":"code","metadata":{"id":"wK3ocjvz20YT"},"source":["# Get 2D encoding of all test examples (in batches of 100 images)\n","num_samples = int(np.floor(x_test.shape[0] / batch_size) * batch_size)\n","z_test = np.zeros((num_samples,latent_dim))\n","num_batches = int(num_samples / batch_size)\n","for i in range(num_batches):\n","  batch = x_test[i*batch_size:(i+1)*batch_size,:,:,:]\n","  [z_m, z_lv, z_val] = encoder.predict(batch,batch_size=batch_size, verbose=False)\n","  z_test[i*batch_size:(i+1)*batch_size,:] = z_m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDZwMmg49oOF"},"source":["# Draw two samples (a 7 and a 9)\n","y_test_category = np.argmax(y_test,axis=1)\n","ix7 = np.where(y_test_category==7)[0][1]\n","ix9 = np.where(y_test_category==9)[0][1]\n","\n","print('Original input images')\n","plt.subplot(221);plt.imshow(x_test[ix7,:].squeeze(),cmap='gray')\n","plt.subplot(222);plt.imshow(x_test[ix9,:].squeeze(),cmap='gray')\n","plt.show()\n","\n","# Subtask 1 (encoding): Calculate the latent representation of each sample\n","z7 = z_test[ix7,:]\n","z9 = z_test[ix9,:]\n","\n","# Subtask 2 (decoding): Reconstruct images from the two latent vectors\n","x_hat_7 = decoder.predict(z7.reshape(1,???)) # Fill in the ??? - same code as in 2.4\n","x_hat_9 = decoder.predict(z9.reshape(1,???)) # Fill in the ??? - same code as in 2.4\n","\n","# Show reconstruction\n","plt.subplot(223);plt.imshow(x_hat_7.squeeze(),cmap='gray')\n","plt.subplot(224);plt.imshow(x_hat_9.squeeze(),cmap='gray')\n","\n","N = 8\n","interp_features = np.zeros((N,latent_dim))\n","for i in range(latent_dim):\n","  interp_features[:,i] = np.linspace(z7[i].squeeze(),z9[i].squeeze(),N)\n","\n","plt.figure(figsize=(20,6))\n","for i in range(N):\n","  x = interp_features[i,:].reshape(1,latent_dim)\n","  out = decoder.predict(x)\n","  plt.subplot(1,N,i+1)\n","  plt.imshow(out.squeeze(),cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg5OtlGMQLYm"},"source":["**Note:** If you want to make nicer reconstructions and better interpolations, increase the latent dimensionality (latent_dim) and re-train the model."]},{"cell_type":"markdown","metadata":{"id":"a_5WzyIk9LFB"},"source":["##5. Task 4: Generative Adversarial Networks\n","Last year's version of this task was based on [this tutorial](https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py). Its an implementation of a Deep Convolutional GAN (DCGAN) for MNIST.\n","\n","Unfortunately that doesn't work anymore and I haven't been able to figure out why. Therefore, I advice you to go through this tutorial instead: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb\n","\n","You can load it as a notebook in Colab using [this link](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb)\n","\n","###5.1 Recommended tasks (advanced and not mandatory)\n","1. See if you can figure out how to decrease the dimensionality of the latent space from 100 to 2? Then retrain the model.\n","\n","2. Extend the code such that you can train the model to learn a 2D latent representation of MNIST and subsequently make it generate images based on some 2D latent vector that you specify. Use this to make a plot of the 2D latent space, like we did above. What do you observe?\n","\n","###5.2 Conditional GANs (not mandatory)\n","The original GAN has no knowledge, and hence no understanding of the data's class labels. This implies that if you tell a traditional GAN to generate an image of a digit, the GAN will most likely generate a realistically looking digit, but it will have absolutely no clue about which digit it is.\n","\n","Conditional GANs (CGANs) aim to solve this issue by telling both the generator and the discriminator what the class label is. Specifically, CGAN concatenates a one-hot vector y to the random noise vector z to result in an architecture that looks like this, where `y` is a one-encoding of the class label.\n","\n","![alt text](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559840765144_Screenshot+2019-06-06+at+10.35.29+PM.png)\n","\n","If you have more time, you could try out the CGAN tutorial:\n","\n","- https://keras.io/examples/generative/conditional_gan/\n","- Colab link: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/conditional_gan.ipynb\n","\n","How does it work?\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"VM8KYT38Ekd3"},"source":[],"execution_count":null,"outputs":[]}]}