{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JQpP-WhCh4s5Kjtau4bjNqd-ZRoEhiSm","timestamp":1730883497899},{"file_id":"1y-F8LTD8O16pXi3zyFG-n7ttfEaNglb9","timestamp":1574776593798},{"file_id":"17xYRON1eFvV0aIDX7rFKuG6r70JEG-z7","timestamp":1574687476240},{"file_id":"1_MTu7dAOUHoOu22zbcF5BEE6ieoVXPkk","timestamp":1574230375682},{"file_id":"1RceoxxzW6RAw1TtiPedQSLhjC9eJqA6B","timestamp":1574195045162},{"file_id":"1G_-muvNCX1ar3fqh8PyCoxaMweWHXb2V","timestamp":1573721215051},{"file_id":"1my2UlrD_Ca4OMZARR5TaoyMdM0tXgQYc","timestamp":1568240058647}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"REaV1GblZovs"},"source":["# Visualization of filters by reconstruction\n","In this part of the lab we consider reconstruction-based filter visualization.\n","\n","The content is inspired by [this blog post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html), which unfortunately is deprecated.\n","\n","We will this this one instead:\n","\n","- Blog post: https://keras.io/examples/vision/visualizing_what_convnets_learn/\n","\n","- Source: https://github.com/keras-team/keras-io/blob/master/examples/vision/visualizing_what_convnets_learn.py\n","\n","- Colab version: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/visualizing_what_convnets_learn.ipynb\n","\n","\n","**Before we start - remember to set runtime to GPU**"]},{"cell_type":"markdown","metadata":{"id":"ohj31dy5QyeA"},"source":["## Set up network\n","We will be using VGG16 like in Lab 10, part 1."]},{"cell_type":"code","metadata":{"id":"5Xdbdit5Q1C_"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# The dimensions of our input image\n","img_width = 224\n","img_height = 224\n","\n","# Our target layer: we will visualize the filters from this layer.\n","# See `model.summary()` for list of layer names, if you want to change this.\n","layer_name = \"block3_conv3\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zINSWTEdRO1W"},"source":["# Build a VGG16 model loaded with pre-trained ImageNet weights\n","model = keras.applications.VGG16(weights=\"imagenet\", include_top=False)\n","\n","# Set up a model that returns the activation values for our target layer\n","layer = model.get_layer(name=layer_name)\n","feature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dWvdIoHFRmzw"},"source":["## Set up the gradient ascent process\n","The \"loss\" we will maximize is simply the mean of the activation of a specific filter in our target layer. To avoid border effects, we exclude border pixels."]},{"cell_type":"code","metadata":{"id":"rYWEpNQTRtim"},"source":["def compute_loss(input_image, filter_index):\n","    activation = feature_extractor(input_image)\n","    # We avoid border artifacts by only involving non-border pixels in the loss.\n","    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n","    return tf.reduce_mean(filter_activation)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AxeP9bCNRz9Q"},"source":["Our gradient ascent function simply computes the gradients of the loss above with respect to the input image, and updates the image so as to move it towards a state that will activate the target filter more strongly."]},{"cell_type":"code","metadata":{"id":"9muuwZpDR9xV"},"source":["@tf.function\n","def gradient_ascent_step(img, filter_index, learning_rate):\n","    with tf.GradientTape() as tape:\n","        tape.watch(img)\n","        loss = compute_loss(img, filter_index)\n","\n","    # Compute gradients.\n","    grads = tape.gradient(loss, img)\n","\n","    # Normalize gradients.\n","    grads = tf.math.l2_normalize(grads)\n","\n","    # Perform gradient ascent step\n","    img += learning_rate * grads\n","    return loss, img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pqB55jY8Wr4y"},"source":["Some background reading in case you are interested:\n","- https://www.tensorflow.org/api_docs/python/tf/GradientTape\n","- https://www.tensorflow.org/guide/autodiff"]},{"cell_type":"markdown","metadata":{"id":"LDD-J8deAV3p"},"source":["## Set up the end-to-end filter visualization loop\n","Our process is as follows:\n","\n","- Start from a random image that is close to \"all gray\" (i.e. visually netural).\n","- Repeatedly apply the gradient ascent step function defined above\n","- Convert the resulting input image back to a displayable form, by normalizing it, center-cropping it, and restricting it to the [0, 255] range.\n"]},{"cell_type":"code","metadata":{"id":"hRInJgueScsr"},"source":["def initialize_image():\n","    # We start from a gray image with some random noise\n","    img = tf.random.uniform((1, img_width, img_height, 3))\n","\n","    # VGG16 expects inputs in the range [-128, +128].\n","    # Here we scale our random inputs to [-32, +32]\n","    return (img - 0.5) * 64\n","\n","def deprocess_image(img):\n","    # Normalize array: center on 0., ensure variance is 0.15\n","    img -= img.mean()\n","    img /= img.std() + 1e-5\n","    img *= 0.15\n","\n","    # Center crop\n","    img = img[25:-25, 25:-25, :]\n","\n","    # Clip to [0, 1]\n","    img += 0.5\n","    img = np.clip(img, 0, 1)\n","\n","    # Convert to RGB array\n","    img *= 255\n","    img = np.clip(img, 0, 255).astype(\"uint8\")\n","    return img\n","\n","def visualize_filter(filter_index):\n","    # We run gradient ascent for 30 steps\n","    iterations = 30\n","    learning_rate = 200.0 # learning must be high for this to converge fast enough\n","    img = initialize_image()\n","    for iteration in range(iterations):\n","        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n","\n","    # Decode the resulting input image\n","    img = deprocess_image(img[0].numpy())\n","    return loss, img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPVp13gnSk5q"},"source":["Let's try it out with filter 0 in the target layer:"]},{"cell_type":"code","metadata":{"id":"TZLaOrrlSnb7"},"source":["from IPython.display import Image, display\n","\n","loss, img = visualize_filter(0)\n","keras.preprocessing.image.save_img(\"0.png\", img)\n","\n","display(Image(\"0.png\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DiNc_MgMSvah"},"source":["## Visualize the first 16 filters in the target layer\n","Now, let's make a 4x4 grid of the first 16 filters in the target layer to get of feel for the range of different visual patterns that the model has learned."]},{"cell_type":"code","metadata":{"id":"edyOQr07UVkX"},"source":["# Compute image inputs that maximize per-filter activations\n","# for the first 16 filters of our target layer\n","all_imgs = []\n","for filter_index in range(16):\n","    print(\"Processing filter %d\" % (filter_index,))\n","    loss, img = visualize_filter(filter_index)\n","    all_imgs.append(img)\n","\n","# Build a black picture with enough space for\n","# our 4 x 4 filters of size 128 x 128, with a 5px margin in between\n","margin = 5\n","n = 4\n","cropped_width = img_width - 25 * 2\n","cropped_height = img_height - 25 * 2\n","width = n * cropped_width + (n - 1) * margin\n","height = n * cropped_height + (n - 1) * margin\n","stitched_filters = np.zeros((width, height, 3))\n","\n","# Fill the picture with our saved filters\n","for i in range(n):\n","    for j in range(n):\n","        img = all_imgs[i * n + j]\n","        stitched_filters[\n","            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n","            (cropped_height + margin) * j : (cropped_height + margin) * j\n","            + cropped_height,\n","            :,\n","        ] = img\n","keras.preprocessing.image.save_img(\"stiched_filters.png\", stitched_filters)\n","\n","from IPython.display import Image, display\n","\n","display(Image(\"stiched_filters.png\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5uukAQ0oFZ2F"},"source":["## Questions\n","1. The computational graph is changed in order to derive the filter response image. How is the graph modified?\n","2. How is the loss calculated? What is being maximized/minimized?\n","3. Try visualizing other filters from another layer (the code block below might be useful). **Note:** You may need to adjust `iterations` and `learning_rate`"]},{"cell_type":"code","metadata":{"id":"WUvLGjTCU9xf"},"source":["# Print names of all conv layers\n","for i, layer in enumerate(model.layers):\n","\n","  # check for convolutional layer\n","  layer_type = layer.__class__.__name__\n","\n","  if 'Conv' not in layer_type:\n","    continue\n","\n","  # get filter weights\n","  layer_name = layer.name\n","  input_shape = layer.input.shape\n","  output_shape = layer.output.shape\n","  filter_shape = layer.get_weights()[0].shape\n","\n","  print(f\"Layer {i} has name {layer_name}, input shape {input_shape}, filter shape {filter_shape}, and output shape {output_shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"01-ookNsIH3U"},"source":["# Grad-CAM\n","If time permits, you may also want to try out Grad-CAM.\n","\n","The code is based on [this tutorial](https://keras.io/examples/vision/grad_cam/)."]},{"cell_type":"code","metadata":{"id":"_QYgZqSiIPEu"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# Display\n","from IPython.display import Image, display\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDv3l7Y5aUkY"},"source":["You can change these to another model.\n","\n","To get the values for last_conv_layer_name use model.summary() to see the names of all layers in the model."]},{"cell_type":"code","metadata":{"id":"m2XUcdL_ZY_H"},"source":["model_builder = keras.applications.vgg16.VGG16\n","img_size = (224, 224)\n","preprocess_input = keras.applications.vgg16.preprocess_input\n","decode_predictions = keras.applications.vgg16.decode_predictions\n","\n","last_conv_layer_name = \"block5_conv3\"\n","\n","# The local path to our target image\n","img_path = keras.utils.get_file(\n","    \"cat.jpg\", \"https://github.com/klaverhenrik/Deep-Learing-for-Visual-Recognition-2023/raw/main/data/cat.jpg\"\n",")\n","\n","display(Image(img_path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"avVS3rGSaydq"},"source":["## The Grad-CAM algorithm"]},{"cell_type":"code","metadata":{"id":"iiAkvdq0auMW"},"source":["def get_img_array(img_path, size):\n","    # `img` is a PIL image of size 224x224\n","    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n","    # `array` is a float32 Numpy array of shape (224, 224, 3)\n","    array = keras.preprocessing.image.img_to_array(img)\n","    # We add a dimension to transform our array into a \"batch\"\n","    # of size (1, 224, 224, 3)\n","    array = np.expand_dims(array, axis=0)\n","    return array\n","\n","\n","def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer as well as the output predictions\n","    grad_model = tf.keras.models.Model(\n","        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n","    )\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    # This is the gradient of the output neuron (top predicted or chosen)\n","    # with respect to the activations of the last conv layer\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over the channels of the activations of the last conv layer\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the activations (feature map) array\n","    # by \"how important this channel is\" with respect to the top predicted class\n","    # then sum all the channels to obtain the heatmap class activation\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","    return heatmap.numpy()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LocIpviGbSJA"},"source":["Let's test-drive it"]},{"cell_type":"code","metadata":{"id":"6Rm6QXtwbPtU"},"source":["# Prepare image\n","img_array = preprocess_input(get_img_array(img_path, size=img_size))\n","\n","# Make model\n","model = model_builder(weights=\"imagenet\")\n","\n","# Remove last layer's softmax\n","model.layers[-1].activation = None\n","\n","# Print what the top predicted class is\n","preds = model.predict(img_array)\n","print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n","\n","# Generate class activation heatmap\n","heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n","\n","# Display heatmap\n","plt.matshow(heatmap)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MLiS4rEgbblo"},"source":["Create a superimposed visualization"]},{"cell_type":"code","metadata":{"id":"lJIVZ9QpbVpS"},"source":["def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n","    # Load the original image\n","    img = keras.preprocessing.image.load_img(img_path)\n","    img = keras.preprocessing.image.img_to_array(img)\n","\n","    # Rescale heatmap to a range 0-255\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # Use jet colormap to colorize heatmap\n","    jet = cm.get_cmap(\"jet\")\n","\n","    # Use RGB values of the colormap\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","\n","    # Create an image with RGB colorized heatmap\n","    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n","    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n","    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","    # Superimpose the heatmap on original image\n","    superimposed_img = jet_heatmap * alpha + img\n","    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n","\n","    # Save the superimposed image\n","    superimposed_img.save(cam_path)\n","\n","    # Display Grad CAM\n","    display(Image(cam_path))\n","\n","\n","save_and_display_gradcam(img_path, heatmap)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQdG5jfublFS"},"source":["## Let's try another image\n","We will see how the grad cam explains the model's outputs for a multi-label image. Let's try an image with a cat and a dog together, and see how the grad cam behaves."]},{"cell_type":"code","metadata":{"id":"7-e05emcbfIB"},"source":["img_path = keras.utils.get_file(\n","    \"cat_and_dog.jpg\",\n","    \"https://storage.googleapis.com/petbacker/images/blog/2017/dog-and-cat-cover.jpg\",\n",")\n","\n","display(Image(img_path))\n","\n","# Prepare image\n","img_array = preprocess_input(get_img_array(img_path, size=img_size))\n","\n","# Print what the two top predicted classes are\n","preds = model.predict(img_array)\n","print(\"Predicted:\", decode_predictions(preds, top=2)[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-YVS-2mbpbk"},"source":["# dog heatmap\n","heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=260)\n","save_and_display_gradcam(img_path, heatmap)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jmn-6xSbwhj"},"source":["# Cat heatmap\n","heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=285)\n","save_and_display_gradcam(img_path, heatmap)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJBlCmh8b5w9"},"source":["## Question\n","Explain all of the steps that are performed to calculate the heatmaps (see `make_gradcam_heatmap(...)`). Like, how is the computational graph modified? What does `pooled_grads`represent? Etc."]},{"cell_type":"code","metadata":{"id":"AzO7Ct83b009"},"source":[],"execution_count":null,"outputs":[]}]}