{"cells":[{"cell_type":"markdown","source":["# Conditioned diffusion models\n","Imagine you want to train a diffusion model on a dataset with 10 classes. How do you control which class is generated? This lab shows one way to add *conditioning information* to a diffusion model. Specifically, we’ll train a class-conditioned diffusion model on FashionMNIST, where we can specify which class we’d like the model to generate at inference time."],"metadata":{"id":"vkYlbJThzdrQ"}},{"cell_type":"markdown","metadata":{"id":"RBVmnjjE-Rtp"},"source":["## Setup and Data Prep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pT-zaDUh-Rtp"},"outputs":[],"source":["%pip install -q diffusers  # Installing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2yyO5qH-Rtr"},"outputs":[],"source":["import torch\n","import torchvision\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader\n","from diffusers import DDPMScheduler, UNet2DModel\n","from matplotlib import pyplot as plt\n","from tqdm.auto import tqdm\n","# Identify and choose device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using device: {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OeMklCBz-Rtr"},"outputs":[],"source":["#Loading the Fahion minst dataset a dataset containing images and numerical class labels\n","dataset = torchvision.datasets.FashionMNIST(root=\"FashionMNIST/\", train=True, download=True, transform=torchvision.transforms.ToTensor())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1nibTd9x4oY"},"outputs":[],"source":["# Stealing a predefined dataloader to show examples\n","train_dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n","\n","# Showing examples from dataset\n","x, y = next(iter(train_dataloader))\n","print('Input shape:', x.shape)\n","print('Labels:', y)\n","plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys');"]},{"cell_type":"markdown","source":["## Creating a Class-Conditioned UNet\n","\n","The way we’ll feed in the class conditioning is as follows:\n","\n","- Create a standard UNet2DModel with some additional input channels\n","- Map the class label to a learned vector of shape (`class_emb_size`) via an embedding layer\n","- Concatenate this information as extra channels for the internal UNet input with `net_input = torch.cat((x, class_cond), 1)`\n","- Feed this net_input (which has (`class_emb_size+1`) channels in total) into the UNet to get the final prediction\n","\n","\n","In this example I’ve set the `class_emb_size` to 4, but this is completely arbitrary and you could explore having it size 1 (to see if it still works), size 10 (to match the number of classes), or replacing the learned nn.Embedding with a simple one-hot encoding of the class label directly.\n","\n","This is what the implementation looks like:"],"metadata":{"id":"DbTzMbvDK27i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aQlPCTK-Rtr"},"outputs":[],"source":["class class_conditioned_unet(nn.Module):\n","  def __init__(self, num_classes=10, class_emb_size=4):\n","    super().__init__()\n","\n","    # Embedding layer will map the class label to a vector of size class_emb_size\n","    self.class_emb = nn.Embedding(num_classes, class_emb_size)\n","\n","    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n","    self.model = UNet2DModel(\n","        sample_size=28,           # the target image resolution\n","        in_channels=1 + class_emb_size, # Additional input for class condition\n","        out_channels=1,           # grea\n","        layers_per_block=2,       # how many ResNet layers to use per UNet block\n","        block_out_channels=(32, 64, 64),\n","        down_block_types=(\n","            \"DownBlock2D\",        # Regular ResNet downsampling block\n","            \"AttnDownBlock2D\",    # Downsampling block with spatial self-attention\n","            \"AttnDownBlock2D\",\n","        ),\n","        up_block_types=(\n","            \"AttnUpBlock2D\",\n","            \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n","            \"UpBlock2D\",          # a regular ResNet upsampling block\n","          ),\n","    )\n","\n","  # forward method  takes the class labels as an additional argument\n","  def forward(self, x, t, class_labels):\n","    # Shape of x:\n","    bs, ch, w, h = x.shape\n","\n","    # class conditioning in right shape to give as additional input\n","    class_cond = self.class_emb(class_labels) # Map to embedding dimension\n","    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h) # Expand to contain class embedding\n","\n","    # Net input is now x and class cond concatenated together along dimension 1\n","    net_input = torch.cat((x, class_cond), 1) # concat to one dim input\n","\n","    # Feed this to the UNet alongside the timestep and return the prediction\n","    return self.model(net_input, t).sample # (bs, 1, 28, 28)"]},{"cell_type":"markdown","source":["If any of the shapes or transforms are confusing, add in print statements to show the relevant shapes and check that they match your expectations. I’ve also annotated the shapes of some intermediate variables in the hopes of making things clearer."],"metadata":{"id":"0ghQSZE5LpCh"}},{"cell_type":"markdown","source":["## Training and Sampling\n","\n","Where previously we’d do something like `prediction = unet(x, t)` we’ll now add the correct labels as a third argument (`prediction = unet(x, t, y)`) during training, and at inference we can pass whatever labels we want and if all goes well the model should generate images that match. `y` in this case is the labels of the FashionMNIST digits, with values from 0 to 9.\n","\n","The training loop is very similar to the unconditioned diffusion model. We’re now predicting the noise to match the objective expected by the default DDPMScheduler which we’re using to add noise during training and to generate samples at inference time. Training takes a while - speeding this up could be a fun mini-project, but most of you can probably just skim the code (and indeed this whole notebook) without running it since we’re just illustrating an idea."],"metadata":{"id":"PJTMtFc2Lto0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7LJndHq-Rts"},"outputs":[],"source":["# Create noise\n","noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOcosBrq-Rts"},"outputs":[],"source":["# Train dataloader\n","train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n","\n","# Number of epochs\n","n_epochs = 5\n","\n","# Our network\n","net = class_conditioned_unet().to(device)\n","\n","# Our loss function\n","loss_fn = nn.MSELoss()\n","\n","# Adam  optimizer and learning rate of\n","opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n","\n","# Save loss for plot later\n","losses = []\n","\n","# Thraditional training loop\n","for epoch in range(n_epochs):\n","    for x, y in tqdm(train_dataloader):\n","\n","        # Retrive data and corrupt it with noise\n","        x = x.to(device) * 2 - 1\n","        y = y.to(device)\n","        noise = torch.randn_like(x) # Generate noise\n","        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(device) # Determine the dregree in which the image is gradually turned into noise,\n","        noisy_x = noise_scheduler.add_noise(x, noise, timesteps) # Add noise  for the given timestep\n","\n","        # Get the model prediction\n","        pred = net(noisy_x, timesteps, y)\n","\n","        # Calculate the loss\n","        loss = loss_fn(pred, noise)\n","\n","        # Backprop and update the params:\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","        # Store loss\n","        losses.append(loss.item())\n","\n","    # Average of the last 50\n","    avg_loss = sum(losses[-50:])/50\n","    print(f'Finished epoch {epoch}. Average of the last 50 loss values: {avg_loss:05f}')\n","\n","# View the loss curve\n","plt.plot(losses)"]},{"cell_type":"markdown","metadata":{"id":"ji_kIVBe-Rtt"},"source":["Once training finishes, we can sample some images feeding in different labels as our conditioning:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZeC4_njJ-Rtt"},"outputs":[],"source":["def prediction(a, b):\n","  a = a # number of images/predictions for each number given as input\n","  b = b # range of number u want to predict remember we start in zero\n","  z = a * b\n","  x = torch.rand(z, 1, 28, 28).to(device)\n","  y = torch.tensor([[i]*a for i in range(b)]).flatten().to(device)\n","\n","  # Sampling loop\n","  for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n","\n","    # Do model prediction\n","      with torch.no_grad():\n","          residual = net(x, t, y)\n","\n","    # Update sample with step\n","      x = noise_scheduler.step(residual, t, x).prev_sample\n","\n","  # Results\n","  fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n","  ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-1, 1), nrow=b)[0], cmap='Greys')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BATanoEOeacs"},"outputs":[],"source":["text = int(input(\"Number of predictions for each category (try 1): \"))\n","cnt = int(input(\"Number of clothes categories you want to predict (try 10, then categories 0, 1, ..., 9 will be predicted): \"))\n","prediction(text, cnt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QGAPaoJ-Rtt"},"outputs":[],"source":["# Exercise (optional): Try this with FashionMNIST. Tweak the learning rate, batch size and number of epochs.\n","# Can you get some decent-looking fashion images with less training time than the example above?"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1OPJwJrl_gir562VN0YF8ISamZV4jA0x8","timestamp":1701250059397},{"file_id":"1Ot7n_trFqFfKWG4HVKKWBdaCvEFy0dpK","timestamp":1700061866235},{"file_id":"https://github.com/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb","timestamp":1700052725123}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
