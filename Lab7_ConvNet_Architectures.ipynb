{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9_efjnXBr7kN"},"source":["# ConvNet Architectures\n","The purpose of this lab is to teach to design your own ConvNets from scratch (and implement other people's architectures from research papers).\n","\n","If you instead feel like catching up on transfer learning, you can revisit one of the following:\n","- [Lab2](https://github.com/klaverhenrik/Deep-Learning-for-Visual-Recognition-2024/blob/main/Lab2_FeatureExtractionAndTransferLearning.ipynb), Task 9 (Keras)\n","- [Lab 5 - Transfer learning](https://github.com/klaverhenrik/Deep-Learning-for-Visual-Recognition-2024/blob/main/Lab5_PyTorch_TransferLearning.ipynb) (PyTorch).\n","\n","**Remember to enable GPU**"]},{"cell_type":"markdown","metadata":{"id":"jWsWu8S_zCpw"},"source":["**NOTE:** In case you have trouble running Keras/TensorFlow in Colab, try one of the following:"]},{"cell_type":"code","metadata":{"id":"pm72GSu1yb6Q"},"source":["# Try this\n","#!pip install --upgrade tensorflow==1.8.0\n","\n","# ... or this\n","#%tensorflow_version 1.x\n","\n","# Check TensorFlow version\n","#import tensorflow as tf\n","#print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPS1O-4hsuPO"},"source":["## 1. Download data and train simple ConvNet with Keras\n","First download the CIFAR10 dataset:"]},{"cell_type":"code","metadata":{"id":"nDPpCY5nr6vs"},"source":["from __future__ import print_function\n","from tensorflow import keras\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, BatchNormalization, ReLU\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","\n","num_classes = 10\n","\n","# Load the CIFAR10 data.\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","# Input image dimensions.\n","input_shape = x_train.shape[1:]\n","\n","# Normalize data.\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255\n","\n","# Subtract pixel mean (zero-center)\n","x_train_mean = np.mean(x_train, axis=0)\n","x_train -= x_train_mean\n","x_test -= x_train_mean\n","\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0n0Ihg9xOBu"},"source":["Set up simple ConvNet:"]},{"cell_type":"code","metadata":{"id":"Q43HWFfavN3J"},"source":["inputs = Input(shape=input_shape)\n","\n","# Encoder (convolutional base)\n","x = Conv2D(8, kernel_size=(3, 3), activation='relu')(inputs)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(16, kernel_size=(3, 3), activation='relu')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n","encoded = Flatten()(x)\n","\n","# Decoder (2 fully connected layers)\n","x = Dense(64, activation='relu')(encoded)\n","x = Dropout(0.5)(x)\n","predictions = Dense(num_classes,activation='softmax')(x)\n","\n","# This creates a callable model that includes the Input layer and the prediction layer\n","model = Model(inputs=inputs, outputs=predictions)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(),\n","              metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHovxkaWxVeJ"},"source":["... and train (while training you may want to read ahead on Task 1):"]},{"cell_type":"code","metadata":{"id":"PVc45dFixT-b"},"source":["batch_size = 32\n","epochs = 4\n","\n","# This will do preprocessing and realtime data augmentation:\n","datagen = ImageDataGenerator(\n","    # set input mean to 0 over the dataset\n","    featurewise_center=False,\n","    # set each sample mean to 0\n","    samplewise_center=False,\n","    # divide inputs by std of dataset\n","    featurewise_std_normalization=False,\n","    # divide each input by its std\n","    samplewise_std_normalization=False,\n","    # apply ZCA whitening\n","    zca_whitening=False,\n","    # epsilon for ZCA whitening\n","    zca_epsilon=1e-06,\n","    # randomly rotate images in the range (deg 0 to 180)\n","    rotation_range=0,\n","    # randomly shift images horizontally\n","    width_shift_range=0.1,\n","    # randomly shift images vertically\n","    height_shift_range=0.1,\n","    # set range for random shear\n","    shear_range=0.,\n","    # set range for random zoom\n","    zoom_range=0.,\n","    # set range for random channel shifts\n","    channel_shift_range=0.,\n","    # set mode for filling points outside the input boundaries\n","    fill_mode='nearest',\n","    # value used for fill_mode = \"constant\"\n","    cval=0.,\n","    # randomly flip images\n","    horizontal_flip=True,\n","    # randomly flip images\n","    vertical_flip=False,\n","    # set rescaling factor (applied before any other transformation)\n","    rescale=None,\n","    # set function that will be applied on each input\n","    preprocessing_function=None,\n","    # image data format, either \"channels_first\" or \"channels_last\"\n","    data_format=None,\n","    # fraction of images reserved for validation (strictly between 0 and 1)\n","    validation_split=0.0)\n","\n","# Compute quantities required for featurewise normalization\n","# (std, mean, and principal components if ZCA whitening is applied).\n","datagen.fit(x_train)\n","\n","# Fit the model on the batches generated by datagen.flow().\n","history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                    validation_data=(x_test, y_test),\n","                    epochs=epochs, verbose=1, shuffle=True)\n","\n","# Score trained model.\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","print('Test loss:', scores[0])\n","print('Test accuracy:', scores[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ou1ry0kg3B8-"},"source":["Plot loss and accuracy curves"]},{"cell_type":"code","metadata":{"id":"gHvYQsFjyRP3"},"source":["import matplotlib.pyplot as plt\n","\n","def show_history(history):\n","  plt.figure(figsize=(20,6))\n","\n","  # summarize history for accuracy\n","  plt.subplot(121)\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('model accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'test'], loc='upper left')\n","\n","  # summarize history for loss\n","  plt.subplot(122)\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'test'], loc='upper left')\n","  plt.show()\n","\n","show_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0svLoKzu47CK"},"source":["###Bonus question (not related to today's topic):\n","- Has this network finished training? Why? Why not?"]},{"cell_type":"markdown","metadata":{"id":"FfZDz-Gy3n-b"},"source":["##2. Task 1: Write down the network architecture\n","The architecture of AlexNet (including output shapes) can be written in this format (see lecture 6 slides):\n","\n","```\n","[227x227x3] INPUT\n","[55x55x96] CONV1: 96 11x11 filters at stride 4, pad 0\n","[27x27x96] MAX POOL1: 3x3 filters at stride 2\n","[27x27x96] NORM1: Normalization layer\n","[27x27x256] CONV2: 256 5x5 filters at stride 1, pad 2\n","[13x13x256] MAX POOL2: 3x3 filters at stride 2\n","[13x13x256] NORM2: Normalization layer\n","[13x13x384] CONV3: 384 3x3 filters at stride 1, pad 1\n","[13x13x384] CONV4: 384 3x3 filters at stride 1, pad 1\n","[13x13x256] CONV5: 256 3x3 filters at stride 1, pad 1\n","[6x6x256] MAX POOL3: 3x3 filters at stride 2\n","[4096] FC6: 4096 neurons\n","[4096] FC7: 4096 neurons\n","[1000] FC8: 1000 neurons (class scores)\n","```\n","where `[...]` denotets the output shape of the layer."]},{"cell_type":"markdown","metadata":{"id":"jUOx8lIK4SL5"},"source":["Using the same format, write down the architecture of the network you just trained (you also need to specify stride and pad):"]},{"cell_type":"markdown","metadata":{"id":"CAcSObtv4jNW"},"source":["\n","```\n","[???] INPUT\n","[???]\n","...\n","```\n","\n"]},{"cell_type":"markdown","source":["Need help? Run this:"],"metadata":{"id":"g3p9hlAU2ijj"}},{"cell_type":"code","metadata":{"id":"xr-VVTTEIkRx"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2DK7wIOp7mzP"},"source":["##3. Task 2: Implement simple network architectures\n","###Task 2.1 Padding for preserving shape\n","Padding (or zero-padding) is mostly used when we want the output of a convolution layer to have the same shape is the input.\n","\n","Keras' Conv2D layer does not allow you to specify the padding directly (as an integer). Rather you can chose between `'valid'` and `'same'`. Check the [documentation](https://keras.io/api/layers/convolution_layers/convolution2d/) to see what the difference is. By changing the padding only, modify our model to have the following architecture.\n","\n","Also fill in the `???` (i.e., what is the pad size?):\n","```\n","[32x32x3] INPUT\n","[32x32x8] CONV1: 8 3x3 filters at stride 1, pad 1\n","[16x16x8] MAX POOL1: 2x2 filters at stride 2\n","[16x16x16] CONV2: 16 3x3 filters at stride 1, pad 1\n","[8x8x16] MAX POOL2: 2x2 filters at stride 2\n","[8x8x32] CONV3: 32 3x3 filters at stride 1, pad 1\n","[64] FC1: 64 neurons\n","[10] FC2: 10 neurons (class scores)\n","```\n","Note that the shape is the same before and after each convolution layer."]},{"cell_type":"code","metadata":{"id":"qI6kUuV_8Mhw"},"source":["# Encoder (convolutional base)\n","x = Conv2D(8, kernel_size=(3, 3), padding=???, activation='relu')(inputs)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(16, kernel_size=(3, 3), padding=???, activation='relu')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(32, kernel_size=(3, 3), padding=???, activation='relu')(x)\n","encoded = Flatten()(x)\n","\n","# Decoder (2 fully connected layers)\n","x = Dense(64, activation='relu')(encoded)\n","x = Dropout(0.5)(x)\n","predictions = Dense(num_classes,activation='softmax')(x)\n","\n","# This creates a callable model that includes the Input layer and the prediction layer\n","model = Model(inputs=inputs, outputs=predictions)\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRe-4VZBffMt"},"source":["**Note:** It is possible to exactly control zero-padding Keras. See [ZeroPadding2D](https://keras.io/api/layers/reshaping_layers/zero_padding2d/)."]},{"cell_type":"markdown","metadata":{"id":"JGmBgcqcBmxE"},"source":["###Task 2.2 Stride for downsampling\n","As mentioned in Lecture 4, *max pool* for downsampling is more or less deprecated. Instead, modern network architectures use `stride > 1` in the convolutiton layers for downsampling.\n","\n","Change the stride of the convolution layers to implement the following architecture.\n","\n","Also fill in the `???` (i.e., what is the stride and pad size?):\n","```\n","[32x32x3] INPUT\n","[16x16x8] CONV1: 8 3x3 filters at stride 2, pad 1\n","[8x8x16] CONV2: 16 3x3 filters at stride 2, pad 1\n","[4x4x32] CONV3: 32 3x3 filters at stride 2, pad 1\n","[64] FC1: 64 neurons\n","[10] FC2: 10 neurons (class scores)\n","```"]},{"cell_type":"code","metadata":{"id":"V4a6ZUSMFNc8"},"source":["# Encoder (convolutional base)\n","x = Conv2D(8, kernel_size=(3, 3), strides=???, padding=???, activation='relu')(inputs)\n","x = Conv2D(16, kernel_size=(3, 3), strides=???, padding=???, activation='relu')(x)\n","x = Conv2D(32, kernel_size=(3, 3), strides=???, padding=???, activation='relu')(x)\n","encoded = Flatten()(x)\n","\n","# Decoder (2 fully connected layers)\n","x = Dense(64, activation='relu')(encoded)\n","x = Dropout(0.5)(x)\n","predictions = Dense(num_classes,activation='softmax')(x)\n","\n","# This creates a callable model that includes the Input layer and the prediction layer\n","model = Model(inputs=inputs, outputs=predictions)\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMyfNVqiI8RQ"},"source":["###Task 2.3: Batch normalization\n","Always use batch normalization! It reduces the risk of vanishing gradients (i.e., it ensures that we have gradient flow through the whole network) and makes training faster.\n","\n","Batch normalization is performed after convolution, but before activation.\n","\n","To implement the architecture below you will need to separate the ReLU from the convolution layer. That is, instead of\n","\n","```\n","x = Conv2D(16, kernel_size=(3, 3), activation='relu')(x)\n","```\n","\n","write\n","\n","```\n","x = Conv2D(16, kernel_size=(3, 3))(x)\n","x = Activation('relu')(x)\n","```\n","\n","Required documentation:\n","- https://keras.io/activations/\n","- https://keras.io/layers/normalization/\n","\n","Your task is to implement this architecture, where I have written out batch normalization (BN) and ReLU activation explicitly:\n","\n","```\n","[32x32x3] INPUT\n","[16x16x8] CONV1: 8 3x3 filters at stride 2, pad 1\n","[16x16x8] BN1: Batch normalization\n","[16x16x8] ReLU\n","[8x8x16] CONV2: 16 3x3 filters at stride 2, pad 1\n","[8x8x16] BN2: Batch normalization\n","[8x8x16] ReLU\n","[4x4x32] CONV3: 32 3x3 filters at stride 2, pad 1\n","[4x4x32] BN3: Batch normalization\n","[4x4x32] ReLU\n","[64] FC1: 64 neurons\n","[10] FC2: 10 neurons (class scores)\n","```"]},{"cell_type":"code","metadata":{"id":"1zxxMAC074cA"},"source":["from tensorflow.keras.layers import BatchNormalization, Activation\n","\n","# Encoder (convolutional base)\n","??? # Your code goes here\n","\n","# Decoder (2 fully connected layers)\n","x = Dense(64, activation='relu')(encoded)\n","x = Dropout(0.5)(x)\n","predictions = Dense(num_classes,activation='softmax')(x)\n","\n","# This creates a callable model that includes the Input layer and the prediction layer\n","model = Model(inputs=inputs, outputs=predictions)\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5pvJ3vjNTtF"},"source":["###Task 2.4: Global average pooling\n","Recall that global average pooling takes an NxNxC volume and reduces it to a C-dimensional vector by averaging the NxN valus of each channel (c).\n","\n","Thus, we can replace the `Flatten` layer and the first `Dense` layer (i.e., fully connected layer) with a global average pooling layer.\n","\n","Check the documentation here:\n","https://keras.io/api/layers/pooling_layers/global_average_pooling2d/\n","\n","Your task is to implement the following architecture:\n","\n","```\n","[32x32x3] INPUT\n","[16x16x8] CONV1: 8 3x3 filters at stride 2, pad 1\n","[8x8x16] CONV2: 16 3x3 filters at stride 2, pad 1\n","[4x4x32] CONV3: 32 3x3 filters at stride 2, pad 1\n","[32] GAP: Global average pooling\n","[10] FC2: 10 neurons (class scores)\n","```"]},{"cell_type":"code","metadata":{"id":"5sSIJpdNLk4a"},"source":["from tensorflow.keras.layers import GlobalAveragePooling2D\n","\n","# Encoder (convolutional base)\n","???\n","\n","# Decoder (1 fully connected layer)\n","predictions = Dense(num_classes,activation='softmax')(x)\n","\n","# This creates a callable model that includes the Input layer and the prediction layer\n","model = Model(inputs=inputs, outputs=predictions)\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"seRuB8D_QrAp"},"source":["**Note** that because we removed a dense layer, the above model looses some of its original capacity, so it might perform worse on the validation set.\n","\n","By inspecting the output of model.summary() you can see exactly how many trainable parameters the model has. Compare the number of parameters before and after replacing Flatten with GlobalAveragePooling2D."]},{"cell_type":"markdown","metadata":{"id":"7YcyGCORRUXB"},"source":["###Task 2.6: Variable input size\n","The really cool thing about global average pooling is that it allows the network to handle input images of variable size.\n","\n","Consider the following piece of code (it's supposed to crash...):\n"]},{"cell_type":"code","metadata":{"id":"NErawUzCU81q"},"source":["# Tell Keras that height/width can be arbitrary\n","variable_input_shape = ((None,None,3))\n","variable_inputs = Input(variable_input_shape)\n","\n","# Encoder (convolutional base)\n","x = Conv2D(8, kernel_size=(3, 3), activation='relu')(variable_inputs)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(16, kernel_size=(3, 3), activation='relu')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n","encoded = Flatten()(x)\n","\n","# Decoder (2 fully connected layers)\n","x = Dense(64, activation='relu')(encoded)\n","x = Dropout(0.5)(x)\n","predictions = Dense(num_classes,activation='softmax')(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UyKohUFBbTb4"},"source":["What goes wrong is that the Flatten layer must know the shape of the input `x` in advance. Now, lets replace it with a global average pooling layer and see what happens:"]},{"cell_type":"code","metadata":{"id":"UAsH3NA1bo3l"},"source":["# Tell Keras that height/width can be arbitrary\n","variable_input_shape = ((None,None,3))\n","variable_inputs = Input(variable_input_shape)\n","\n","# Encoder (convolutional base)\n","x = Conv2D(8, kernel_size=(3, 3), activation='relu')(variable_inputs)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(16, kernel_size=(3, 3), activation='relu')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n","encoded = GlobalAveragePooling2D()(x)\n","\n","# Decoder (2 fully connected layers)\n","x = Dense(64, activation='relu')(encoded)\n","x = Dropout(0.5)(x)\n","predictions = Dense(num_classes,activation='softmax')(x)\n","# This creates a callable model that includes the Input layer and the prediction layer\n","\n","model = Model(inputs=variable_inputs, outputs=predictions)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"De6l35Dib-lB"},"source":["Now, we can run the network on images of arbitrary shape:"]},{"cell_type":"code","metadata":{"id":"NTs3Q-OGRVww"},"source":["# Make a batch of images that have size 36x36 instead of 32x32\n","import cv2\n","batch = x_train[0:batch_size,:]\n","batch_resized = np.zeros((batch_size,36,36,3))\n","for i in range(batch_size):\n","  batch_resized[i,:,:,:] = cv2.resize(batch[i,:,:,:],(36,36))\n","\n","# Shape 32x32\n","predictions = model.predict(batch)\n","print(predictions.shape)\n","\n","# Shape 36x36\n","predictions = model.predict(batch_resized)\n","print(predictions.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HNcuPRhFdRJO"},"source":["####Question\n","There is a small caveat, however. The network can only handle input images down to a certain shape (height/width).\n","- Why?\n","- What is the smallest input width/height this model can handle?"]},{"cell_type":"markdown","metadata":{"id":"n3h5lMG4O3Aw"},"source":["##Task 3: Implement building blocks of advanced network architectures\n","###Task 3.1 Naive Inception module\n","The simple, naive inception module combines 1x1, 3x3, and 5x5 convolution, as well as max pooling, into a single *layer* or *module*. It does so by concatenating along the channel dimension.\n","\n","Suppose the input to the module (called `x`) has shape `NxNxC`. Then the outputs of the convolutions should be:\n","\n","- `NxNxf1` for `conv1` (1x1 convolution), where `f1` is the number of 1x1 filters.\n","- `NxNxf3` for `conv3` (3x3 convolution), where `f3` is the number of 3x3 filters.\n","- `NxNxf5` for `conv5` (5x5 convolution), where `f5` is the number of 5x5 filters.\n","\n","and the output of the max pooling layer should be\n","\n","- `NxNxC` for `pool` (3x3 max pooling)\n","\n","You can then use Keras' `Concatenate`layer to combine the outputs. The shape of the output should be `NxNx(f1+f3+f5+C)`.\n","\n","Below is a template for implementing and testing a naive inception module. Your task is to fill in the empty gaps. The output should look like this:\n","\n","![alt text](https://github.com/aivclab/dlcourse/blob/master/data/inception_naive.png?raw=true)\n","\n","**NOTE** that the `InputLayer` is *not* part of the inception module, and neither is the first `Conv2D` layer. The first `Conv2D` layer just serves to generate an input feature map with a \"nice\" shape, meaning that the dimensions are powers of 2 (in this case `16x16x64`).\n","\n","If you need to consult the documentation, it is here: [Conv2d](https://keras.io/api/layers/convolution_layers/convolution2d/) [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/).\n"]},{"cell_type":"code","metadata":{"id":"-2SUTxkeMeQ6"},"source":["from tensorflow.keras.layers import Concatenate\n","from tensorflow.keras.utils import plot_model\n","\n","# Naive inception module\n","def naive_inception_module(x, f1=64, f3=128, f5=32):\n","  # 1x1 conv\n","  conv1 = Conv2D(filters=???, kernel_size=???, padding=???, activation='relu')(x)\n","  # 3x3 conv\n","  conv3 = Conv2D(filters=???, kernel_size=???, padding=???, activation='relu')(x)\n","  # 5x5 conv\n","  conv5 = Conv2D(filters=???, kernel_size=???, padding=???, activation='relu')(x)\n","  # 3x3 max pooling\n","  pool = MaxPooling2D(pool_size=(3,3), strides=???, padding=???)(x)\n","  # concatenate filters, assumes filters/channels last\n","  y = Concatenate()([conv1, conv3, conv5, pool])\n","\n","  return y\n","\n","# Prior convolution(s) - not part of the inception module:\n","# - inputs has shape 32x32x3\n","# - x has shape 16x16x64 and serves as input to the inception module\n","x = Conv2D(64, kernel_size=(3, 3), strides=2, padding='same',activation='relu')(inputs)\n","\n","# Add inception block 1\n","layer = naive_inception_module(x)\n","\n","# create model\n","model = Model(inputs=inputs, outputs=layer)\n","model.summary()\n","\n","# plot model architecture\n","plot_model(model, show_shapes=True, to_file='inception_module.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJ4U5fdFuvvK"},"source":["###Question:\n","- What is the number of trainable parameters in the simple model above?"]},{"cell_type":"markdown","metadata":{"id":"YwLjVu5qu_VA"},"source":["###Task 3.2 Inception module\n","The version of the inception module that we have implemented is called the *naive inception module*.\n","\n","A modification to the module was made in order to reduce the amount of computation required. Specifically, 1×1 convolutional layers were added to reduce the number of filters before the 3×3 and 5×5 convolutional layers, and to decrease the number of channels after the pooling layer.\n","\n","1×1 convolutions are used to compute reductions **before** the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation making them dual-purpose. In addition, a 1x1 convolution is added **after** the max pooling layer to reduce dimensionality.\n","\n","Suppose the input to the module (called `x`) has shape `NxNxC`. Then the outputs of the convolutions should be:\n","\n","- `NxNxf1` for `conv1` (1x1 convolution), where `f1` is the number of 1x1 filters.\n","- `NxNxf3_reduce` for `conv3_reduce` (1x1 convolution), where `f3_reduce` is the number of 1x1 filters.\n","- `NxNxf3` for `conv3` (3x3 convolution), where `f3` is the number of 3x3 filters. Note that the input to `conv3` is not `x`, but `conv3_reduce`.\n","- `NxNxf5_reduce` for `conv5_reduce` (1x1 convolution), where `f5_reduce` is the number of 1x1 filters.\n","- `NxNxf5` for `conv5` (5x5 convolution), where `f5` is the number of 5x5 filters. Note that the input to `conv5` is not `x`, but `conv5_reduce`.\n","\n","and the output of the max pooling layer should be\n","\n","- `NxNxC` for `pool` (3x3 max pooling)\n","- `NxNxfpool_reduce` for `pool_reduce` (1x1 convolution), where `fpool_reduce` is the number of 1x1 filters. Note that the input to `pool_reduce` is not `x`, but `pool`.\n","\n","Below is a template for implementing and testing an inception module. Your task is to fill in the empty gaps. The output should look like this:\n","\n","![alt text](https://github.com/aivclab/dlcourse/blob/master/data/inception.png?raw=true)"]},{"cell_type":"code","metadata":{"id":"93xKdv6SyV6B"},"source":["def inception_module(x, f1=64, f3=128, f5=32, f3_reduce=32, f5_reduce=32, fpool_reduce=32):\n","  # 1x1 conv\n","  conv1 = Conv2D(filters=???, kernel_size=???, padding=???, activation='relu')(x)\n","  # 3x3 conv\n","  conv3_reduce = Conv2D(filters=???, kernel_size=???, padding=???, activation='relu')(x)\n","  conv3 = Conv2D(filters=???, kernel_size=???, padding=???, activation='relu')(conv3_reduce)\n","  # 5x5 conv\n","  conv5_reduce = Conv2D(filters=???, kernel_size=???, padding=???, activation='relu')(x)\n","  conv5 = Conv2D(filters=???, kernel_size=???, padding=???, activation='relu')(conv5_reduce)\n","  # 3x3 max pooling\n","  pool = MaxPooling2D(pool_size=(3,3), strides=???, padding='same')(x)\n","  pool_reduce = Conv2D(filters=???, kernel_size=???, padding='same', activation='relu')(pool)\n","  # concatenate filters, assumes filters/channels last\n","  y = Concatenate()([conv1, conv3, conv5, pool_reduce])\n","  return y\n","\n","# Prior convolution(s) - not part of the inception module\n","x = Conv2D(64, kernel_size=(3, 3), strides=2, padding='same',activation='relu')(inputs)\n","\n","# Add inception block\n","layer = inception_module(x)\n","\n","# create model\n","model = Model(inputs=inputs, outputs=layer)\n","model.summary()\n","\n","# plot model architecture\n","plot_model(model, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ihty3Ds8yWO8"},"source":["###Question:\n","- What is the number of trainable parameters in the model above?\n","- What is the shape of the output? Compare to *naive* inception module."]},{"cell_type":"markdown","metadata":{"id":"qm9IN-zzy5EF"},"source":["###Task 3.3 ResNet block\n","A key innovation in the ResNet paper was the residual block. The residual block, specifically the identity residual model, is a block of two convolutional layers with the same number of filters and a small filter size, where the output of the second layer is added with the input to the first convolutional layer. Drawn as a graph, the input to the module is added to the output of the module and is called a shortcut connection:\n","\n","We can implement this directly in Keras using the functional API and the add() merge function:"]},{"cell_type":"code","metadata":{"id":"j-C_XeytRgMw"},"source":["from tensorflow.keras.layers import Add\n","\n","def residual_module_type1(x, f=64):\n","\t# conv1\n","\tconv1 = Conv2D(f, (3,3), padding='same', activation='relu', kernel_initializer='he_normal')(x)\n","\t# conv2\n","\tconv2 = Conv2D(f, (3,3), padding='same', activation='linear', kernel_initializer='he_normal')(conv1)\n","\t# add filters, assumes filters/channels last\n","\ty = Add()([conv2, x])\n","\t# activation function\n","\ty = Activation('relu')(y)\n","\treturn y\n","\n","# Prior convolution(s) - not part of the inception module\n","x = Conv2D(64, kernel_size=(3, 3), strides=2, padding='same',activation='relu')(inputs)\n","\n","# Add inception block\n","layer = residual_module_type1(x)\n","\n","# create model\n","model = Model(inputs=inputs, outputs=layer)\n","\n","# plot model architecture\n","plot_model(model, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lEUi1uB1xHv"},"source":["The **Type 1** block above assumes that input `x` and output `y` have exactly the same shape. If this is not the case, we will get an error.\n","\n","The **Type 2** block serves to\n","- downsample the input by a factor of two (along height and width dimensions)\n","- double the number of channels\n","\n","Thus, if the input shape is `x.shape = N x N x C`, the output shape is `y.shape = N/2 x N/2 x C*2`.\n","\n","In the top figure below, the solid skip connections mark Type 1 blocks (shape preserved), and dashed skip connections mark Type 2 blocks (height/width halfed and channels doubled):\n","\n","![alt text](https://miro.medium.com/max/1372/1*6hF97Upuqg_LdsqWY6n_wg.png)\n","\n","One solution to implementing a Type 2 block is as follows:\n","- `conv1`: Apply convolution on input `x` with a stride of 2 and `padding=same`. This has the effect of reducing the height and width by a factor of 2.\n","- `conv2`: Apply convolution on output of `conv1` with a stride of 1 to preserve height/width.\n","- `x_reshape`: This is the shortcut connection that must assure that `x` is reshaped to half the width/height. This can be done by applying a 1×1 convolution with a stride of 2 (before the addition).\n","\n","Your task is to fill in the blanks here:"]},{"cell_type":"code","metadata":{"id":"I-DtxmucRQC1"},"source":["def residual_module_type2(x, f=64):\n","\t# conv1 (reduces height/width by a factor of two and doubles the number of filters)\n","\tconv1 = Conv2D(filters=2*f, kernel_size=(3,3), strides=2, padding='same', activation='relu', kernel_initializer='he_normal')(x)\n","\t# conv2\n","\tconv2 = Conv2D(filters=2*f, kernel_size=(3,3), padding='same', activation='linear', kernel_initializer='he_normal')(conv1)\n","  # reshape x (downsample x by factor 2 and double number of filters)\n","\tx_reshape = Conv2D(filters=???,kernel_size=???, strides=???, padding=???,activation='linear')(x)\n","\t# add filters, assumes filters/channels last\n","\ty = Add()([conv2, x_reshape])\n","\t# activation function\n","\ty = Activation('relu')(y)\n","\treturn y\n","\n","# Prior convolution(s) - not part of the residual module\n","x = Conv2D(64, kernel_size=(3, 3), strides=2, padding='same',activation='relu')(inputs)\n","\n","# Add residual module\n","layer = residual_module_type2(x)\n","\n","# create model\n","model = Model(inputs=inputs, outputs=layer)\n","model.summary()\n","\n","# plot model architecture\n","plot_model(model, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gTDnmjL4Famo"},"source":["###Task 3.4 Depthwise Separable Convolution\n","Depthwise separable convolution is a depthwise convolution followed by a\n","pointwise convolution as follows:\n","\n","- **Depthwise convolution** is the channel-wise `K × K` spatial convolution. Suppose we have 64 channels, then we will have 64 `K × K` spatial convolutions. In Keras this is implemented using [DepthwiseConv2D](https://keras.io/api/layers/convolution_layers/depthwise_convolution2d/).\n","- **Pointwise convolution** actually is the `1 × 1` convolution to change the dimension to, say 256.\n","\n","Your task is to convert the second convolution below\n","\n","```\n","x = Conv2D(256, kernel_size=(3, 3), activation='relu')(x)\n","```\n","\n","to a depthwis separable convolution. The filter size is `K=3`, the input shape is `30x30x64` and the output shape should be `28x28x256`."]},{"cell_type":"code","metadata":{"id":"q-KHsElQGFkF"},"source":["x = Conv2D(64, kernel_size=(3, 3), activation='relu')(inputs)\n","x = Conv2D(256, kernel_size=(3, 3), activation='relu')(x)\n","model = Model(inputs=inputs, outputs=x)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kfYlCaOJRlao"},"source":["from tensorflow.keras.layers import DepthwiseConv2D\n","\n","x = Conv2D(64, kernel_size=(3, 3), activation='relu')(inputs)\n","??? # your code goes here\n","model = Model(inputs=inputs, outputs=x)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Flh6DmlJnUz"},"source":["###Questions\n","1. What is the number of parameters before and after converting to depthwise separate convolution (DSC)?\n","2. What is the operation cost with standard convolution vs. DSC (see slides Lecture 8)?"]},{"cell_type":"code","source":[],"metadata":{"id":"BPn4o82B3dLh"},"execution_count":null,"outputs":[]}]}