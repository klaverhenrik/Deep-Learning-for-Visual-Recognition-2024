{"cells":[{"cell_type":"markdown","metadata":{"id":"cqEEx6pF_ZNh"},"source":["# Text classification with Transformer\n","\n","This lab is based on [this](https://keras.io/examples/nlp/text_classification_with_transformer/) Keras tutorial. It shows how to train a transformer model to predict the sentiment (positive or negative) of IMDB movie reviews.\n","\n","Your task:\n","- See if you can identify and understand the individual layers of the `TransformerBlock` (consult the slides from lecture 11).\n","- See if you can understand how `TokenAndPositionEmbedding` works. Hint: In this example, both the word embeddings and the positional embeddings are trainable. This means they are initialized with random values, and then they get updated as we do the training."]},{"cell_type":"markdown","metadata":{"id":"XNI1Doz__ZNi"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-bfOOPT_ZNi"},"outputs":[],"source":["import keras\n","from keras import ops\n","from keras import layers"]},{"cell_type":"markdown","metadata":{"id":"1Hbpy6kg_ZNj"},"source":["## Implement a Transformer block as a layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iP_5q95m_ZNj"},"outputs":[],"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs):\n","        # This defines the forward pass of the TransformerBlock\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output)\n","        return self.layernorm2(out1 + ffn_output)"]},{"cell_type":"markdown","metadata":{"id":"XnlaQ0VQ_ZNj"},"source":["## Implement embedding layer\n","\n","Two  embedding layers, one for tokens, one for token index (positions)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YuG6yQ6V_ZNj"},"outputs":[],"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super().__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        # This defines the forward pass of the TokenAndPositionEmbedding\n","\n","        # Get position embedding of each token\n","        maxlen = ops.shape(x)[-1]\n","        positions = ops.arange(start=0, stop=maxlen, step=1)\n","        positions = self.pos_emb(positions)\n","\n","        # Get word embedding of each token\n","        x = self.token_emb(x)\n","\n","        # Add word embeddings and position embeddings\n","        return x + positions"]},{"cell_type":"markdown","metadata":{"id":"UrC3pLJZ_ZNj"},"source":["## Download and prepare dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WPGpDkQw_ZNk"},"outputs":[],"source":["vocab_size = 20000  # Only consider the top 20k words\n","maxlen = 200  # Only consider the first 200 words of each movie review\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","print(len(x_train), \"Training sequences\")\n","print(len(x_val), \"Validation sequences\")\n","x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"]},{"cell_type":"markdown","metadata":{"id":"bpsrzSCA_ZNk"},"source":["## Create classifier model using transformer layer\n","\n","Transformer layer outputs one vector for each word of our input sentence. We need a way to embed the sentence as a whole, i.e., a **sentence embedding**. In this example, we take the mean across all time steps (using `GlobalAveragePooling1D`) and use that as our sentence embedding. We then use a feed forward network on top of it to classify text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xg28VFtR_ZNk"},"outputs":[],"source":["embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n","\n","inputs = layers.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","embedding = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(embedding)\n","x = layers.GlobalAveragePooling1D()(x) # sentence embedding\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(20, activation=\"relu\")(x)\n","x = layers.Dropout(0.1)(x)\n","outputs = layers.Dense(2, activation=\"softmax\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"markdown","metadata":{"id":"xE5pzReA_ZNk"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NRVUbQQ_ZNk"},"outputs":[],"source":["model.compile(\n","    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",")"]},{"cell_type":"markdown","source":["## Evaluation\n","Now that we have trained your model, let's evaluate it on the first sample of our valiation set `x_val[0]`."],"metadata":{"id":"cjtHjmlKMsoa"}},{"cell_type":"code","source":["x_val[0]"],"metadata":{"id":"DjC27JL9K0nS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wait what, shouldn't that be a sentence? Well yes, and it is! Each integer corresponds to a word in the vocabulary.\n","\n","Below is the code to to decode the sentence into a readable format (based on  the [IMDB dataset documentation](https://keras.io/api/datasets/imdb/))."],"metadata":{"id":"VyJOmNk3Ri19"}},{"cell_type":"code","source":["# Use the default parameters to keras.datasets.imdb.load_data\n","start_char = 1\n","oov_char = 2\n","index_from = 3\n","\n","# Retrieve the word index file mapping words to indices\n","word_index = keras.datasets.imdb.get_word_index()\n","# Reverse the word index to obtain a dict mapping indices to words\n","# And add `index_from` to indices to sync with `x_train`\n","inverted_word_index = dict(\n","    (i + index_from, word) for (word, i) in word_index.items()\n",")\n","# Update `inverted_word_index` to include `start_char` and `oov_char`\n","inverted_word_index[0] = \"\"\n","inverted_word_index[start_char] = \"[START]\"\n","inverted_word_index[oov_char] = \"[OOV]\"\n","\n","# Decode the first movie review in the validation dataset\n","decoded_sequence = \" \".join(inverted_word_index[i] for i in x_val[0])\n","decoded_sequence"],"metadata":{"id":"llrFD47ZRiMy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Predicted class probabilities (0 is positive and 1 is negative)', model.predict(x_val[:1]))\n","print(\"Ground truth label:\", y_val[0])"],"metadata":{"id":"XOmtDr0MS6i8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Word embeddings\n","You can use this to get the word embeddings of the first review in the training dataset:"],"metadata":{"id":"79G3ws4gUPeF"}},{"cell_type":"code","source":["word_embedder = keras.Model(inputs=inputs, outputs=embedding)\n","word_embeddings = word_embedder.predict(x_train[:1]).squeeze()\n","print(word_embeddings.shape)"],"metadata":{"id":"d8YBwkUnL0tc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the decoded sentence, printed as a list:"],"metadata":{"id":"t-R7iHr-U7YY"}},{"cell_type":"code","source":["decoded_sequence = [inverted_word_index[i] for i in x_train[0]]\n","decoded_sequence"],"metadata":{"id":"F--KVvGEMRlA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This piece of code computes the pairwise cosine distance between all word en the sentence."],"metadata":{"id":"fJ4k4VzEV1Nj"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import pairwise_distances\n","from matplotlib import pyplot as plt\n","d = pairwise_distances(word_embeddings, word_embeddings, metric='cosine')\n","plt.imshow(d)"],"metadata":{"id":"8HOhWXKlQ2fH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Task:** Which 10 words in the sentence are semantically closest to \"amazing\" (`decoded_sequence[11]`)?"],"metadata":{"id":"-qzTK4B_WD4r"}},{"cell_type":"code","source":["???"],"metadata":{"id":"hKmjPg8JRd4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7WEAV7NbQf4_"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_with_transformer.ipynb","timestamp":1731360600387}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}