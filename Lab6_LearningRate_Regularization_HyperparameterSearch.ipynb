{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9Mihd9hpHKVh"},"source":["#Lab 7: Training neural networks (part 2)\n","**Like always, remember to set Runtime environment to GPU**\n","\n","In this lab we will explore more tools that will help you train your own neural networks. This time we will be using convolutional neural networks (CNNs).\n"]},{"cell_type":"markdown","metadata":{"id":"0tGBcfgIumcH"},"source":["##1. Download the CIFAR 10 dataset\n","We will be using the CIFAR 10 dataset."]},{"cell_type":"code","metadata":{"id":"u_awI9jy3V9P"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","mean = 0.5\n","std = 0.5\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((mean, mean, mean), (std, std, std))])\n","\n","# Batch size\n","bs = 64\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n","                                          shuffle=True, num_workers=2,drop_last=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=bs,\n","                                         shuffle=False, num_workers=2,drop_last=True)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iorLYSEz8Xbs"},"source":["Display some stats"]},{"cell_type":"code","metadata":{"id":"U50pDb-_5uKq"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Function to show an image\n","def imshow(img):\n","    npimg = img.numpy() / 2 + 0.5 # Un-normalize\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","# Number of samples\n","num_train = bs * len(trainloader)\n","num_test = bs * len(testloader)\n","print('num_train',num_train)\n","print('num_test',num_test)\n","\n","# Get a batch of some random training images\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","print('images.shape',images.shape)\n","\n","# show 16 images and print labels\n","imshow(torchvision.utils.make_grid(images[0:16]))\n","print(' '.join('%5s' % classes[labels[j]] for j in range(16)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p410rTQvawvk"},"source":["## 2. Task 1 (optional): Setting up your CNN\n","**You can skip this task and jump directly to Task 2, if you feel comfortable with PyTorch and the different layer types used in CNNs.**\n","\n","Here, we will set up our own CNN. For this purpose we will need the following layer types:\n","\n","- [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n","- [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n","- [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)\n","- [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n","- [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)\n","- [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear])\n","\n","as well as the reshape operation (called `view` in PyTorch):\n","\n","- [view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view)\n","\n","See complete overview of the basic building block for graphs [here](https://pytorch.org/docs/stable/nn.html).\n","\n","Let's see how each of these work:"]},{"cell_type":"code","metadata":{"id":"PiLeU_-tYy6g"},"source":["# First import what we need\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ywU6hEA5FVx2"},"source":["**Side-note:** What's the difference between torch.nn and torch.nn.functional? Read [here](https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597/7)"]},{"cell_type":"markdown","metadata":{"id":"RRTKA44Wg-Pq"},"source":["###2.1 Conv2d\n","Reference to documentation: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d"]},{"cell_type":"code","metadata":{"id":"3hJym68ndltc"},"source":["# Print shape of input\n","print('Input dimensions are (batch_size, channels, height, width)')\n","print('images.shape',images.shape)\n","\n","# Perform convolution\n","conv = nn.Conv2d(3, 6, 5)\n","x = conv(images)\n","\n","# Print shape of output\n","print('x.shape',x.shape)\n","\n","# Print parameter shapes\n","for name, param in conv.named_parameters(): print('parameter',name,param.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HfUehTJBdvi9"},"source":["####2.1.1 Questions\n","1. What do each of the parameters (3, 6, 5) mean?\n","2. Recall the formula to calculate the output size of a convolution:\n","\n"," `output_size = (input_size - kernel_size) / stride + 1`\n","\n"," or as defined in the slides of Lecture 4:\n","\n"," `output_size = (N - W) / stride + 1`\n","\n"," Is this formula satisfied in the above example?\n","3. How could you make the output (x) have the same width and height as the input (images)? (Think \"padding\"...)\n","4. If the convolution operation performed above was a layer in a CNN, what would the number of parameters of that layer be?"]},{"cell_type":"markdown","metadata":{"id":"2cjifwxHhELC"},"source":["###2.2 MaxPool2d\n","Reference to documentation: https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d"]},{"cell_type":"code","metadata":{"id":"agvWu8aS_MvI"},"source":["# Random input - dimensions are (batch_size, channels, height, width)\n","x = torch.randn(64,6,28,28)\n","\n","# Perform max pool\n","pool = nn.MaxPool2d(2, 2)\n","y = pool(x)\n","print('y.shape',y.shape)\n","\n","# Print some elements\n","print('x[0,0,0:4,0:4]:\\n',x[0,0,0:4,0:4])\n","print('y[0,0,0:2,0:2]:\\n',y[0,0,0:2,0:2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ah3lVjAkhlSA"},"source":["####2.2.1 Questions\n","1. What does MaxPool2d do? (Explain the output `y[0,0,0:2,0:2]` and compare to `x[0,0,0:2,0:2]`)\n","2. What do each of the parameters (2, 2) mean?"]},{"cell_type":"markdown","metadata":{"id":"Dr03cZUtiePH"},"source":["###2.3 BatchNorm2d\n","Reference to documentation: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d"]},{"cell_type":"code","metadata":{"id":"G8fNLPvkk97G"},"source":["# Random input - dimensions are (batch_size, channels, height, width)\n","x = torch.randn(64,6,28,28)*10 + 2\n","print('x.mean',x.mean())\n","print('x.std',x.std())\n","\n","# Perform batch normalization\n","bn = nn.BatchNorm2d(6)\n","y = bn(x)\n","print('y.mean',y.mean())\n","print('y.std',y.std())\n","\n","# Print parameter shapes\n","for name, param in bn.named_parameters(): print('parameter',name,param.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lTVNmKE1lodt"},"source":["####2.3.1 Questions\n","1. What does BacthNorm2D do?\n","2. What does the input parameter (6) mean?\n","3. Is the mean and standard deviation after batch normalization as expected?\n","4. How many parameters does BatchNorm2D have? Why?"]},{"cell_type":"markdown","metadata":{"id":"uotfgd9LmE71"},"source":["###2.4 ReLU\n","Reference to documentation: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"]},{"cell_type":"code","metadata":{"id":"qGdYW9i8mLke"},"source":["# Random input - dimensions are (batch_size, channels, height, width)\n","x = torch.randn(64,6,28,28)\n","print('x.min',x.min())\n","print('x.max',x.max())\n","print('x[0,0,0:4,0:4]:\\n',x[0,0,0:4,0:4])\n","\n","# Apply ReLU\n","y = F.relu(x)\n","print('y.min',y.min())\n","print('y.max',y.max())\n","print('y[0,0,0:4,0:4]:\\n',y[0,0,0:4,0:4])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIpw0OsNmh69"},"source":["####2.4.1 Question\n","1. What does ReLU do? (Explain the output `y[0,0,0:4,0:4]` and compare to `x[0,0,0:4,0:4]`)"]},{"cell_type":"markdown","metadata":{"id":"ZMdfxXxfnXNc"},"source":["###2.5 Linear and view\n","Reference to documentation:\n","- https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n","- https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view\n","\n","Recall that CNNs for classification are divided into two blocks:\n","\n","1. **Convolutional base (encoder):** A sequence of convolutional layers resulting in a feature map that you may think of as a multi-dimensional image with a certain width, height, and depth (number of channels).\n","2. **Fully connected layers (decoder):** One or more fully connected layers ending with a classifier.\n","\n","The input to the decoder has to be a vector with the correct dimensions. Thus, our task is to convert the feature map into a vector. In the code below we wish to take the feature map `x`, vectorize it, and feed it through a fully connected layer to produce 10-dimensional output.\n","\n","Your task is to figure out, what value `N` should have (see helper questions below):"]},{"cell_type":"code","metadata":{"id":"J8QTkgQsnkWQ"},"source":["# Random input - dimensions are (batch_size, channels, height, width)\n","x = torch.randn(64,6,28,28)\n","\n","N = 6*28*28 # Solution\n","\n","# Vectorize\n","y = x.view(-1, N)\n","print('y.shape',y.shape)\n","\n","# Fully connected layer\n","fc = nn.Linear(N,10)\n","z = fc(y)\n","print('z.shape',z.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_VFPz_h-puRE"},"source":["The correct output shape (of z) is [64, 10], where 64 is the batch size and 10 is the dimensionality we want in the output side of the fully connected layer."]},{"cell_type":"markdown","metadata":{"id":"J1b1TV5toah0"},"source":["####2.5.1 Helper questions\n","1. What does `view` do?\n","2. What does `Linear` do?\n"]},{"cell_type":"markdown","metadata":{"id":"EykCYUemtgEa"},"source":["##2.6 Dropout\n","Reference to documentation: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout\n","\n","Dropout is normally used only in relation to fully connected layers (i.e., not convolutional layers). The example below demonstrates the effect of the Dropout layer:"]},{"cell_type":"code","metadata":{"id":"rdvUyIHytmFM"},"source":["# Random input - dimensions are (batch_size, num_in_features)\n","x = torch.randn(64,10)\n","print('x[0,:]',x[0,:])\n","\n","drop = nn.Dropout(p=0.5)\n","y = drop(x)\n","print('y[0,:]',y[0,:])\n","\n","print('ratios',y[0,:]/x[0,:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q2IUvi7uulBs"},"source":["**IMPORTANT NOTE:** If necessary, re-run the code block until the printed part of `y[0,:]` contains both zeros and non-zeroes.\n","\n","####2.6.1 Questions\n","1. Explain why there are zeroes in `y`.\n","2. Ratios: Why are the values of the non-zero entries of `y` twice as large as the corresponding values of the input `x`?\n","3. What happens to the ratios if you set `p=0.75`?"]},{"cell_type":"markdown","metadata":{"id":"3JF7Iuo4-Ktj"},"source":["##2.7 Setting up the model\n","Your task is to fill in the blanks (???) below.\n","\n","Hint: You can insert print statements in the `forward` function, which is executed when calling `scores = model(images)`."]},{"cell_type":"code","metadata":{"id":"Gj73e2T2-NXM"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0)\n","        self.bn1 = nn.BatchNorm2d(num_features=6)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n","        self.bn2 = nn.BatchNorm2d(num_features=???)\n","        self.drop = nn.Dropout(p=0.5)\n","        self.fc1 = nn.Linear(in_features=???, out_features=120)\n","        self.fc2 = nn.Linear(in_features=???, out_features=10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = self.pool(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = self.pool(x)\n","        x = x.view(-1, ???)\n","        x = self.drop(x)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        return x\n","\n","model = Net()\n","\n","print('Network architecture:\\n')\n","print(model)\n","\n","# Print parameter shapes\n","print('Network parameters:\\n')\n","for name, param in model.named_parameters(): print('parameter',name,param.shape)\n","\n","# Test model\n","scores = model(images)\n","print(scores.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6Mls_th96_f"},"source":["Example code to find shape of input to first fully connected layer:"]},{"cell_type":"code","metadata":{"id":"bS_0Z4Mb91IP"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0)\n","        self.bn1 = nn.BatchNorm2d(num_features=6)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n","        self.bn2 = nn.BatchNorm2d(num_features=16) # num_features = 16 because conv2 has out_channels = 16\n","        self.drop = nn.Dropout(p=0.5)\n","        #self.fc1 = nn.Linear(in_features=???, out_features=120) # in_features = 16*5*5 because output shape of previous layer is 16x5x5 (see code below)\n","        #self.fc2 = nn.Linear(in_features=???, out_features=10) # in_features = 120 becuase out_features of previous layer is 120\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = self.pool(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = self.pool(x)\n","        print(x.shape)\n","        #x = x.view(-1, ???)\n","        #x = self.drop(x)\n","        #x = self.fc1(x)\n","        #x = F.relu(x)\n","        #x = self.fc2(x)\n","        return x\n","\n","model = Net()\n","\n","# Test model\n","x = model(images)\n","print(x.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjLJIHy_rI76"},"source":["##3. Task 2: Training the model\n","In this task, just run the code, observe what happens and do your best to understand it."]},{"cell_type":"code","metadata":{"id":"Yi7JRPN09Oft"},"source":["# First import what we need\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZAtdZzzOTV0"},"source":["###3.1 Setting up the CNN model\n","First, lets set up a simple CNN with 2 convolutional layers, followed by two fully connected layers. The network is equivalent to the network defined above in Task 1. Each convolution operation is followed by batch normalization, ReLU, and max pooling (Note that instead of max pooling we could also have applied convolution with a stride of 2 to achieve the same level of down-sizing). The self-made Lambda layer helps us reshape the output of the last conv layer to a vector, because PyTorch has no built-in reshape layer. We perform dropout on this vector with p = 0.5, before applying the two fully connected layers."]},{"cell_type":"code","metadata":{"id":"dxomv8SqMeZX"},"source":["class Lambda(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","\n","    def forward(self, x):\n","        return self.func(x)\n","\n","def Net():\n","  net = nn.Sequential(\n","      nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0),\n","      nn.BatchNorm2d(num_features=6),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2),\n","      nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n","      nn.BatchNorm2d(num_features=16),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2),\n","      Lambda(lambda x: x.view(x.size(0), -1)), # Vectorize\n","      nn.Dropout(p=0.5),\n","      nn.Linear(in_features=16*5*5,out_features=120),\n","      nn.ReLU(),\n","      nn.Linear(in_features=120, out_features=10)\n","  )\n","  return net\n","\n","model = Net()\n","\n","print('Network parameters:\\n')\n","print(model)\n","\n","# Print parameter shapes\n","for name, param in model.named_parameters(): print('parameter',name,param.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"URqB3o81VhDt"},"source":["Here is a useful function that you can call to re-initialize the weights/parameters of the model:"]},{"cell_type":"code","metadata":{"id":"bulDYEyuUVWN"},"source":["def reset_parameters(net):\n","    '''Init layer parameters.'''\n","    for m in net.modules():\n","        if isinstance(m, nn.Conv2d):\n","            torch.nn.init.kaiming_normal_(m.weight)\n","            if m.bias is not None:\n","                torch.nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.BatchNorm2d):\n","            torch.nn.init.constant_(m.weight, 1) # Why 1? Gamma = scaling = 1\n","            torch.nn.init.constant_(m.bias, 0) # Why 0? Beta = offset = 0\n","        elif isinstance(m, nn.Linear):\n","            torch.nn.init.kaiming_normal_(m.weight)\n","            if m.bias is not None:\n","                torch.nn.init.constant_(m.bias, 0)\n","\n","reset_parameters(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TezEOgS6Qd0b"},"source":["###3.2 Testing the model\n","Let's test the model:"]},{"cell_type":"code","metadata":{"id":"BZL4vNm1ylf9"},"source":["# Move data to GPU\n","images = images.cuda()\n","labels = labels.cuda()\n","\n","# Calculate scores\n","model = Net().cuda()\n","scores = model(images)  # predictions\n","\n","print(scores.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCBQB7MkQrqo"},"source":["###3.3 Calculating the accuracy\n","The scores (logits) don't have any meaningful interpretation, but we can convert them into class probabilities using softmax. Since we are only going to be interested in the model's accuracy, we will wrap the softmax inside function accuracy that calculates the accuracy on a batch:"]},{"cell_type":"code","metadata":{"id":"k7omGq3PxDL2"},"source":["def accuracy(scores, yb):\n","    score2prob = nn.Softmax(dim=1)\n","    preds = torch.argmax(score2prob(scores), dim=1)\n","    return (preds == yb).float().mean()\n","\n","print('Accuracy', accuracy(scores,labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5PW0hiPEQliS"},"source":["###3.4 Calculating the loss\n","I order to train your model, we also need a loss function. We will use the cross entropy loss already provided in PyTorch:"]},{"cell_type":"code","metadata":{"id":"ckkpUHBYyi45"},"source":["loss_func = F.cross_entropy\n","loss = loss_func(scores, labels)\n","print('Loss', loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aXmfKk9tQ8He"},"source":["###3.5 Training function (fit)\n","The code below can be used to train a model and monitor important stats as training progresses. The training is carried out by calling the `fit` function, which takes any model as input.\n","\n","You can change the optimizer by replacing `base_optimizer` with your own function handle.\n","\n","The function `base_lr_scheduler` is a learning rate scheduler that updates the learning rate of the optimizer during training. The dummy-implementation does nothing, but you can modify it to implement your own learning rate scheduler. The inputs are\n","- `T` : Total number of batches\n","- `t` : Current batch index (max(t) = T)\n","- `lr` : Current learning rate\n","\n","Other parameters to `fit` are:\n","- `bs` the batch size\n","- `epochs` the number of epochs\n","- `batches_per_epoch` the number of batches per epoch. If set to `None`, all images in the dataset are used.\n","\n","Note that you can use `batches_per_epoch` to reduce the size of the training set. The effective size of the training set is `bs*batches_per_epoch`."]},{"cell_type":"code","metadata":{"id":"PdmJibJ8xfvn"},"source":["from torch import optim\n","\n","# Function handle that returns an optimizer\n","def base_optimizer(model,lr=0.001, momentum=0.9):\n","    return optim.SGD(model.parameters(), lr=lr,momentum=momentum)\n","\n","# Function handle that updates the learning rate\n","# (note this is a dummy implementation that does nothing)\n","def base_lr_scheduler(t,T,lr):\n","  return lr\n","\n","# Function to fit a model\n","def fit(model,\n","        opt_func=base_optimizer,\n","        lr_scheduler=base_lr_scheduler,\n","        bs=256,\n","        epochs=1,\n","        batches_per_epoch=None, # Default: Use entire training set\n","        show_summary=True):\n","\n","  # Set up data loaders\n","  if batches_per_epoch == None:\n","    # Use all images\n","    train_dl = torch.utils.data.DataLoader(trainset, batch_size=bs,\n","                                          shuffle=True, num_workers=2)\n","    valid_dl = torch.utils.data.DataLoader(testset, batch_size=bs,\n","                                         shuffle=False, num_workers=2)\n","    batches_per_epoch = len(train_dl)\n","  else:\n","    # Only use a subset of the data\n","    subset_indices = list(range(batches_per_epoch*bs))\n","    train_dl = torch.utils.data.DataLoader(trainset, batch_size=bs, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices), num_workers=2)\n","\n","    # Use one fourth for validation\n","    subset_indices = list(range(int(np.ceil(batches_per_epoch/4))*bs))\n","    valid_dl = torch.utils.data.DataLoader(testset, batch_size=bs, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices), num_workers=2)\n","\n","  # Initialize optimizer\n","  opt = opt_func(model)\n","\n","  # For book keeping\n","  train_loss_history = []\n","  valid_loss_history = []\n","  plot_time_train = []\n","  plot_time_valid = []\n","\n","  # Index of current batch\n","  t = 1\n","\n","  # Total number of batches\n","  T = batches_per_epoch * epochs\n","\n","  print('Epochs:',epochs,'Batches per epoch:',batches_per_epoch,'Total number of batches',T)\n","\n","  # Get initial validation loss and accuracy\n","  model.eval()\n","  with torch.no_grad():\n","    valid_acc = sum(accuracy(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n","    valid_loss = sum(loss_func(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n","    valid_loss_history.append(valid_loss.detach().cpu().numpy())\n","    plot_time_valid.append(t)\n","\n","  # Train\n","  for epoch in range(epochs):\n","    model.train() # Train mode\n","    for xb, yb in train_dl:\n","\n","      # Update learning rate\n","      opt.param_groups[0]['lr'] = lr_scheduler(t,T,lr=opt.param_groups[0]['lr'])\n","\n","      # Forward prop\n","      pred = model(xb.cuda())\n","      loss = loss_func(pred, yb.cuda())\n","\n","      # Book keeping\n","      train_loss_history.append(loss.detach().cpu().numpy())\n","      plot_time_train.append(t)\n","      t += 1\n","\n","      # Backward prop (calculate gradient)\n","      loss.backward()\n","\n","      # Update model parameters\n","      opt.step()\n","      opt.zero_grad()\n","\n","      # Validation loss and accuracy\n","      if t % 10 == 0:    # print every 10 mini-batches\n","        model.eval() # Test mode\n","        with torch.no_grad():\n","            valid_acc = sum(accuracy(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n","            valid_loss = sum(loss_func(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n","            valid_loss_history.append(valid_loss.detach().cpu().numpy())\n","            plot_time_valid.append(t-1)\n","            print('t',t,'lr',opt.param_groups[0]['lr'],'train loss',loss.detach().cpu().numpy(), 'val loss',valid_loss.detach().cpu().numpy(),'val accuracy', valid_acc.detach().cpu().numpy())\n","        model.train() # Back to train mode\n","\n","  # Summary\n","  if show_summary:\n","    plt.figure()\n","    lines = []\n","    labels = []\n","    l, = plt.plot(plot_time_train,train_loss_history)\n","    lines.append(l)\n","    labels.append('Training')\n","    print(valid_loss_history)\n","    l, = plt.plot(plot_time_valid,valid_loss_history)\n","    lines.append(l)\n","    labels.append('Validation')\n","    plt.title('Loss')\n","    plt.legend(lines, labels, loc=(1, 0), prop=dict(size=14))\n","    plt.show()\n","\n","  return train_loss_history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0brbs2QTF4o"},"source":["###3.6 Train the model\n","Train the model and inspect the loss curves."]},{"cell_type":"code","metadata":{"id":"OmWk0LUvy18m"},"source":["# Re-initialize weights\n","reset_parameters(model)\n","\n","# Train with defaul settings.\n","train_loss_history = fit(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dvABXtUhGSad"},"source":["**Note:** The validation loss (on the unseen data) is overall *lower* than the training loss. This might initially seem like an unexpected behaviour, but it results from the fact that we use 50% Dropout during training, meaning that half of the weights of the fully connected layers are not used during training. When evaluating the model on the validation set, Dropout is disabled, and all weights are used. This leads to a better prediction (i.e., a lower loss) on the validation set."]},{"cell_type":"markdown","metadata":{"id":"u1N4oj_tRABK"},"source":["###3.7 Performing quick experiments on reduced dataset\n","When searching for hyperparameters it is often useful to perform quick experiments on only a small subset of the data. You can do this by decreasing the batch size and setting `batches_per_epoch` to say 20. The effective size of the training set is then `bs*batches_per_epoch` (say `128*20 = 2,560`), and the total number of batches being processed during training is `bs*batches_per_epoch*epochs` (say `128*20*5 = 51,200`).\n"]},{"cell_type":"code","metadata":{"id":"zYEcxzoHQxoT"},"source":["# Re-initialize weights\n","reset_parameters(model)\n","\n","# Example: Train on small subset\n","train_loss_history = fit(model,bs=128,epochs=5,batches_per_epoch=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnsvkrbOWl3r"},"source":["###3.7 Train with custom optimizer\n","Fell free to experiment with these optimizers."]},{"cell_type":"code","metadata":{"id":"5BXxefozVuWo"},"source":["#SGD + Momentum\n","def momentum_optimizer(model):\n","    return optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","\n","# Adagrad\n","def adagrad_optimizer(model):\n","  return optim.Adagrad(model.parameters(), lr=0.1, lr_decay=0.0, weight_decay=0.0, initial_accumulator_value=0)\n","\n","# RMSProp\n","def rmsprop_optimizer(model):\n","  return optim.RMSprop(model.parameters(), lr=0.1, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n","\n","# Adam\n","def adam_optimizer(model):\n","  return optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n","\n","# Train with custom optimizer (on small subset)\n","reset_parameters(model) # Reset network weights\n","train_loss_history = fit(model,opt_func=momentum_optimizer,bs=128,epochs=5,batches_per_epoch=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UbnzkuxTX4wV"},"source":["##4. Tasks: You are free to choose any of the tasks below\n","The overall goal is to make the model perform as good as possible on the CIFAR 10 dataset. Solving the tasks below might help you, but feel free to try other options. For inspiration, here is a list of other things, you could do:\n","\n","- **Increase the capacity of the network** (add more conv layers, add more kernels in each layer, add another fully connected layer, add more connections in the fully connected layer, etc.)\n","- **Experiment with weight decay** (weight decay is an option for, say, the [SGD optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD))\n","\n","General recommendations for hyperparameter search (see Lecture 6 slides):\n","- Stage your search from course to fine\n","- Start with a small sample (subset) of the dataset to tune in on hyperparameters such as learning rate, and start with 1 epoch or less\n","- The second stage could then perform a narrower search with more data and/or more epochs\n","- Search for learning rates in powers of 10. Good learning rates to try: 1e-1, 1e-2, 1e-3, 1e-4, etc.)\n","- **Look at loss curves!!!**\n"]},{"cell_type":"markdown","metadata":{"id":"hwjZvWPzN-LE"},"source":["###4.1 Implement a learning rate scheduler\n","**Your task** is to replace the function `base_lr_scheduler` with your own `step_decay` function (or another type of learning rate scheduler if you prefer). In step decay the lerning rate is updated for every `step` batches according to the rule `lr = lr * decay_rate`. The input `t` is the current batch index, ranging from 1 to `T`, where `T = batches_per_epoch * epochs` is the total number of batches."]},{"cell_type":"markdown","metadata":{"id":"notrUPJFYY05"},"source":["**Solution:**"]},{"cell_type":"code","metadata":{"id":"qJbbqKF6a5AI"},"source":["def step_decay(t,T,lr,step=10,decay_rate=0.9):\n","  # Your code goes here\n","\n","# Train\n","model = Net().cuda()\n","reset_parameters(model)\n","train_loss_history = fit(model,opt_func=momentum_optimizer,lr_scheduler=step_decay,bs=128,epochs=5,batches_per_epoch=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79vJl3fENgXF"},"source":["**Sub-tasks:**\n","- What new hyperparameters are added with learning rate decay?\n","- See if you can make the model perform better with learning rate decay."]},{"cell_type":"markdown","metadata":{"id":"eWy7obmYYaWY"},"source":["###4.2 Implement a learning rate finder\n","Finding a good initial learning rate can be done efficiently with a learning rate finder. Here is the basic idea:\n","\n","You can modify the learning rate scheduler `base_lr_scheduler` to use a hardcoded list of learning rates. This can be used to implement a learning rate finder according to this principle:\n","- Start training and increase learning rate linearly after each batch (!!!), and calculate the loss\n","- Then display the loss as a function of learning rate.\n","- By manual inspection, select a suitable range of learning rates by locating the strongest downward slope.\n","\n","Say the hardcoded learning rates are:"]},{"cell_type":"code","metadata":{"id":"n2b6Vl1bZkzn"},"source":["# Ordered list of learning rates (ascending order)\n","lr_list = [0.001,0.01,0.05,0.1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9c3CL-xO0YD"},"source":["Then simply implement a learning rate scheduler that returns the t´th entry of the learning rate list:"]},{"cell_type":"code","metadata":{"id":"z0GyYWN2Lb8Q"},"source":["def lr_finder(t,T,lr):\n","  return lr_list[t-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75cMHVsJLc3K"},"source":["Note, the learning rate scheduler must have this interface: `(t,T,lr)`\n","\n","Then train the model for as many batches as there are entries in the learning rate list:"]},{"cell_type":"code","metadata":{"id":"NN1oLDpqZwNB"},"source":["# Make sure that we process as many batches as there are learning rates in our list:\n","batches_per_epoch = len(lr_list)\n","epochs = 1\n","\n","# Train\n","model = Net().cuda()\n","reset_parameters(model)\n","train_loss_history = fit(model,lr_scheduler=lr_finder,epochs=epochs,batches_per_epoch=batches_per_epoch,show_summary=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6ZOzLWjLvrl"},"source":["Plot the loss versus learning rate and inspect:"]},{"cell_type":"code","metadata":{"id":"kgPokRI_L2Tx"},"source":["plt.figure()\n","plt.plot(lr_list,train_loss_history)\n","plt.xlabel('Learning rate');\n","plt.ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1Zzl2zcQESM"},"source":["**Your task** is to implement a learning rate finder that searches through a range of learning rates, like above.\n","\n","The range of learning rates should be logarithmic (say from 10^-6 to 10^0). You can define the list of learning rates using `np.logspace`.\n","\n","Then plot the loss versus the learning rate. It is a good idea to use a logarithmic scale on the x-axis (see `plt.xscale('log')`)."]},{"cell_type":"code","metadata":{"id":"3ESD1B4JQc1S"},"source":["# Your code goes here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JMkJAqzXU8q7"},"source":["**Optional sub-task:** Can you come up with a simple way to implement Cyclic Learning Rate (CLR) based on the above?\n","\n","Hint: Run training multiple times using a carefully selected list of learning rates each time."]},{"cell_type":"code","metadata":{"id":"6bd9PAX1VIJu"},"source":["# Your code goes here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5zgyxJjXNYv"},"source":["###4.3 Experiment with dropout\n","Below is a slight modification of our model that allows changing the dropout fraction (p).\n","\n","**Your task** is to experiment with different values of p.\n","\n","- What does setting p = 0 mean?\n","- Try different values of p (say 0, 0.1, 0.5 and 0.9). What is the optimal value of p?"]},{"cell_type":"code","metadata":{"id":"YF_mWFkCXhw1"},"source":["def Net(p=0.5):\n","  net = nn.Sequential(\n","      nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0),\n","      nn.BatchNorm2d(num_features=6),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2),\n","      nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n","      nn.BatchNorm2d(num_features=16),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2),\n","      Lambda(lambda x: x.view(x.size(0), -1)), # Vectorize\n","      nn.Dropout(p=p),\n","      nn.Linear(in_features=16*5*5,out_features=120),\n","      nn.ReLU(),\n","      nn.Linear(in_features=120, out_features=10)\n","  )\n","  return net"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model with p=0.25\n","model = Net(p=0.25).cuda()\n","reset_parameters(model)\n","train_loss_history = fit(model,bs=128,epochs=5,batches_per_epoch=20)"],"metadata":{"id":"Hj6CV0XbhDZ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZG7wXrgbIRh"},"source":["###4.4 Data augmentation\n","Adding more training data is the best way to improve your models abiliy to generalize to unseen data. Data augmentations is basically about generating additional *fake data\" to expand the size of your training set.\n","\n","**Your task** is to experiment with different types of data augmentation. All you have to do is change the transformer below and add some of the available transforms: https://pytorch.org/vision/stable/transforms.html\n","\n","You can find inspiration in Lab 5 (trasnfer learning): https://github.com/klaverhenrik/Deep-Learning-for-Visual-Recognition-2024/blob/main/Lab5_PyTorch_TransferLearning.ipynb"]},{"cell_type":"code","metadata":{"id":"2Rurvqei1ZF8"},"source":["mean = 0.5\n","std = 0.5\n","transform = transforms.Compose(\n","    [\n","      # Add your transforms here (before calling ToTensor),\n","      transforms.ToTensor(),\n","      transforms.Normalize((mean, mean, mean), (std, std, std))\n","     ])\n","\n","# Batch size\n","bs = 64\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n","                                          shuffle=True, num_workers=2,drop_last=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=bs,\n","                                         shuffle=False, num_workers=2,drop_last=True)\n","\n","# Get a batch of some random training images\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","print('images.shape',images.shape)\n","\n","# show 10 images and print labels\n","imshow(torchvision.utils.make_grid(images[0:16]))\n","print(' '.join('%5s' % classes[labels[j]] for j in range(16)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8KrOsoCNoeZ"},"source":["# Train\n","model = Net().cuda()\n","reset_parameters(model)\n","train_loss_history = fit(model,bs=128,epochs=5,batches_per_epoch=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vnl8gRJqP8nl"},"source":["###4.5 Multi-task learning (MTL)\n","By sharing representations between related tasks, we can enable our model to generalize better on our original task. This is the basic idea behind MTL.\n","\n","There is one additional task that we can always add, and that is an **autoencoder**. Recall that an autoencoder is basically an identity function that maps input x to itself. It does so by first encoding x into a more compact representation E(x), and then decodes this representation to get a reconstruction of x = D(E(x)), where E() is the encoder, and D() is the decoder.\n","\n","The network below uses the **same encoder** to solve both a **classification task** and an **autoencoder task**."]},{"cell_type":"code","metadata":{"id":"5rmVdxMMgCy9"},"source":["# define the NN architecture\n","class MTLNet(nn.Module):\n","    def __init__(self):\n","        super(MTLNet, self).__init__()\n","\n","        ## shared encoder layers ##\n","        self.conv1 = nn.Conv2d(3, 16, 5, stride=2, padding=2)\n","        self.conv2 = nn.Conv2d(16, 4, 5, stride=2, padding=2)\n","\n","        ## decoder layers ##\n","        # Classifier\n","        self.fc1 = nn.Linear(in_features=4*8*8, out_features=120)\n","        self.fc2 = nn.Linear(in_features=120, out_features=10)\n","\n","        ## Autoencoder\n","        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n","        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)\n","\n","    def forward(self, x):\n","        ## encode ##\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","\n","        # decoder for task 1 - outputs scores for classifier\n","        scores = x.view(-1, 4*8*8)\n","        scores = F.relu(self.fc1(scores))\n","        scores = self.fc2(scores)\n","\n","        ## decoder for task 2 - outputs reconstruction of input x\n","        recon = F.relu(self.t_conv1(x))\n","        recon = self.t_conv2(recon)\n","\n","        return scores, recon\n","\n","# initialize the NN\n","model = MTLNet()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V-AO2RNBQ2S6"},"source":["Let's look at the output of this model:"]},{"cell_type":"code","metadata":{"id":"0XRN0F3ws4Y4"},"source":["# Get a batch of some random training images\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","\n","scores,recon = model(images)\n","print('scores.shape',scores.shape)\n","print('recon.shape',recon.shape)\n","\n","# show 16 images and print labels\n","print('\\nInput images:\\n')\n","imshow(torchvision.utils.make_grid(images[0:16]))\n","print('Reconstructed images:\\n')\n","imshow(torchvision.utils.make_grid(recon[0:16].detach().cpu()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z_imIbERRY21"},"source":["In order to optimize our MTL model we need a loss function. However, since we are solving two tasks, we also need two terms in our loss function - one for each task.\n","\n","The loss for the classification task is the familiar cross entropy:"]},{"cell_type":"code","metadata":{"id":"-0-KmzXxjbIe"},"source":["# Classification loss\n","loss_func_1 = F.cross_entropy\n","loss1 = loss_func_1(scores, labels)\n","print('Loss 1', loss1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YEPtN5-3Rw0v"},"source":["A suitable loss for the autoencoder task would be the Mean Squared Error (MSE) between the input images (say `images`) and the networks reonstruction (say `recon`):"]},{"cell_type":"code","metadata":{"id":"iz02YBIBlBH9"},"source":["# MSE loss for reconstruction\n","loss_func_2 = F.mse_loss\n","loss2 = loss_func_2(images,recon)\n","print('Loss 2', loss2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cBXZ074LSRdB"},"source":["Below is a modified function to fit the MTL model.\n","\n","**Your task** is simply to fill in the missing code (marked with ???).\n","\n","This is not as trivial as you might think (hint: do the two losses have the same scale?)."]},{"cell_type":"code","metadata":{"id":"QrOnBz0sgt1b"},"source":["# Function to fit a model\n","def fit(model,\n","        opt_func=base_optimizer,\n","        lr_scheduler=base_lr_scheduler,\n","        bs=256,\n","        epochs=1,\n","        batches_per_epoch=None,\n","        show_summary=True):\n","\n","  # Set up data loaders\n","  if batches_per_epoch == None:\n","    # Use all images\n","    train_dl = torch.utils.data.DataLoader(trainset, batch_size=bs,\n","                                          shuffle=True, num_workers=2)\n","    valid_dl = torch.utils.data.DataLoader(testset, batch_size=bs,\n","                                         shuffle=False, num_workers=2)\n","    batches_per_epoch = len(train_dl)\n","  else:\n","    # Only use a subset of the data\n","    subset_indices = list(range(batches_per_epoch*bs))\n","    train_dl = torch.utils.data.DataLoader(trainset, batch_size=bs, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices), num_workers=2)\n","\n","    subset_indices = list(range(int(np.ceil(batches_per_epoch/4))*bs))\n","    valid_dl = torch.utils.data.DataLoader(testset, batch_size=bs, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices), num_workers=2)\n","\n","  loss1_history = []\n","  loss2_history = []\n","  plot_time_train = []\n","\n","  # Initialize optimizer\n","  opt = opt_func(model)\n","\n","  t = 1 # Index of current batch\n","  T = batches_per_epoch * epochs # Total number of batches\n","\n","  print('Epochs:',epochs,'Batches per epoch:',batches_per_epoch,'Total number of batches',T)\n","\n","  # Train\n","  for epoch in range(epochs):\n","    model.train() # Train mode\n","    for xb, yb in train_dl:\n","\n","      # Update learning rate\n","      opt.param_groups[0]['lr'] = lr_scheduler(t,T,lr=opt.param_groups[0]['lr'])\n","\n","      # Forward prop\n","      scores,recon = model(xb.cuda())\n","      loss1 = loss_func_1(scores, yb.cuda())\n","      loss2 = loss_func_2(recon, xb.cuda())\n","\n","      loss = ??? # Your code goes here\n","\n","      if t % 10 == 0:    # print every 10 mini-batches\n","        print('loss1',loss1.detach().cpu().numpy(),'loss2',loss2.detach().cpu().numpy())\n","\n","      loss1_history.append(loss1.detach().cpu().numpy())\n","      loss2_history.append(loss2.detach().cpu().numpy())\n","      plot_time_train.append(t)\n","      t += 1\n","\n","      # Backward prop (calculate gradient)\n","      loss.backward()\n","\n","      # Update model parameters\n","      opt.step()\n","      opt.zero_grad()\n","\n","  # Summary\n","  if show_summary:\n","    plt.figure()\n","    lines = []\n","    labels = []\n","    l, = plt.plot(plot_time_train,loss1_history)\n","    lines.append(l)\n","    labels.append('Loss1 (classification)')\n","    l, = plt.plot(plot_time_train,loss2_history)\n","    lines.append(l)\n","    labels.append('Loss2 (autoencoder)')\n","    plt.title('Loss')\n","    plt.legend(lines, labels, loc=(1, 0), prop=dict(size=14))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5toVL8DfmHK9"},"source":["model = MTLNet().cuda()\n","\n","# Re-initialize weights\n","reset_parameters(model)\n","\n","def momentum_optimizer(model):\n","    return optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# Example (in practise you need more epochs than 5)\n","train_loss_history = fit(model,opt_func=momentum_optimizer,bs=128,epochs=5,batches_per_epoch=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zskdBSGFE_dG"},"source":["Look at some results:"]},{"cell_type":"code","metadata":{"id":"3GjuU2vUmPID"},"source":["scores,recon = model(images.cuda())\n","\n","# validation accuracy\n","acc = 0\n","for xb, yb in testloader:\n","  scores,recon_dummy = model(xb.cuda())\n","  acc += accuracy(scores,yb.cuda())\n","print('accuracy',acc/len(testloader))\n","\n","# show 16 images and print labels\n","print('\\nInput images:\\n')\n","imshow(torchvision.utils.make_grid(images[0:16]))\n","print('Reconstructed images:\\n')\n","imshow(torchvision.utils.make_grid(recon[0:16].detach().cpu()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4ZH46_tFD6Y"},"source":[],"execution_count":null,"outputs":[]}]}